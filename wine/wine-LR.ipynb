{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(file_path, delimiter, skiprows=0):\n",
    "    \"\"\"loads a data file and returns a numpy array\"\"\"\n",
    "    file = open(file_path, \"rb\")\n",
    "    arr = np.loadtxt(file, delimiter=delimiter, skiprows=skiprows)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"winequality-red.csv\", \";\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    row[-1] = 0 if row[-1] <= 5 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[:,0:-1]\n",
    "Y=np.ravel((data[:,-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = true labels\n",
    "# y_hat = training labels\n",
    "# return: accuracy of training labels (in percentage)\n",
    "# Ensure that y and y_hat contain the labels for the same training examples.\n",
    "def evaluate_acc(y, y_hat):\n",
    "    score = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == y_hat[i]:\n",
    "            score += 1\n",
    "    return (score / y.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = class labels of training examples\n",
    "# x = feature data of training examples\n",
    "# 2 < k = number of folds to use in validation\n",
    "# return: average of prediction error over the k rounds of execution\n",
    "def k_fold(x, y, k, model):\n",
    "    if k < 1:\n",
    "        return \"Must have at least 1 fold.\"\n",
    "    elif k > (x.shape[0]//2):\n",
    "        return \"Too many folds.\"\n",
    "    elif k == 1:\n",
    "        print(\"1 fold selected - model will be trained and validated on same data set\")\n",
    "        model.fit(x, y)\n",
    "        return evaluate_acc(y, model.predict(x))\n",
    "    else:\n",
    "        rows_per_fold = (x.shape[0] + 1)//k       # a few rows at the end of the training data will be unused\n",
    "        accuracy = 0\n",
    "\n",
    "        for exec_round in range(k):\n",
    "            # determine held-out range\n",
    "            lower_row = exec_round * rows_per_fold\n",
    "            upper_row = ((exec_round + 1) * rows_per_fold) - 1\n",
    "            \n",
    "            # create validation set\n",
    "            x_val = np.copy(x)[lower_row:upper_row]\n",
    "            y_val = np.copy(y)[lower_row:upper_row]\n",
    "\n",
    "            # create training set\n",
    "            x_trn = np.concatenate((x[0:lower_row], x[upper_row:]))\n",
    "            y_trn = np.concatenate((y[0:lower_row], y[upper_row:]))\n",
    "\n",
    "            # train model\n",
    "            model.fit(x_trn, y_trn)\n",
    "\n",
    "            # run validation set through model\n",
    "            y_hat = model.predict(x_val)\n",
    "            accuracy += evaluate_acc(y_val, y_hat)\n",
    "\n",
    "        return accuracy / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, alpha=0.0001, threshold = 2.5):\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.stop = False\n",
    "        self.weights = None\n",
    "        self.max_iter = 10000\n",
    "        self.change = []\n",
    "\n",
    "    def __intercept(self, X):\n",
    "        return np.c_[np.ones(len(X)), X]\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def __grad(self, X_i, y_i):\n",
    "        z = np.dot(self.weights.T, X_i)\n",
    "        return X_i*(y_i-self.__sigmoid(z))\n",
    "    \n",
    "    def __update(self, X, Y):\n",
    "        changeW = np.zeros(np.size(X, 1))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            grad = self.__grad(X[i], Y[i])\n",
    "            changeW = changeW + self.alpha*grad\n",
    "#         print(np.linalg.norm(changeW))\n",
    "        self.change.append(np.linalg.norm(changeW))\n",
    "        self.weights = self.weights + changeW\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.change = [] # reset the gradients before running a new fit\n",
    "        padded_X = self.__intercept(X)\n",
    "        self.weights = np.zeros(np.size(padded_X,1))\n",
    "        \n",
    "        num_iter = 0\n",
    "        while len(self.change) < 1000 or self.change[-1] > self.threshold and num_iter < self.max_iter:\n",
    "            self.__update(padded_X, Y)\n",
    "            num_iter+=1\n",
    "            \n",
    "            if (num_iter == self.max_iter):\n",
    "                print(f\"Warning, reached max iterations of {self.max_iter}, stopping because we haven't converged yet\")\n",
    "                break\n",
    "\n",
    "        print(f\"learning rate:{self.alpha} \\n stop threshold:{self.threshold} \\n number of iterations: {num_iter}\")\n",
    "        print(f\"weights:{self.weights}\")\n",
    "        \n",
    "        return self.weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        padded_X = self.__intercept(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(X)):\n",
    "            Z = np.dot(self.weights.T, padded_X[i])\n",
    "            pred = self.__sigmoid(Z).round()\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 10000\n",
      "weights:[-12.11842787  -4.12360625 -33.12781238  15.18441164  -2.21369087\n",
      "  -2.4392379    3.13966974  -3.01235549 -12.11321932 -44.01184925\n",
      "  18.38059553  21.81374871]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 6231\n",
      "weights:[ -7.57855634  -5.66879451 -22.18205089   8.36686696  -3.49826839\n",
      "  -2.44048974   3.98298528  -0.85691794  -7.60909407 -27.88214045\n",
      "   8.66108312  18.12914758]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 5989\n",
      "weights:[ -8.03765654  -0.31358847 -20.90659036   7.51013181  -3.40524004\n",
      "  -2.36381191   3.37976482  -3.34916919  -8.01499359 -28.16457386\n",
      "  10.04111767  14.68475331]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1002\n",
      "weights:[-1.35172067 -0.72678651 -3.59473228  1.66251224  0.61848476 -0.26298051\n",
      "  2.29212268 -2.31276385 -1.35151153 -4.69370729  1.71389785  4.67242151]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 5441\n",
      "weights:[ -7.02497639  -0.27392183 -18.22388636   7.15288307  -3.32658912\n",
      "  -1.88285887   3.5302808   -3.33221173  -7.03051398 -25.71864922\n",
      "   7.3652256   13.41510863]\n",
      "62.13166144200627\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 10000\n",
      "weights:[-11.98003168   6.22088982 -32.98006801  16.28388611  -2.28368125\n",
      "  -2.39536565   3.10305497  -3.02120527 -11.96973904 -44.25161168\n",
      "  18.63423255  22.65359187 -10.47529208]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 6155\n",
      "weights:[ -7.30611815   2.99903649 -22.00234525   9.19530799  -3.77564785\n",
      "  -2.28615494   3.06841554  -3.104186    -7.33370504 -27.6049487\n",
      "   8.82270034  18.72768334  -9.80800077]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 10000\n",
      "weights:[-11.98188077   9.91375335 -31.43606046  11.36337077  -3.24505257\n",
      "  -3.7331093    3.72332919  -0.92688527 -11.93624256 -41.72616592\n",
      "  15.58442697  21.55199288 -10.15893634]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1002\n",
      "weights:[-1.29354212  0.27327029 -3.58065517  1.7309661   0.84208821 -0.25825332\n",
      "  2.2592053  -1.90248507 -1.29332327 -4.54236099  1.78740098  5.37957981\n",
      " -1.57641878]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 5919\n",
      "weights:[ -7.42247869   5.38094689 -19.57968651   8.10447444  -3.44401544\n",
      "  -2.02100831   3.52974507  -3.31876776  -7.42515601 -27.46678653\n",
      "   8.11542989  14.87714699  -6.09426518]\n",
      "56.990595611285265\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 7452\n",
      "weights:[-10.14458526  -5.08856077 -28.12355304  13.37791286  -2.82509261\n",
      "  -1.99791052   1.93343923  -1.14878876 -10.14878413 -37.2977355\n",
      "  14.52126865  21.65805945  -3.08222799]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1001\n",
      "weights:[-1.28503992 -3.49980052 -4.06581921  1.68068817 -1.30643215 -0.38352933\n",
      "  1.66902109 -0.44879108 -1.29650681 -4.56084317  1.37676087  7.77238921\n",
      " -2.11781217]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1001\n",
      "weights:[-1.33648261 -0.25326236 -3.96125562  1.66574934 -1.11211076 -0.40697315\n",
      "  1.72863543 -1.13973245 -1.3376102  -4.70497708  1.74078619  6.79434599\n",
      " -2.86836788]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1001\n",
      "weights:[-1.296411   -0.22483991 -3.56092324  1.68182915  0.8546138  -0.25415231\n",
      "  1.64339615  0.37887417 -1.29616802 -4.50898882  1.69341945  5.33829202\n",
      " -1.26452198]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1000\n",
      "weights:[-1.26839612 -1.26303984 -3.74254547  1.60285273 -0.76191243 -0.35015074\n",
      "  2.33631978  0.83203787 -1.27494849 -4.55631855  1.44606326  7.35214698\n",
      " -1.50428191]\n",
      "59.49843260188088\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 10000\n",
      "weights:[-12.21760353  -4.6958355  -32.99766734  15.06679478  -2.22570192\n",
      "  -2.4384269    3.14424686  -3.01060223 -12.20922767 -44.066638\n",
      "  18.15777292  19.48670013   9.31763712]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 6049\n",
      "weights:[ -7.5401038   -6.44946875 -21.70300298   8.03947791  -3.61597097\n",
      "  -2.42804814   3.1664484   -3.12798859  -7.56800405 -27.51246402\n",
      "   8.25728038  15.63025499   7.11445845]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 5819\n",
      "weights:[ -7.91164139  -0.61730076 -20.37754041   7.31722272  -3.42100343\n",
      "  -2.29744619   3.37997767  -3.35747132  -7.88838635 -27.64417487\n",
      "   9.71604576  13.14059768   5.40581996]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 1000\n",
      "weights:[-1.36191205 -0.8107182  -3.58863036  1.65930661  0.65642381 -0.26313172\n",
      "  2.49298691 -1.61786225 -1.36163261 -4.72604977  1.69584201  4.53331185\n",
      "  1.52915702]\n",
      "learning rate:0.0001 \n",
      " stop threshold:2.5 \n",
      " number of iterations: 5227\n",
      "weights:[ -6.86237235  -0.6041477  -17.59301951   6.9007293   -3.35491687\n",
      "  -1.81598232   3.55592258  -3.33539355  -6.86644364 -25.02144341\n",
      "   7.03557177  11.70316645   5.74419308]\n",
      "59.49843260188088\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "# acidity\n",
    "F1 = []\n",
    "for i in range (0, len(X)):\n",
    "    F1.append(X[i][0] + X[i][1] + X[i][2])\n",
    "F1 = np.hstack((X, np.atleast_2d(F1).T))\n",
    "\n",
    "# Bounded Sulfur Dioxide\n",
    "F2 = []\n",
    "for i in range (0, len(X)):\n",
    "    F2.append(X[i][6] - X[i][5])\n",
    "F2 = np.hstack((X, np.atleast_2d(F2).T))\n",
    "\n",
    "# Acidity alcohol ratio\n",
    "F3 = []\n",
    "for i in range (0, len(X)):\n",
    "    F3.append(X[i][10]/X[i][8])\n",
    "F3 = np.hstack((X, np.atleast_2d(F3).T))\n",
    "\n",
    "print(k_fold(X, Y, 5, lr))\n",
    "print(k_fold(F1, Y, 5, lr))\n",
    "print(k_fold(F2, Y, 5, lr))\n",
    "print(k_fold(F3, Y, 5, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
