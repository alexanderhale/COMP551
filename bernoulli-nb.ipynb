{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_path = './Data/reddit_test.csv'\n",
    "\n",
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#clean\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "#load\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "#clean\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "\n",
    "comment_data['prep'] = comment_data['prep'].apply(lemmatize_col)\n",
    "test_data['prep'] = comment_data['prep'].apply(lemmatize_col)\n",
    "\n",
    "# stopwords\n",
    "stop = stopwords.words('english')\n",
    "comment_data['prep'] = comment_data['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "test_data['prep'] = test_data['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = comment_data['prep'].to_numpy()\n",
    "clean_labels = comment_data['subreddits'].to_numpy()\n",
    "\n",
    "train_comments = []\n",
    "for idx in range(clean_data.shape[0]):\n",
    "    item = (clean_data[idx], clean_labels[idx])\n",
    "    train_comments.append(item)\n",
    "train_comments = np.asarray(train_comments)\n",
    "print(train_comments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Bernoulli NB implemented with `numpy` arrays. Similar to the Bernoulli NB algorithm found in slides, but rather than a decision boundary between two classes, the prediction takes the class with the max probability according to the formula we saw in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes():\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.alpha = alpha # used for Laplace smoothing\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "    \n",
    "    def __group_samples(self, X,Y):\n",
    "        # labels -> numbers\n",
    "        labels = []\n",
    "        for label in Y:\n",
    "            labels.append(list(self.classes).index(label))\n",
    "\n",
    "        # append X|Y arrays\n",
    "        XY = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "        XY[:,:X.shape[1]] = X\n",
    "        XY[:,-1:] = np.array(labels).reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        # initialize array of empty arrays with length of number of classes\n",
    "        group_by_class = [[] for _ in range(self.classes.shape[0])]\n",
    "        \n",
    "        # for each class, append an X|y sample into array index i if y == classes[i]\n",
    "        for class_index in range(self.classes.shape[0]):\n",
    "            for sample in XY:\n",
    "                if sample[-1] == class_index:\n",
    "                    group_by_class[class_index].append(sample)\n",
    "        return group_by_class\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "        num_samples = X.shape[0]\n",
    "        groups = self.__group_samples(X,Y)\n",
    "\n",
    "        self.priors = np.array(list(map(lambda g: np.log(len(g)/num_samples), groups)))\n",
    "\n",
    "        # get count per group, number of samples per group, and divide.\n",
    "        word_count = np.array(list(map(lambda g: np.array(g).sum(axis=0)[:-1] + self.alpha, groups)))\n",
    "        group_count = np.array(list(map(lambda g: len(g), groups)))\n",
    "        \n",
    "        # get probabilities, apply laplace smoothing\n",
    "        self.features_probs = word_count/((group_count + 2*self.alpha)[:,None])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        scores = np.zeros(self.classes.shape[0])\n",
    "        \n",
    "        for i in range(self.classes.shape[0]):\n",
    "            sum = 0\n",
    "            for j in range(X.shape[0]):\n",
    "                if (X[j] == 0):\n",
    "                    sum += np.log(1 - self.features_probs[i][j])\n",
    "                else:\n",
    "                    sum += np.log(self.features_probs[i][j])\n",
    "            sum += self.priors[i]\n",
    "            scores[i] = sum\n",
    "        \n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "Used to test if the `fit` and `predict` methods will execute without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,1,0],[1,0,1],[0,0,1],[1,1,1]])\n",
    "Y = np.array([6,7,6,6])\n",
    "b = BernoulliNaiveBayes()\n",
    "\n",
    "b.fit(X,Y)\n",
    "b.predict(np.array([1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Pipeline\n",
    "Default: 5-fold with 80/10/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentFolds = kFold(train_comments)\n",
    "commentFolds.generateSplits()\n",
    "splits = commentFolds.splits\n",
    "x, y, z = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: unpacks data that was zipped together when we shuffled\n",
    "def unpack(subset):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for x,y in subset:\n",
    "        data.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNaiveBayes()\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "for split in splits:\n",
    "    train, val, test = split\n",
    "    \n",
    "    training_data, training_labels = unpack(train)\n",
    "    validation_data, validation_labels = unpack(val)\n",
    "    testing_data, testing_labels = unpack(test)\n",
    "    \n",
    "    # tokenize and remove min words on \"training set\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, ngram_range=(1,1), min_df=3, binary=True, use_idf=False, norm=None)\n",
    "    tfidf = tfidf_vectorizer.fit(training_data)\n",
    "    \n",
    "    # filter out bad words\n",
    "    training_vec = tfidf_vectorizer.transform(training_data).astype(np.float32).toarray()\n",
    "    validation_vec = tfidf_vectorizer.transform(validation_data).astype(np.float32).toarray()\n",
    "    testing_vec = tfidf_vectorizer.transform(testing_data).astype(np.float32).toarray()\n",
    "    \n",
    "    print(np.unique(training_vec))\n",
    "    \n",
    "    nb.fit(training_vec,training_labels)\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, vec in enumerate(validation_vec):\n",
    "        result = (nb.predict(vec))\n",
    "        if nb.classes[result] == validation_labels[idx]:\n",
    "            num_correct += 1\n",
    "#         print(\"Current accuracy is: \" , num_correct/(idx+1))\n",
    "    print(\"Fold \", idx+1, \" Validation accuracy is: \" , num_correct/(validation_vec.shape[0]))\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, vec in enumerate(testing_vec):\n",
    "        result = (nb.predict(vec))\n",
    "        if nb.classes[result] == testing_labels[idx]:\n",
    "            num_correct += 1\n",
    "    print(\"Fold \", idx+1, \" Testing accuracy is: \" , num_correct/(testing_vec.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
