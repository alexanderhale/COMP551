{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Reddit Comment Classification\n",
    "\n",
    "Outstanding TODOs:\n",
    "* incorporate meta-feature and POS tags into feature list\n",
    "* run tfidf with more computing power so that n-grams can be increased to 2 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                           comments  \\\n",
      "0          0  Honestly, Buffalo is the correct answer. I rem...   \n",
      "1          1  Ah yes way could have been :( remember when he...   \n",
      "2          2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...   \n",
      "3          3  He wouldn't have been a bad signing if we woul...   \n",
      "4          4  Easy. You use the piss and dry technique. Let ...   \n",
      "...      ...                                                ...   \n",
      "69995  69995  Thank you, you confirm Spain does have nice pe...   \n",
      "69996  69996  Imagine how many he would have killed with a r...   \n",
      "69997  69997  Yes. Only. As in the guy I was replying to was...   \n",
      "69998  69998  Looking for something light-hearted or has a v...   \n",
      "69999  69999  I love how I never cry about casters because I...   \n",
      "\n",
      "            subreddits  \n",
      "0               hockey  \n",
      "1                  nba  \n",
      "2      leagueoflegends  \n",
      "3               soccer  \n",
      "4                funny  \n",
      "...                ...  \n",
      "69995           europe  \n",
      "69996  leagueoflegends  \n",
      "69997           canada  \n",
      "69998            anime  \n",
      "69999  GlobalOffensive  \n",
      "\n",
      "[70000 rows x 3 columns]\n",
      "          id                                           comments\n",
      "0          0  Trout and Bryant have both led the league in s...\n",
      "1          1  &gt; Just like Estonians have good reasons to ...\n",
      "2          2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3          3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4          4         You have to take it out of the bag, Morty!\n",
      "...      ...                                                ...\n",
      "29995  29995  I have no idea what's going on this trailer an...\n",
      "29996  29996  I misread that at David Cross, and now I'm try...\n",
      "29997  29997  Well lets be reasonable next time and dont unb...\n",
      "29998  29998  Jaime dumping on Jon for going off to serve in...\n",
      "29999  29999  I think he'll be on par, but more mechanic tha...\n",
      "\n",
      "[30000 rows x 2 columns]\n",
      "https://youtu.be/6xxbBR8iSZ0?t=40m49s\n",
      "\n",
      "If you didn't find it already.\n",
      "\n",
      "Nothing out of the ordinary though, she just has eye constant eye contact.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "comment_data = pd.read_csv('../Data/reddit_train.csv')\n",
    "test_data = pd.read_csv('../Data/reddit_test.csv')\n",
    "print(comment_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup of Input Data\n",
    "Standardize the text snippets of the comments, preparing the comments for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                           comments  \\\n",
      "0          0  Honestly, Buffalo is the correct answer. I rem...   \n",
      "1          1  Ah yes way could have been :( remember when he...   \n",
      "2          2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...   \n",
      "3          3  He wouldn't have been a bad signing if we woul...   \n",
      "4          4  Easy. You use the piss and dry technique. Let ...   \n",
      "...      ...                                                ...   \n",
      "69995  69995  Thank you, you confirm Spain does have nice pe...   \n",
      "69996  69996  Imagine how many he would have killed with a r...   \n",
      "69997  69997  Yes. Only. As in the guy I was replying to was...   \n",
      "69998  69998  Looking for something light-hearted or has a v...   \n",
      "69999  69999  I love how I never cry about casters because I...   \n",
      "\n",
      "            subreddits                                               prep  \n",
      "0               hockey  honestly buffalo is the correct answer i remem...  \n",
      "1                  nba  ah yes way could have been remember when he wa...  \n",
      "2      leagueoflegends  wasurl if you didnt find it already nothing ou...  \n",
      "3               soccer  he wouldnt have been a bad signing if we would...  \n",
      "4                funny  easy you use the piss and dry technique let a ...  \n",
      "...                ...                                                ...  \n",
      "69995           europe  thank you you confirm spain doe have nice peop...  \n",
      "69996  leagueoflegends  imagine how many he would have killed with a r...  \n",
      "69997           canada  yes only a in the guy i wa replying to wa clai...  \n",
      "69998            anime  looking for something lighthearted or ha a ver...  \n",
      "69999  GlobalOffensive  i love how i never cry about caster because i ...  \n",
      "\n",
      "[70000 rows x 4 columns]\n",
      "          id                                           comments  \\\n",
      "0          0  Trout and Bryant have both led the league in s...   \n",
      "1          1  &gt; Just like Estonians have good reasons to ...   \n",
      "2          2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...   \n",
      "3          3  Moving Ostwald borders back to the pre 1967 bo...   \n",
      "4          4         You have to take it out of the bag, Morty!   \n",
      "...      ...                                                ...   \n",
      "29995  29995  I have no idea what's going on this trailer an...   \n",
      "29996  29996  I misread that at David Cross, and now I'm try...   \n",
      "29997  29997  Well lets be reasonable next time and dont unb...   \n",
      "29998  29998  Jaime dumping on Jon for going off to serve in...   \n",
      "29999  29999  I think he'll be on par, but more mechanic tha...   \n",
      "\n",
      "                                                    prep  \n",
      "0      trout and bryant have both led the league in s...  \n",
      "1      gt just like estonian have good reason to fear...  \n",
      "2      will sol_primeval sotp being oblivious find ou...  \n",
      "3      moving ostwald border back to the pre num bord...  \n",
      "4               you have to take it out of the bag morty  \n",
      "...                                                  ...  \n",
      "29995  i have no idea whats going on this trailer and...  \n",
      "29996  i misread that at david cross and now im tryin...  \n",
      "29997  well let be reasonable next time and dont unba...  \n",
      "29998  jaime dumping on jon for going off to serve in...  \n",
      "29999  i think hell be on par but more mechanic than ...  \n",
      "\n",
      "[30000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# punctuation removal\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "# lowercase\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "\n",
    "# convert numbers to 'num'\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "\n",
    "# replace links with 'wasurl'\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "\n",
    "# replace newlines and tabs with spaces\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "\n",
    "# fix any double spaces we created in the previous steps\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "# # typo correction (commented out because it takes too long)\n",
    "# spell = SpellChecker()\n",
    "# def spellcheck_col(row):\n",
    "#     return [spell.correction(word) for word in row]\n",
    "# comment_data['prep'] = comment_data.prep.apply(spellcheck_col)\n",
    "# test_data['prep'] = test_data.prep.apply(spellcheck_col)\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "comment_data['prep'] = comment_data.prep.apply(lemmatize_col)\n",
    "test_data['prep'] = test_data.prep.apply(lemmatize_col)\n",
    "\n",
    "print(comment_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Count, TF-IDF (with stopword removal), and Meta-Feature Extraction\n",
    "Use POS tagging, TF-IDF, and meta-feature extraction to generate feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix size: (70000, 68455)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "# tag comments\n",
    "# comment_data['comments_tagged'] = pos_tag_sents(comment_data['prep'].tolist())\n",
    "# test_data['comments_tagged'] = pos_tag_sents(test_data['prep'].tolist())\n",
    "# TODO count totals of each part of speech (noun, adjective, etc) and use the counts as features\n",
    "\n",
    "\n",
    "# in the bag of words matrix, remove punctuation and stopwords\n",
    "# count_vectorizer = CountVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,3))\n",
    "# counts = count_vectorizer.fit_transform(comment_data.comments)\n",
    "# print(\"raw word count matrix size: \" + str(counts.shape))\n",
    "# count_t_vectorizer = CountVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,3))\n",
    "# counts_t = count_t_vectorizer.fit_transform(test_data.comments)\n",
    "# print(\"raw word count matrix size: \" + str(counts_t.shape))\n",
    "\n",
    "\n",
    "# TF-IDF, also removing punctuation and stopwords\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,1))\n",
    "tfidf = tfidf_vectorizer.fit_transform(comment_data.prep)\n",
    "print(\"TF-IDF matrix size: \" + str(tfidf.shape))\n",
    "tfidf_t = tfidf_vectorizer.transform(test_data.prep)\n",
    "print(\"TF-IDF_t matrix size: \" + str(tfidf_t.shape))\n",
    "\n",
    "\n",
    "# meta-feature extraction\n",
    "# comment_data['word_count'] = comment_data['comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "# test_data['word_count'] = test_data['comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "# comment_data['char_count'] = comment_data['comments'].str.len()\n",
    "# test_data['char_count'] = test_data['comments'].str.len()\n",
    "\n",
    "# np.sparse.hstack((tfidf,comment_data['word_count'].to_numpy())[:,None]).A\n",
    "# np.sparse.hstack((tfidf,comment_data['char_count'].to_numpy())[:,None]).A\n",
    "# np.sparse.hstack((tfidf_t,test_data['word_count'].to_numpy())[:,None]).A\n",
    "# np.sparse.hstack((tfidf_t,test_data['char_count'].to_numpy())[:,None]).A\n",
    "\n",
    "# mfe = MFE(groups=[\"general\", \"statistical\", \"info-theory\"], summary=[\"min\", \"median\", \"max\"])\n",
    "# # TODO remove .head(1000) (currently included because memory limit doesn't allow computation of full list)\n",
    "# mfe.fit(comment_data['comments'].head(1000).tolist(), comment_data['subreddits'].head(1000).tolist())\n",
    "# ft = mfe.extract()\n",
    "# print(ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Export\n",
    "Dump the vectorizer, feature matrices, and dataframes to a pickle file in order for other group members to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-73985ef23a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data/vectorizer.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# feature matrices (load on the other side with scipy.sparse.load_npz())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# vectorizer\n",
    "with open('../Data/vectorizer.pk', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "# feature matrices (load on the other side with scipy.sparse.load_npz())\n",
    "scipy.sparse.save_npz('../Data/feature_matrix_train.npz', tfidf)\n",
    "scipy.sparse.save_npz('../Data/feature_matrix_test.npz', tfidf_t)\n",
    "\n",
    "# dataframes of comment data\n",
    "np.save('../Data/cleaned_data_train', comment_data.to_numpy())\n",
    "np.save('../Data/cleaned_data_test', test_data.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Pick the important information out of the large matrix generated above. Only necessary if the number of features is so high that the models don't have enough computation power to train, OR when some features are providing poor information and throwing off results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform PCA on the TF-IDF matrix\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# tsvd = TruncatedSVD(n_components=20)\n",
    "# reduced_features = tsvd.fit_transform(tfidf)\n",
    "# print(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# N = 2\n",
    "# for Product, category_id in sorted(category_to_id.items()):\n",
    "#   features_chi2 = chi2(tfidf, labels == category_id)\n",
    "#   indices = np.argsort(features_chi2[0])\n",
    "#   feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#   print(\"# '{}':\".format(Product))\n",
    "#   print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "#   print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "SciKit-Learn implementations of decision tree and Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# decision tree on test set\n",
    "d_tree = DecisionTreeClassifier(random_state=0).fit(tfidf, comment_data['subreddits'])\n",
    "d_tree_predict = d_tree.predict(tfidf_t)\n",
    "print(\"Decision Tree test set predictions:\")\n",
    "print(d_tree_predict)\n",
    "\n",
    "# niave bayes on test set\n",
    "nb = MultinomialNB().fit(tfidf, comment_data['subreddits'])\n",
    "nb_predict = nb.predict(tfidf_t)\n",
    "print(\"Naive Bayes test set predictions:\")\n",
    "print(nb_predict)\n",
    "\n",
    "# check accuracy using same training set values (overfitting, this is just a quick-and-dirty check)\n",
    "sanity_score = d_tree.score(tfidf, comment_data['subreddits'])\n",
    "print(\"Sanity check - decision tree accuracy on training set is: \" + str(sanity_score))\n",
    "\n",
    "# make a validation set to properly check accuracy\n",
    "X_train, X_val, y_train, y_val = train_test_split(comment_data['comments'], comment_data['subreddits'], random_state = 0)\n",
    "x_train_features = tfidf_vectorizer.transform(X_train)\n",
    "x_val_features = tfidf_vectorizer.transform(X_val)\n",
    "d_tree_val = DecisionTreeClassifier(random_state=0).fit(x_train_features, y_train)\n",
    "d_tree_score = d_tree_val.score(x_val_features, y_val)\n",
    "print(\"Decision tree validation score: \" + str(d_tree_score))\n",
    "nb_val = MultinomialNB().fit(x_train_features, y_train)\n",
    "nb_score = nb_val.score(x_val_features, y_val)\n",
    "print(\"Naive Bayes validation score: \" + str(nb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_val = RandomForestClassifier(n_estimators=10).fit(x_train_features, y_train)\n",
    "rf_score = rf_val.score(x_val_features, y_val)\n",
    "print(\"Random Forest validation score: \" + str(rf_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Classifiers\n",
    "To be used for the models implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch' 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']\n",
    "\n",
    "\n",
    "# default setup\n",
    "full_reference = np.eye(len(labels))\n",
    "partial_reference = np.append(np.zeros((1,len(labels)-1), dtype=np.int8),\\\n",
    "                              np.eye(len(labels)-1, dtype=np.int8), axis = 0)\n",
    "\n",
    "# encoder\n",
    "def encode(label, labels=labels, ref=full_reference):\n",
    "    location = labels.index(label)\n",
    "    return ref[location]\n",
    "\n",
    "\n",
    "# shapes\n",
    "print(full_reference.shape)\n",
    "print(partial_reference.shape)\n",
    "\n",
    "# example\n",
    "print(encode('hockey'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
