{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Reddit Comment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "   id                                           comments\n",
      "0   0  Trout and Bryant have both led the league in s...\n",
      "1   1  &gt; Just like Estonians have good reasons to ...\n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4   4         You have to take it out of the bag, Morty!\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "import pickle\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "\n",
    "# natural language toolkit\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# meta-feature extraction\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "# import data\n",
    "comment_data = pd.read_csv('../Data/reddit_train.csv')\n",
    "test_data = pd.read_csv('../Data/reddit_test.csv')\n",
    "print(comment_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Functions\n",
    "#### TF-IDF Vectorizer and Feature Selector\n",
    "Using only one vectorizer and selector for all preprocessing calls means that each feature matrix has the same number of features (which is required for the inputs of the classifier algorithms to match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(train_data, test_data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,2), min_df=2)\n",
    "    return tfidf_vectorizer.fit(pd.concat([train_data['prep'], test_data['prep']]))\n",
    "\n",
    "def get_selector(feature_map, classifiers):\n",
    "    return SelectPercentile(chi2, percentile=25).fit(feature_map, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-Processing\n",
    "Input: Pandas dataframe with a column called \"comments\" containing comments and a column called \"pos_tag\" with the words of those comments tagged with the corresponding part of speech\n",
    "\n",
    "Output: sparse matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for spelling correction\n",
    "tt = TweetTokenizer()\n",
    "spell = SpellChecker(distance=1)  # set distance to 1 instead of the default 2 to speed things up\n",
    "def spellcheck_col(row):\n",
    "    return \" \".join([spell.correction(word) for word in tt.tokenize(row)])\n",
    "\n",
    "# helper function for lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_col(row):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in tt.tokenize(row)])\n",
    "\n",
    "# helper function for part of speech tagging\n",
    "def pos_col(row):\n",
    "    return pos_tag(tt.tokenize(row))\n",
    "\n",
    "# helper function for average word count\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "# stopwords for stopword count\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# object for meta-feature extraction\n",
    "mfe = MFE(groups=[\"general\", \"statistical\", \"info-theory\"], summary=[\"min\", \"median\", \"max\"])\n",
    "\n",
    "def text_cleanup(data):\n",
    "    ##### CLEANUP OF INPUT DATA #####\n",
    "    # punctuation removal\n",
    "    data['prep'] = data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "    # lowercase\n",
    "    data['prep'] = data['prep'].str.lower()\n",
    "\n",
    "    # convert numbers to 'num'\n",
    "    data['prep'] = data['prep'].str.replace('(\\d+)', ' num ')\n",
    "\n",
    "    # replace links with 'wasurl'\n",
    "    data['prep'] = data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "\n",
    "    # replace newlines and tabs with spaces\n",
    "    data['prep'] = data['prep'].str.replace(r'\\s+', \" \")\n",
    "\n",
    "    # fix any double spaces we created in the previous steps\n",
    "    data['prep'] = data['prep'].str.replace(\" +\", \" \")\n",
    "    print(\"Superficial standardization complete\")\n",
    "\n",
    "    # typo correction\n",
    "    data['prep'] = data.prep.apply(spellcheck_col)\n",
    "    print(\"Typo correction complete\")\n",
    "\n",
    "    # lemmatization\n",
    "    data['prep'] = data.prep.apply(lemmatize_col)\n",
    "    print(\"Lemmatization complete\")\n",
    "    \n",
    "    # part-of-speech extraction\n",
    "    data['pos_tag'] = data.prep.apply(pos_col)\n",
    "    print(\"POS extraction complete\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocess(inFrame):\n",
    "    data = inFrame.copy(deep=True)\n",
    "    \n",
    "    ##### META-FEATURE EXTRACTION #####\n",
    "    # word count\n",
    "    data['word_count'] = data['comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    wc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['word_count'].to_numpy()))\n",
    "\n",
    "    # character count\n",
    "    data['char_count'] = data['comments'].str.len()\n",
    "    # cc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['char_count'].to_numpy()))\n",
    "    # TODO fix issue that is including NaNs in the char_count list\n",
    "\n",
    "    # average word length\n",
    "    data['avg_word'] = data['comments'].apply(lambda x: avg_word(x))\n",
    "    aw = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['avg_word'].to_numpy()))\n",
    "\n",
    "    # stopword count (stopwords will be removed later)\n",
    "    data['stop_count'] = data['comments'].apply(lambda x: len([x for x in x.split() if x in stopwords]))\n",
    "    sc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['stop_count'].to_numpy()))\n",
    "\n",
    "    # digit count\n",
    "    data['digit_count'] = data['comments'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    dc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['digit_count'].to_numpy()))\n",
    "\n",
    "    \n",
    "    ##### PART-OF-SPEECH COUNTING #####\n",
    "    # all the POS tags in NLTK's system\n",
    "    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    \n",
    "    # add a column in the Pandas DataFrame for each POS tag\n",
    "    for tag in pos_tags:\n",
    "        data[tag] = 0.0\n",
    "        \n",
    "    # iterate through the column that contains tagged comments\n",
    "    for idx, row in enumerate(data['pos_tag']):\n",
    "        # get the number of words in the column in this row\n",
    "        word_count = data['word_count'][idx]\n",
    "        \n",
    "        # count the number of each tag present in this comment\n",
    "        counts = Counter([j for i,j in row])\n",
    "        \n",
    "        # iterate through all of NLTK's tags\n",
    "        for tag in pos_tags:\n",
    "            # find the ratio of this POS tag : total words in this comment\n",
    "            pos_per_word = counts[tag] / word_count\n",
    "\n",
    "            # place the ratio in the dataframe column for this tag, at the row of this comment\n",
    "            data.at[idx, tag] = pos_per_word\n",
    "    \n",
    "    # create a sparse matrix out of the count data\n",
    "    pos = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data[pos_tags.pop()].to_numpy()))\n",
    "    for tag in pos_tags:\n",
    "        new_pos = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data[tag].to_numpy()))\n",
    "        pos = scipy.sparse.hstack((pos, new_pos))\n",
    "    \n",
    "    \n",
    "    ##### TF-IDF #####\n",
    "    tfidf = tfidf_vectorizer.transform(data.prep)\n",
    "    \n",
    "    \n",
    "    ##### FEATURE COMBINATION #####\n",
    "    feature_matrix = scipy.sparse.hstack((tfidf, wc, aw, sc, dc, pos))\n",
    "    \n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superficial standardization complete\n",
      "Typo correction complete\n",
      "Lemmatization complete\n",
      "POS extraction complete\n",
      "   id                                           comments       subreddits  \\\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey   \n",
      "1   1  Ah yes way could have been :( remember when he...              nba   \n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends   \n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer   \n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny   \n",
      "\n",
      "                                                prep  \\\n",
      "0  honestly buffalo is the correct answer i remem...   \n",
      "1  ah yes way could have been remember when he wa...   \n",
      "2  wasurl if you dint find it already nothing out...   \n",
      "3  he couldnt have been a bad signing if we could...   \n",
      "4  easy you use the piss and dry technique let a ...   \n",
      "\n",
      "                                             pos_tag  \n",
      "0  [(honestly, RB), (buffalo, NN), (is, VBZ), (th...  \n",
      "1  [(ah, JJ), (yes, NNS), (way, NN), (could, MD),...  \n",
      "2  [(wasurl, NN), (if, IN), (you, PRP), (dint, VB...  \n",
      "3  [(he, PRP), (couldnt, VBZ), (have, VBP), (been...  \n",
      "4  [(easy, RB), (you, PRP), (use, VBP), (the, DT)...  \n",
      "Superficial standardization complete\n",
      "Typo correction complete\n",
      "Lemmatization complete\n",
      "POS extraction complete\n",
      "   id                                           comments  \\\n",
      "0   0  Trout and Bryant have both led the league in s...   \n",
      "1   1  &gt; Just like Estonians have good reasons to ...   \n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...   \n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...   \n",
      "4   4         You have to take it out of the bag, Morty!   \n",
      "\n",
      "                                                prep  \\\n",
      "0  trout and bryant have both led the league in s...   \n",
      "1  gt just like estonian have good reason to fear...   \n",
      "2  will sol_primeval stop being oblivious find ou...   \n",
      "3  moving oswald border back to the pre num borde...   \n",
      "4           you have to take it out of the bag forty   \n",
      "\n",
      "                                             pos_tag  \n",
      "0  [(trout, NN), (and, CC), (bryant, NN), (have, ...  \n",
      "1  [(gt, NN), (just, RB), (like, IN), (estonian, ...  \n",
      "2  [(will, MD), (sol_primeval, VB), (stop, VB), (...  \n",
      "3  [(moving, VBG), (oswald, PRP), (border, VB), (...  \n",
      "4  [(you, PRP), (have, VBP), (to, TO), (take, VB)...  \n",
      "Full matrix shape:\n",
      "(70000, 215373)\n",
      "Test matrix shape:\n",
      "(30000, 215373)\n",
      "Reduced matrix shape:\n",
      "(70000, 53843)\n",
      "Reduced test matrix shape:\n",
      "(30000, 53843)\n"
     ]
    }
   ],
   "source": [
    "# clean up comments\n",
    "comment_data = text_cleanup(comment_data)\n",
    "print(comment_data.head())\n",
    "test_data = text_cleanup(test_data)\n",
    "print(test_data.head())\n",
    "\n",
    "# get tfidf vectorizer\n",
    "tfidf_vectorizer = get_vectorizer(comment_data, test_data)\n",
    "\n",
    "# preprocess training set\n",
    "full_matrix_train = preprocess(comment_data)\n",
    "print(\"Full matrix shape:\")\n",
    "print(full_matrix_train.shape)\n",
    "full_matrix_test = preprocess(test_data)\n",
    "print(\"Test matrix shape:\")\n",
    "print(full_matrix_test.shape)\n",
    "\n",
    "# reduce feature space\n",
    "feature_selector = get_selector(full_matrix_train, comment_data['subreddits'])\n",
    "reduced_feature_train = feature_selector.transform(full_matrix_train)\n",
    "print(\"Reduced matrix shape:\")\n",
    "print(reduced_feature_train.shape)\n",
    "reduced_feature_test = feature_selector.transform(full_matrix_test)\n",
    "print(\"Reduced test matrix shape:\")\n",
    "print(reduced_feature_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "SciKit-Learn implementations of classification models.\n",
    "\n",
    "#### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reduced_feature_train\n",
    "y = comment_data['subreddits']\n",
    "\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1, max_iter=1000)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = MultinomialNB()\n",
    "clf4 = DecisionTreeClassifier()\n",
    "clf5 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf6 = SVC(gamma='scale', kernel='rbf', probability=True)\n",
    "accuracies = [0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40 (+/- 0.01) [Random Forest]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf2, X, y, cv=5, scoring='accuracy')\n",
    "accuracies[1] = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"Random Forest\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_randomforest.pk', 'wb') as file:\n",
    "    pickle.dump(clf2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23 (+/- 0.00) [Multinomial Naive Bayes]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf3, X, y, cv=5, scoring='accuracy')\n",
    "accuracies[2] = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"Multinomial Naive Bayes\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_MNBayes.pk', 'wb') as file:\n",
    "    pickle.dump(clf3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25 (+/- 0.00) [Decision Tree]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf4, X, y, cv=5, scoring='accuracy')\n",
    "accuracies[3] = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"Decision Tree\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_DecisionTree.pk', 'wb') as file:\n",
    "    pickle.dump(clf4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.07 (+/- 0.00) [K-Nearest Neighbours]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf5, X, y, cv=5, scoring='accuracy')\n",
    "accuracies[4] = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"K-Nearest Neighbours\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_KNearest.pk', 'wb') as file:\n",
    "    pickle.dump(clf5, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf6, X, y, cv=5, scoring='accuracy')\n",
    "accuracies[5] = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"SVC\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_SVC.pk', 'wb') as file:\n",
    "    pickle.dump(clf6, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf1, X, y, cv=5, scoring='accuracy')\n",
    "accuracies[0] = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"Logistic Regression\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_logisticReg.pk', 'wb') as file:\n",
    "    pickle.dump(clf1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('mnb', clf3), ('dtc', clf4), ('knn', clf5), ('svc', clf6)], weights=accuracies)\n",
    "scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"Voting Classifier\"))\n",
    "\n",
    "# dump to pickle file\n",
    "with open('train_votingclass.pk', 'wb') as file:\n",
    "    pickle.dump(eclf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = eclf.predict(reduced_feature_test)\n",
    "\n",
    "# dump predictions to file\n",
    "with open('predictions.csv', mode='w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    writer.writerow(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Export\n",
    "Dump the vectorizer, selector, feature matrices, and dataframes to a files in order for other group members to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "with open('vectorizer.pk', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "    \n",
    "# selector\n",
    "with open('selector.pk', 'wb') as file:\n",
    "    pickle.dump(feature_selector, file)\n",
    "\n",
    "# feature matrices (load on the other side with scipy.sparse.load_npz())\n",
    "scipy.sparse.save_npz('feature_matrix_train.npz', full_matrix_train)\n",
    "scipy.sparse.save_npz('feature_matrix_test.npz', full_matrix_test)\n",
    "scipy.sparse.save_npz('reduced_matrix_train.npz', full_matrix_train)\n",
    "scipy.sparse.save_npz('reduced_matrix_test.npz', full_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Classifiers\n",
    "To be used for the models implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 19)\n",
      "(19, 18)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# categories\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch' 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']\n",
    "\n",
    "\n",
    "# default setup\n",
    "full_reference = np.eye(len(labels))\n",
    "partial_reference = np.append(np.zeros((1,len(labels)-1), dtype=np.int8),\\\n",
    "                              np.eye(len(labels)-1, dtype=np.int8), axis = 0)\n",
    "\n",
    "# encoder\n",
    "def encode(label, labels=labels, ref=full_reference):\n",
    "    location = labels.index(label)\n",
    "    return ref[location]\n",
    "\n",
    "\n",
    "# shapes\n",
    "print(full_reference.shape)\n",
    "print(partial_reference.shape)\n",
    "\n",
    "# example\n",
    "print(encode('hockey'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
