{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Reddit Comment Classification\n",
    "\n",
    "Outstanding TODOs:\n",
    "* incorporate meta-feature and POS tags into feature list\n",
    "* run tfidf with more computing power so that n-grams can be increased to 2 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "   id                                           comments\n",
      "0   0  Trout and Bryant have both led the league in s...\n",
      "1   1  &gt; Just like Estonians have good reasons to ...\n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4   4         You have to take it out of the bag, Morty!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "comment_data = pd.read_csv('../Data/reddit_train.csv')\n",
    "test_data = pd.read_csv('../Data/reddit_test.csv')\n",
    "print(comment_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup of Input Data\n",
    "Standardize the text snippets of the comments, preparing the comments for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                           comments  \\\n",
      "0          0  Honestly, Buffalo is the correct answer. I rem...   \n",
      "1          1  Ah yes way could have been :( remember when he...   \n",
      "2          2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...   \n",
      "3          3  He wouldn't have been a bad signing if we woul...   \n",
      "4          4  Easy. You use the piss and dry technique. Let ...   \n",
      "...      ...                                                ...   \n",
      "69995  69995  Thank you, you confirm Spain does have nice pe...   \n",
      "69996  69996  Imagine how many he would have killed with a r...   \n",
      "69997  69997  Yes. Only. As in the guy I was replying to was...   \n",
      "69998  69998  Looking for something light-hearted or has a v...   \n",
      "69999  69999  I love how I never cry about casters because I...   \n",
      "\n",
      "            subreddits                                               prep  \n",
      "0               hockey  honestly buffalo is the correct answer i remem...  \n",
      "1                  nba  ah yes way could have been remember when he wa...  \n",
      "2      leagueoflegends  wasurl if you didnt find it already nothing ou...  \n",
      "3               soccer  he wouldnt have been a bad signing if we would...  \n",
      "4                funny  easy you use the piss and dry technique let a ...  \n",
      "...                ...                                                ...  \n",
      "69995           europe  thank you you confirm spain doe have nice peop...  \n",
      "69996  leagueoflegends  imagine how many he would have killed with a r...  \n",
      "69997           canada  yes only a in the guy i wa replying to wa clai...  \n",
      "69998            anime  looking for something lighthearted or ha a ver...  \n",
      "69999  GlobalOffensive  i love how i never cry about caster because i ...  \n",
      "\n",
      "[70000 rows x 4 columns]\n",
      "          id                                           comments  \\\n",
      "0          0  Trout and Bryant have both led the league in s...   \n",
      "1          1  &gt; Just like Estonians have good reasons to ...   \n",
      "2          2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...   \n",
      "3          3  Moving Ostwald borders back to the pre 1967 bo...   \n",
      "4          4         You have to take it out of the bag, Morty!   \n",
      "...      ...                                                ...   \n",
      "29995  29995  I have no idea what's going on this trailer an...   \n",
      "29996  29996  I misread that at David Cross, and now I'm try...   \n",
      "29997  29997  Well lets be reasonable next time and dont unb...   \n",
      "29998  29998  Jaime dumping on Jon for going off to serve in...   \n",
      "29999  29999  I think he'll be on par, but more mechanic tha...   \n",
      "\n",
      "                                                    prep  \n",
      "0      trout and bryant have both led the league in s...  \n",
      "1      gt just like estonian have good reason to fear...  \n",
      "2      will sol_primeval sotp being oblivious find ou...  \n",
      "3      moving ostwald border back to the pre num bord...  \n",
      "4               you have to take it out of the bag morty  \n",
      "...                                                  ...  \n",
      "29995  i have no idea whats going on this trailer and...  \n",
      "29996  i misread that at david cross and now im tryin...  \n",
      "29997  well let be reasonable next time and dont unba...  \n",
      "29998  jaime dumping on jon for going off to serve in...  \n",
      "29999  i think hell be on par but more mechanic than ...  \n",
      "\n",
      "[30000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# punctuation removal\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "# lowercase\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "\n",
    "# convert numbers to 'num'\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "\n",
    "# replace links with 'wasurl'\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "\n",
    "# replace newlines and tabs with spaces\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "\n",
    "# fix any double spaces we created in the previous steps\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "# # typo correction (TODO commented out because it takes too long)\n",
    "# spell = SpellChecker()\n",
    "# def spellcheck_col(row):\n",
    "#     return [spell.correction(word) for word in row]\n",
    "# comment_data['prep'] = comment_data.prep.apply(spellcheck_col)\n",
    "# test_data['prep'] = test_data.prep.apply(spellcheck_col)\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "comment_data['prep'] = comment_data.prep.apply(lemmatize_col)\n",
    "test_data['prep'] = test_data.prep.apply(lemmatize_col)\n",
    "\n",
    "print(comment_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Count, TF-IDF (with stopword removal), and Meta-Feature Extraction\n",
    "Use POS tagging, TF-IDF, and meta-feature extraction to generate feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits  \\\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey   \n",
      "1   1  Ah yes way could have been :( remember when he...              nba   \n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends   \n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer   \n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny   \n",
      "\n",
      "                                                prep  word_count  char_count  \\\n",
      "0  honestly buffalo is the correct answer i remem...          58         357   \n",
      "1  ah yes way could have been remember when he wa...          29         145   \n",
      "2  wasurl if you didnt find it already nothing ou...          18         145   \n",
      "3  he wouldnt have been a bad signing if we would...          24         123   \n",
      "4  easy you use the piss and dry technique let a ...          46         212   \n",
      "\n",
      "   avg_word  stop_count  digit_count  \n",
      "0  5.245614          20            1  \n",
      "1  4.034483          12            0  \n",
      "2  6.200000           9            0  \n",
      "3  4.347826          12            0  \n",
      "4  3.711111          17            0  \n",
      "   id                                           comments  \\\n",
      "0   0  Trout and Bryant have both led the league in s...   \n",
      "1   1  &gt; Just like Estonians have good reasons to ...   \n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...   \n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...   \n",
      "4   4         You have to take it out of the bag, Morty!   \n",
      "\n",
      "                                                prep  word_count  char_count  \\\n",
      "0  trout and bryant have both led the league in s...          58         164   \n",
      "1  gt just like estonian have good reason to fear...          29         237   \n",
      "2  will sol_primeval sotp being oblivious find ou...          18          70   \n",
      "3  moving ostwald border back to the pre num bord...          24         903   \n",
      "4           you have to take it out of the bag morty          46          42   \n",
      "\n",
      "   avg_word  stop_count  digit_count  \n",
      "0  4.689655          11            0  \n",
      "1  5.583333           9            1  \n",
      "2  5.272727           4            0  \n",
      "3  4.857143          63            2  \n",
      "4  3.300000           6            0  \n"
     ]
    }
   ],
   "source": [
    "# meta-feature extraction\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "# word count\n",
    "comment_data['word_count'] = comment_data['comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "test_data['word_count'] = comment_data['comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# character count\n",
    "comment_data['char_count'] = comment_data['comments'].str.len()\n",
    "test_data['char_count'] = test_data['comments'].str.len()\n",
    "\n",
    "# average word length\n",
    "comment_data['avg_word'] = comment_data['comments'].apply(lambda x: avg_word(x))\n",
    "test_data['avg_word'] = test_data['comments'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# stopword count (stopwords will be removed later)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "comment_data['stop_count'] = comment_data['comments'].apply(lambda x: len([x for x in x.split() if x in stopwords]))\n",
    "test_data['stop_count'] = test_data['comments'].apply(lambda x: len([x for x in x.split() if x in stopwords]))\n",
    "\n",
    "# digit count\n",
    "comment_data['digit_count'] = comment_data['comments'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "test_data['digit_count'] = test_data['comments'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "# mfe = MFE(groups=[\"general\", \"statistical\", \"info-theory\"], summary=[\"min\", \"median\", \"max\"])\n",
    "# # TODO remove .head(1000) (currently included because memory limit doesn't allow computation of full list)\n",
    "# mfe.fit(comment_data['comments'].head(1000).tolist(), comment_data['subreddits'].head(1000).tolist())\n",
    "# ft = mfe.extract()\n",
    "# print(ft)\n",
    "\n",
    "print(comment_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging\n",
    "# comment_data['comments_tagged'] = pos_tag_sents(comment_data['prep'].tolist())\n",
    "# test_data['comments_tagged'] = pos_tag_sents(test_data['prep'].tolist())\n",
    "# TODO count totals of each part of speech (noun, adjective, etc) and use the counts as features\n",
    "\n",
    "# print(comment_data.head())\n",
    "# print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix size: (70000, 927355)\n",
      "TF-IDF_t matrix size: (30000, 927355)\n"
     ]
    }
   ],
   "source": [
    "# in the bag of words matrix, remove punctuation and stopwords\n",
    "# count_vectorizer = CountVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,3))\n",
    "# counts = count_vectorizer.fit_transform(comment_data.comments)\n",
    "# print(\"raw word count matrix size: \" + str(counts.shape))\n",
    "# count_t_vectorizer = CountVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,3))\n",
    "# counts_t = count_t_vectorizer.fit_transform(test_data.comments)\n",
    "# print(\"raw word count matrix size: \" + str(counts_t.shape))\n",
    "\n",
    "# TF-IDF, also removing punctuation and stopwords\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(comment_data.prep)\n",
    "print(\"TF-IDF matrix size: \" + str(tfidf.shape))\n",
    "tfidf_t = tfidf_vectorizer.transform(test_data.prep)\n",
    "print(\"TF-IDF_t matrix size: \" + str(tfidf_t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix size: (70000, 927359)\n",
      "Test matrix size: (30000, 927359)\n"
     ]
    }
   ],
   "source": [
    "# append extra features to TFIDF matrix\n",
    "c_wc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(comment_data['word_count'].to_numpy()))\n",
    "# c_cc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(comment_data['char_count'].to_numpy()))\n",
    "c_aw = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(comment_data['avg_word'].to_numpy()))\n",
    "c_sc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(comment_data['stop_count'].to_numpy()))\n",
    "c_dc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(comment_data['digit_count'].to_numpy()))\n",
    "t_wc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(test_data['word_count'].to_numpy()))\n",
    "# t_cc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(test_data['char_count'].to_numpy()))\n",
    "t_aw = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(test_data['avg_word'].to_numpy()))\n",
    "t_sc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(test_data['stop_count'].to_numpy()))\n",
    "t_dc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(test_data['digit_count'].to_numpy()))\n",
    "\n",
    "# feature_matrix = scipy.sparse.hstack((tfidf, c_wc, c_cc, c_aw, c_sc, c_dc))\n",
    "# feature_matrix_t = scipy.sparse.hstack((tfidf_t, t_wc, t_cc, t_aw, t_sc, t_dc))\n",
    "feature_matrix = scipy.sparse.hstack((tfidf, c_wc, c_aw, c_sc, c_dc))\n",
    "feature_matrix_t = scipy.sparse.hstack((tfidf_t, t_wc, t_aw, t_sc, t_dc))\n",
    "\n",
    "# TODO fix NaN values in c_cc and t_cc\n",
    "\n",
    "print(\"Training matrix size: \" + str(feature_matrix.shape))\n",
    "print(\"Test matrix size: \" + str(feature_matrix_t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Export\n",
    "Dump the vectorizer, feature matrices, and dataframes to a pickle file in order for other group members to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 9)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# vectorizer\n",
    "with open('../Data/vectorizer.pk', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "# feature matrices (load on the other side with scipy.sparse.load_npz())\n",
    "scipy.sparse.save_npz('../Data/feature_matrix_train.npz', feature_matrix)\n",
    "scipy.sparse.save_npz('../Data/feature_matrix_test.npz', feature_matrix_t)\n",
    "\n",
    "# dataframes of comment data\n",
    "cd = comment_data.to_numpy()\n",
    "print(cd.shape)\n",
    "np.save('../Data/cleaned_data_train.npy', comment_data.to_numpy())\n",
    "np.save('../Data/cleaned_data_test.npy', test_data.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Pick the important information out of the large matrix generated above. Only necessary if the number of features is so high that the models don't have enough computation power to train, OR when some features are providing poor information and throwing off results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform PCA on the TF-IDF matrix\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# tsvd = TruncatedSVD(n_components=20)\n",
    "# reduced_features = tsvd.fit_transform(tfidf)\n",
    "# print(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# N = 2\n",
    "# for Product, category_id in sorted(category_to_id.items()):\n",
    "#   features_chi2 = chi2(tfidf, labels == category_id)\n",
    "#   indices = np.argsort(features_chi2[0])\n",
    "#   feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#   print(\"# '{}':\".format(Product))\n",
    "#   print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "#   print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "SciKit-Learn implementations of decision tree and Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree test set predictions:\n",
      "['soccer' 'worldnews' 'movies' ... 'Overwatch' 'gameofthrones' 'wow']\n",
      "Naive Bayes test set predictions:\n",
      "['Music' 'Music' 'Music' ... 'Music' 'Music' 'Music']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# decision tree on test set\n",
    "d_tree = DecisionTreeClassifier(random_state=0).fit(feature_matrix, comment_data['subreddits'])\n",
    "d_tree_predict = d_tree.predict(feature_matrix_t)\n",
    "print(\"Decision Tree test set predictions:\")\n",
    "print(d_tree_predict)\n",
    "\n",
    "# niave bayes on test set\n",
    "nb = MultinomialNB().fit(feature_matrix, comment_data['subreddits'])\n",
    "nb_predict = nb.predict(feature_matrix_t)\n",
    "print(\"Naive Bayes test set predictions:\")\n",
    "print(nb_predict)\n",
    "\n",
    "# random forest classifier on test set\n",
    "rf = RandomForestClassifier().fit(feature_matrix, comment_data['subreddits'])\n",
    "rf_predict = rf.predict(feature_matrix_t)\n",
    "print(\"Random Forest test set predictions:\")\n",
    "print(rf_predict)\n",
    "\n",
    "# check accuracy using same training set values (overfitting, this is just a quick-and-dirty check)\n",
    "sanity_score = d_tree.score(feature_matrix, comment_data['subreddits'])\n",
    "print(\"Sanity check - decision tree accuracy on training set is: \" + str(sanity_score))\n",
    "\n",
    "\n",
    "#### TODO run the train-test split data through proper pre-processing steps ####\n",
    "# make a validation set to properly check accuracy\n",
    "X_train, X_val, y_train, y_val = train_test_split(comment_data['comments'], comment_data['subreddits'], random_state = 0)\n",
    "x_train_features = tfidf_vectorizer.transform(X_train)\n",
    "x_val_features = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "d_tree_val = DecisionTreeClassifier(random_state=0).fit(x_train_features, y_train)\n",
    "d_tree_score = d_tree_val.score(x_val_features, y_val)\n",
    "print(\"Decision tree validation score: \" + str(d_tree_score))\n",
    "\n",
    "nb_val = MultinomialNB().fit(x_train_features, y_train)\n",
    "nb_score = nb_val.score(x_val_features, y_val)\n",
    "print(\"Naive Bayes validation score: \" + str(nb_score))\n",
    "\n",
    "rf_val = RandomForestClassifier().fit(x_train_features, y_train)\n",
    "rf_score = rf_val.score(x_val_features, y_val)\n",
    "print(\"Random Forest validation score: \" + str(rf_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Classifiers\n",
    "To be used for the models implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch' 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']\n",
    "\n",
    "\n",
    "# default setup\n",
    "full_reference = np.eye(len(labels))\n",
    "partial_reference = np.append(np.zeros((1,len(labels)-1), dtype=np.int8),\\\n",
    "                              np.eye(len(labels)-1, dtype=np.int8), axis = 0)\n",
    "\n",
    "# encoder\n",
    "def encode(label, labels=labels, ref=full_reference):\n",
    "    location = labels.index(label)\n",
    "    return ref[location]\n",
    "\n",
    "\n",
    "# shapes\n",
    "print(full_reference.shape)\n",
    "print(partial_reference.shape)\n",
    "\n",
    "# example\n",
    "print(encode('hockey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
