{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for Reddit Comment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "   id                                           comments\n",
      "0   0  Trout and Bryant have both led the league in s...\n",
      "1   1  &gt; Just like Estonians have good reasons to ...\n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4   4         You have to take it out of the bag, Morty!\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "\n",
    "# natural language toolkit\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# meta-feature extraction\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "# import data\n",
    "comment_data = pd.read_csv('../Data/reddit_train.csv')\n",
    "test_data = pd.read_csv('../Data/reddit_test.csv')\n",
    "print(comment_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Functions\n",
    "#### TF-IDF Vectorizer\n",
    "Using only one vectorizer for all preprocessing calls means that each feature matrix has the same number of features (which is required for the inputs of the classifier algorithms to match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(train_data, test_data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,2), min_df=2)\n",
    "    tfidf_vectorizer.fit(pd.concat([train_data['prep'], test_data['prep']]))\n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-Processing\n",
    "Input: Pandas dataframe with a column called \"comments\" containing comments\n",
    "\n",
    "Output: sparse matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for spelling correction\n",
    "tt = TweetTokenizer()\n",
    "spell = SpellChecker(distance=1)  # set distance to 1 instead of the default 2 to speed things up\n",
    "def spellcheck_col(row):\n",
    "    return \" \".join([spell.correction(word) for word in tt.tokenize(row)])\n",
    "\n",
    "# helper function for lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_col(row):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in tt.tokenize(row)])\n",
    "\n",
    "# helper function for average word count\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "# stopwords for stopword count\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# object for meta-feature extraction\n",
    "mfe = MFE(groups=[\"general\", \"statistical\", \"info-theory\"], summary=[\"min\", \"median\", \"max\"])\n",
    "\n",
    "def text_cleanup(data):\n",
    "    ##### CLEANUP OF INPUT DATA #####\n",
    "    # punctuation removal\n",
    "    data['prep'] = data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "    # lowercase\n",
    "    data['prep'] = data['prep'].str.lower()\n",
    "\n",
    "    # convert numbers to 'num'\n",
    "    data['prep'] = data['prep'].str.replace('(\\d+)', ' num ')\n",
    "\n",
    "    # replace links with 'wasurl'\n",
    "    data['prep'] = data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "\n",
    "    # replace newlines and tabs with spaces\n",
    "    data['prep'] = data['prep'].str.replace(r'\\s+', \" \")\n",
    "\n",
    "    # fix any double spaces we created in the previous steps\n",
    "    data['prep'] = data['prep'].str.replace(\" +\", \" \")\n",
    "    print(\"Superficial standardization complete\")\n",
    "\n",
    "    # typo correction\n",
    "    data['prep'] = data.prep.apply(spellcheck_col)\n",
    "    print(\"Typo correction complete\")\n",
    "\n",
    "    # lemmatization\n",
    "    data['prep'] = data.prep.apply(lemmatize_col)\n",
    "    print(\"Lemmatization complete\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocess(inFrame):\n",
    "    data = inFrame.copy(deep=True)\n",
    "    \n",
    "    ##### META-FEATURE EXTRACTION #####\n",
    "    # word count\n",
    "    data['word_count'] = data['comments'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    wc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['word_count'].to_numpy()))\n",
    "\n",
    "    # character count\n",
    "    data['char_count'] = data['comments'].str.len()\n",
    "    # cc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['char_count'].to_numpy()))\n",
    "    # TODO fix issue that is including NaNs in the char_count list\n",
    "\n",
    "    # average word length\n",
    "    data['avg_word'] = data['comments'].apply(lambda x: avg_word(x))\n",
    "    aw = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['avg_word'].to_numpy()))\n",
    "\n",
    "    # stopword count (stopwords will be removed later)\n",
    "    data['stop_count'] = data['comments'].apply(lambda x: len([x for x in x.split() if x in stopwords]))\n",
    "    sc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['stop_count'].to_numpy()))\n",
    "\n",
    "    # digit count\n",
    "    data['digit_count'] = data['comments'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    dc = scipy.sparse.csr_matrix.transpose(scipy.sparse.csr_matrix(data['digit_count'].to_numpy()))\n",
    "    \n",
    "    # mathematical meta-feature extraction\n",
    "    # mfe.fit(comment_data['comments'].tolist(), comment_data['subreddits'].tolist())\n",
    "    # ft = mfe.extract()\n",
    "    \n",
    "    \n",
    "    ##### PART-OF-SPEECH TAGGING #####\n",
    "    # data['pos_tag'] = pos_tag_sents(data['prep'].tolist())\n",
    "    # TODO count totals of each part of speech (noun, adjective, etc) and use the counts as features\n",
    "    \n",
    "    \n",
    "    ##### TF-IDF #####\n",
    "    tfidf = tfidf_vectorizer.transform(data.prep)\n",
    "    \n",
    "    \n",
    "    ##### FEATURE COMBINATION #####\n",
    "    feature_matrix = scipy.sparse.hstack((tfidf, wc, aw, sc, dc))\n",
    "    \n",
    "    \n",
    "    ##### FEATURE SELECTION #####\n",
    "    # TODO if necessary, reduce the number of features by selecting the most informative ones\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superficial standardization complete\n",
      "Typo correction complete\n",
      "Lemmatization complete\n",
      "   id                                           comments       subreddits  \\\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey   \n",
      "1   1  Ah yes way could have been :( remember when he...              nba   \n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends   \n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer   \n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny   \n",
      "\n",
      "                                                prep  \n",
      "0  honestly buffalo is the correct answer i remem...  \n",
      "1  ah yes way could have been remember when he wa...  \n",
      "2  wasurl if you dint find it already nothing out...  \n",
      "3  he couldnt have been a bad signing if we could...  \n",
      "4  easy you use the piss and dry technique let a ...  \n",
      "Superficial standardization complete\n",
      "Typo correction complete\n",
      "Lemmatization complete\n",
      "   id                                           comments  \\\n",
      "0   0  Trout and Bryant have both led the league in s...   \n",
      "1   1  &gt; Just like Estonians have good reasons to ...   \n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...   \n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...   \n",
      "4   4         You have to take it out of the bag, Morty!   \n",
      "\n",
      "                                                prep  \n",
      "0  trout and bryant have both led the league in s...  \n",
      "1  gt just like estonian have good reason to fear...  \n",
      "2  will sol_primeval stop being oblivious find ou...  \n",
      "3  moving oswald border back to the pre num borde...  \n",
      "4           you have to take it out of the bag forty  \n",
      "Full matrix shape:\n",
      "(70000, 215338)\n",
      "Test matrix shape:\n",
      "(30000, 215338)\n",
      "Training matrix shape:\n",
      "(55000, 215338)\n",
      "Validation matrix shape:\n",
      "(15000, 215338)\n"
     ]
    }
   ],
   "source": [
    "# clean up comments\n",
    "comment_data = text_cleanup(comment_data)\n",
    "print(comment_data.head())\n",
    "test_data = text_cleanup(test_data)\n",
    "print(test_data.head())\n",
    "\n",
    "# get tfidf vectorizer\n",
    "tfidf_vectorizer = get_vectorizer(comment_data, test_data)\n",
    "\n",
    "# whole training set (for use when making predictions for competition submission)\n",
    "full_matrix_train = preprocess(comment_data)\n",
    "print(\"Full matrix shape:\")\n",
    "print(full_matrix_train.shape)\n",
    "full_matrix_test = preprocess(test_data)\n",
    "print(\"Test matrix shape:\")\n",
    "print(full_matrix_test.shape)\n",
    "\n",
    "# split up training set (for use when evaluating model accuracies)\n",
    "X_train = preprocess(comment_data.head(55000))\n",
    "print(\"Training matrix shape:\")\n",
    "print(X_train.shape)\n",
    "X_val = preprocess(comment_data.tail(15000))\n",
    "print(\"Validation matrix shape:\")\n",
    "print(X_val.shape)\n",
    "y_train = comment_data['subreddits'].head(55000)\n",
    "y_val = comment_data['subreddits'].tail(15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "SciKit-Learn implementations of decision tree and Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree validation score: 0.2658\n",
      "Naive Bayes validation score: 0.05753333333333333\n",
      "Random Forest validation score: 0.42793333333333333\n",
      "Decision Tree test set predictions:\n",
      "['nfl' 'europe' 'gameofthrones' ... 'GlobalOffensive' 'gameofthrones'\n",
      " 'wow']\n",
      "Naive Bayes test set predictions:\n",
      "['Music' 'Music' 'anime' ... 'Music' 'Music' 'Music']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest test set predictions:\n",
      "['baseball' 'GlobalOffensive' 'AskReddit' ... 'Overwatch' 'gameofthrones'\n",
      " 'wow']\n",
      "Sanity check - decision tree accuracy on training set is: 0.9997285714285714\n"
     ]
    }
   ],
   "source": [
    "##### ACCURACY CHECK - TRAIN ON TRAINING SET, VALIDATE ON VALIDATION SET #####\n",
    "d_tree_val = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "d_tree_score = d_tree_val.score(X_val, y_val)\n",
    "print(\"Decision tree validation score: \" + str(d_tree_score))\n",
    "\n",
    "nb_val = MultinomialNB().fit(X_train, y_train)\n",
    "nb_score = nb_val.score(X_val, y_val)\n",
    "print(\"Naive Bayes validation score: \" + str(nb_score))\n",
    "\n",
    "rf_val = RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\n",
    "rf_score = rf_val.score(X_val, y_val)\n",
    "print(\"Random Forest validation score: \" + str(rf_score))\n",
    "\n",
    "\n",
    "##### PREDICTIONS - TRAINING ON FULL TRAINING SET, MAKE PREDICTIONS ON TEST SET #####\n",
    "d_tree = DecisionTreeClassifier(random_state=0).fit(full_matrix_train, comment_data['subreddits'])\n",
    "d_tree_predict = d_tree.predict(full_matrix_test)\n",
    "print(\"Decision Tree test set predictions:\")\n",
    "print(d_tree_predict)\n",
    "\n",
    "nb = MultinomialNB().fit(full_matrix_train, comment_data['subreddits'])\n",
    "nb_predict = nb.predict(full_matrix_test)\n",
    "print(\"Naive Bayes test set predictions:\")\n",
    "print(nb_predict)\n",
    "\n",
    "rf = RandomForestClassifier().fit(full_matrix_train, comment_data['subreddits'])\n",
    "rf_predict = rf.predict(full_matrix_test)\n",
    "print(\"Random Forest test set predictions:\")\n",
    "print(rf_predict)\n",
    "\n",
    "\n",
    "# check accuracy using same training set values (overfitting, this is just a quick-and-dirty check)\n",
    "sanity_score = d_tree.score(full_matrix_train, comment_data['subreddits'])\n",
    "print(\"Sanity check - decision tree accuracy on training set is: \" + str(sanity_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Export\n",
    "Dump the vectorizer, feature matrices, and dataframes to a pickle file in order for other group members to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# vectorizer\n",
    "with open('../Data/vectorizer.pk', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "# feature matrices (load on the other side with scipy.sparse.load_npz())\n",
    "scipy.sparse.save_npz('feature_matrix_train.npz', full_matrix_train)\n",
    "scipy.sparse.save_npz('feature_matrix_test.npz', full_matrix_test)\n",
    "\n",
    "# dump predictions to file\n",
    "with open('predictions.csv', mode='w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    writer.writerow(d_tree_predict)\n",
    "    writer.writerow(nb_predict)\n",
    "    writer.writerow(rf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "These are potential approaches to the Feature Selection section of the preprocessing section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform PCA on the TF-IDF matrix\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# tsvd = TruncatedSVD(n_components=20)\n",
    "# reduced_features = tsvd.fit_transform(tfidf)\n",
    "# print(tsvd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# N = 2\n",
    "# for Product, category_id in sorted(category_to_id.items()):\n",
    "#   features_chi2 = chi2(tfidf, labels == category_id)\n",
    "#   indices = np.argsort(features_chi2[0])\n",
    "#   feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#   print(\"# '{}':\".format(Product))\n",
    "#   print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "#   print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding of Classifiers\n",
    "To be used for the models implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 19)\n",
      "(19, 18)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# categories\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch' 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']\n",
    "\n",
    "\n",
    "# default setup\n",
    "full_reference = np.eye(len(labels))\n",
    "partial_reference = np.append(np.zeros((1,len(labels)-1), dtype=np.int8),\\\n",
    "                              np.eye(len(labels)-1, dtype=np.int8), axis = 0)\n",
    "\n",
    "# encoder\n",
    "def encode(label, labels=labels, ref=full_reference):\n",
    "    location = labels.index(label)\n",
    "    return ref[location]\n",
    "\n",
    "\n",
    "# shapes\n",
    "print(full_reference.shape)\n",
    "print(partial_reference.shape)\n",
    "\n",
    "# example\n",
    "print(encode('hockey'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
