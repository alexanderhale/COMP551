{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_path = './Data/reddit_test.csv'\n",
    "\n",
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#clean\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "#load\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "#clean\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "\n",
    "comment_data['prep'] = comment_data['prep'].apply(lemmatize_col)\n",
    "test_data['prep'] = comment_data['prep'].apply(lemmatize_col)\n",
    "\n",
    "# stopwords\n",
    "stop = stopwords.words('english')\n",
    "comment_data['prep'] = comment_data['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "test_data['prep'] = test_data['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2)\n"
     ]
    }
   ],
   "source": [
    "clean_data = comment_data['prep'].to_numpy()\n",
    "clean_labels = comment_data['subreddits'].to_numpy()\n",
    "\n",
    "train_comments = []\n",
    "for idx in range(clean_data.shape[0]):\n",
    "    item = (clean_data[idx], clean_labels[idx])\n",
    "    train_comments.append(item)\n",
    "train_comments = np.asarray(train_comments)\n",
    "print(train_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000/10000\n",
    "training_data = clean_data[:60000]\n",
    "testing_data = clean_data[60000:]\n",
    "training_labels = clean_labels[:60000]\n",
    "testing_labels = clean_labels[60000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "#     ('svm', svm_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = text_clf.predict(testing_data)\n",
    "\n",
    "labels = np.unique(training_labels).tolist()\n",
    "diagonal = np.eye(len(labels))\n",
    "\n",
    "values = []\n",
    "\n",
    "for idx,result in enumerate(results):\n",
    "    values.append(diagonal[labels.index(result)])\n",
    "\n",
    "values = np.array(values)\n",
    "\n",
    "svm_clf = svm.SVC(kernel='poly')\n",
    "svm_clf.fit(values, training_labels)\n",
    "\n",
    "# correct = 0\n",
    "# for idx, result in enumerate(results):\n",
    "#     if (result) == training_labels[idx]:\n",
    "#         correct += 1\n",
    "\n",
    "# print(correct/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0485\n"
     ]
    }
   ],
   "source": [
    "# results = text_clf.predict(testing_data)\n",
    "\n",
    "# values = []\n",
    "\n",
    "# for idx,result in enumerate(results):\n",
    "#     values.append(diagonal[labels.index(result)])\n",
    "\n",
    "# values = np.array(values)\n",
    "\n",
    "# predictions = svm_clf.predict(values)\n",
    "\n",
    "# correct = 0\n",
    "# for idx, result in enumerate(predictions):\n",
    "#     if (result) == testing_labels[idx]:\n",
    "#         correct += 1\n",
    "\n",
    "# print(correct/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
