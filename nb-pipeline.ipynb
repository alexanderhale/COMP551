{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes():\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.alpha = alpha # used for Laplace smoothing\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "    \n",
    "    def __group_samples(self, X,Y):\n",
    "        # labels -> numbers\n",
    "        labels = []\n",
    "        for label in Y:\n",
    "            labels.append(list(self.classes).index(label))\n",
    "        print(labels[:10])\n",
    "        # append X|Y arrays\n",
    "        XY = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "        XY[:,:X.shape[1]] = X\n",
    "        XY[:,-1:] = np.array(labels).reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        # initialize array of empty arrays with length of number of classes\n",
    "        group_by_class = [[] for _ in range(self.classes.shape[0])]\n",
    "        \n",
    "        # for each class, append an X|y sample into array index i if y == classes[i]\n",
    "        for class_index in range(self.classes.shape[0]):\n",
    "            for sample in XY:\n",
    "                if sample[-1] == class_index:\n",
    "                    group_by_class[class_index].append(sample)\n",
    "        return group_by_class\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "        num_samples = X.shape[0]\n",
    "        groups = self.__group_samples(X,Y)\n",
    "\n",
    "        self.priors = np.array(list(map(lambda g: np.log(len(g)/num_samples), groups)))\n",
    "\n",
    "        # get count per group, number of samples per group, and divide.\n",
    "        word_count = np.array(list(map(lambda g: np.array(g).sum(axis=0)[:-1] + self.alpha, groups)))\n",
    "        group_count = np.array(list(map(lambda g: len(g), groups)))\n",
    "        \n",
    "        # get probabilities, apply laplace smoothing\n",
    "        self.features_probs = word_count/((group_count + 2*self.alpha)[:,None])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        scores = np.zeros(self.classes.shape[0])\n",
    "        for i in range(self.classes.shape[0]):\n",
    "            sum = 0\n",
    "            for j in range(X.shape[0]):\n",
    "                if (X[j] == 0):\n",
    "                    sum += np.log(1 - self.features_probs[i][j])\n",
    "                else:\n",
    "                    sum += np.log(self.features_probs[i][j])\n",
    "            sum += self.priors[i]\n",
    "            scores[i] = sum\n",
    "        \n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,1,0],[1,0,1],[0,0,1],[1,1,1]])\n",
    "Y = np.array([6,7,6,6])\n",
    "b = BernoulliNaiveBayes()\n",
    "\n",
    "b.fit(X,Y)\n",
    "# print(b.predict(np.array([1,1,1])))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_path = './Data/reddit_test.csv'\n",
    "\n",
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#clean\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "#load\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "#clean\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "\n",
    "comment_data['prep'] = comment_data['prep'].apply(lemmatize_col)\n",
    "test_data['prep'] = comment_data['prep'].apply(lemmatize_col)\n",
    "\n",
    "# stopwords\n",
    "stop = stopwords.words('english')\n",
    "comment_data['prep'] = comment_data['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "test_data['prep'] = test_data['prep'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2)\n"
     ]
    }
   ],
   "source": [
    "clean_data = comment_data['prep'].to_numpy()\n",
    "clean_labels = comment_data['subreddits'].to_numpy()\n",
    "\n",
    "train_comments = []\n",
    "for idx in range(clean_data.shape[0]):\n",
    "    item = (clean_data[idx], clean_labels[idx])\n",
    "    train_comments.append(item)\n",
    "train_comments = np.asarray(train_comments)\n",
    "print(train_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 2) (7000, 2) (7000, 2)\n"
     ]
    }
   ],
   "source": [
    "# # 60000/10000\n",
    "# training_data = clean_data[:60000]\n",
    "# testing_data = clean_data[60000:]\n",
    "# training_labels = clean_labels[:60000]\n",
    "# testing_labels = clean_labels[60000:]\n",
    "\n",
    "commentFolds = kFold(train_comments)\n",
    "commentFolds.generateSplits()\n",
    "splits = commentFolds.splits\n",
    "x, y, z = splits[0]\n",
    "print(x.shape, y.shape, z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(subset):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for x,y in subset:\n",
    "        data.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    data = np.array(X)\n",
    "    labels = np.array(Y)\n",
    "    \n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 14, 12, 16, 9, 9, 13, 4, 13, 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2acc660a041b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1d8a44c0f63d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb = BernoulliNaiveBayes()\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "for split in splits:\n",
    "    train, val, test = split\n",
    "    \n",
    "    training_data, training_labels = unpack(train)\n",
    "    validation_data, validation_labels = unpack(val)\n",
    "    testing_data, testing_labels = unpack(test)\n",
    "    \n",
    "    # tokenize and remove min words on \"training set\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, ngram_range=(1,1), min_df=3)\n",
    "    tfidf = tfidf_vectorizer.fit(training_data)\n",
    "    \n",
    "    # filter out bad words\n",
    "    training_vec = tfidf_vectorizer.transform(training_data).astype(np.float32).toarray()\n",
    "    validation_vec = tfidf_vectorizer.transform(validation_data).astype(np.float32).toarray()\n",
    "    testing_vec = tfidf_vectorizer.transform(testing_data).astype(np.float32).toarray()\n",
    "    \n",
    "    nb.fit(training_vec,training_labels)\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, vec in enumerate(validation_vec):\n",
    "        result = (nb.predict(vec))\n",
    "        if nb.classes[result] == validation_labels[idx]:\n",
    "            num_correct += 1\n",
    "    print(\"Validation accuracy is: \" , num_correct/(validation_vec.shape[0]))\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, vec in enumerate(testing_vec):\n",
    "        result = (nb.predict(vec))\n",
    "        if nb.classes[result] == testing_labels[idx]:\n",
    "            num_correct += 1\n",
    "    print(\"Testing accuracy is: \" , num_correct/(testing_vec.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove min words on \"training set\"\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, ngram_range=(1,1), min_df=3)\n",
    "tfidf = tfidf_vectorizer.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out bad words\n",
    "training_vec = tfidf_vectorizer.transform(training_data).astype(np.float32)\n",
    "testing_vec = tfidf_vectorizer.transform(testing_data).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNaiveBayes()\n",
    "\n",
    "nb.fit(training_vec.toarray(),training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "num_correct = 0\n",
    "for idx, vec in enumerate(testing_vec.toarray()):\n",
    "    result = (nb.predict(vec))\n",
    "    if nb.classes[result] == testing_labels[idx]:\n",
    "        num_correct += 1\n",
    "    print(num_correct/(idx+1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
