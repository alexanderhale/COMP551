{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OriginalCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQs3FolX_vO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f2ed4a6-5b5a-40aa-8f7d-fd118712b459"
      },
      "source": [
        "import gzip\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "baseFilepath = '/content/drive/My Drive/School/COMP551/Assignment4/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdw3DhU2AzhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "24b68919-24d3-4661-9504-dea2e47c8cfb"
      },
      "source": [
        "import numpy as np\n",
        "import cPickle\n",
        "from collections import defaultdict\n",
        "import sys, re\n",
        "import pandas as pd\n",
        "\n",
        "def build_data_cv(data_folder, cv=10, clean_string=True):\n",
        "    \"\"\"\n",
        "    Loads data and split into 10 folds.\n",
        "    \"\"\"\n",
        "    revs = []\n",
        "    pos_file = data_folder[0]\n",
        "    neg_file = data_folder[1]\n",
        "    vocab = defaultdict(float)\n",
        "    with open(pos_file, \"rb\") as f:\n",
        "        for line in f:       \n",
        "            rev = []\n",
        "            rev.append(line.strip())\n",
        "            if clean_string:\n",
        "                orig_rev = clean_str(\" \".join(rev))\n",
        "            else:\n",
        "                orig_rev = \" \".join(rev).lower()\n",
        "            words = set(orig_rev.split())\n",
        "            for word in words:\n",
        "                vocab[word] += 1\n",
        "            datum  = {\"y\":1, \n",
        "                      \"text\": orig_rev,                             \n",
        "                      \"num_words\": len(orig_rev.split()),\n",
        "                      \"split\": np.random.randint(0,cv)}\n",
        "            revs.append(datum)\n",
        "    with open(neg_file, \"rb\") as f:\n",
        "        for line in f:       \n",
        "            rev = []\n",
        "            rev.append(line.strip())\n",
        "            if clean_string:\n",
        "                orig_rev = clean_str(\" \".join(rev))\n",
        "            else:\n",
        "                orig_rev = \" \".join(rev).lower()\n",
        "            words = set(orig_rev.split())\n",
        "            for word in words:\n",
        "                vocab[word] += 1\n",
        "            datum  = {\"y\":0, \n",
        "                      \"text\": orig_rev,                             \n",
        "                      \"num_words\": len(orig_rev.split()),\n",
        "                      \"split\": np.random.randint(0,cv)}\n",
        "            revs.append(datum)\n",
        "    return revs, vocab\n",
        "    \n",
        "def get_W(word_vecs, k=300):\n",
        "    \"\"\"\n",
        "    Get word matrix. W[i] is the vector for word indexed by i\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_vecs)\n",
        "    word_idx_map = dict()\n",
        "    W = np.zeros(shape=(vocab_size+1, k), dtype='float32')            \n",
        "    W[0] = np.zeros(k, dtype='float32')\n",
        "    i = 1\n",
        "    for word in word_vecs:\n",
        "        W[i] = word_vecs[word]\n",
        "        word_idx_map[word] = i\n",
        "        i += 1\n",
        "    return W, word_idx_map\n",
        "\n",
        "def load_bin_vec(fname, vocab):\n",
        "    \"\"\"\n",
        "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
        "    \"\"\"\n",
        "    word_vecs = {}\n",
        "    with gzip.open(fname, 'rb') as f:\n",
        "      # TODO confirm that this isn't only using the first chunk in the zip\n",
        "    # with open(fname, \"rb\") as f:\n",
        "        header = f.readline()\n",
        "        vocab_size, layer1_size = map(int, header.split())\n",
        "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
        "        for line in xrange(vocab_size):\n",
        "            word = []\n",
        "            while True:\n",
        "                ch = f.read(1)\n",
        "                if ch == ' ':\n",
        "                    word = ''.join(word)\n",
        "                    break\n",
        "                if ch != '\\n':\n",
        "                    word.append(ch)   \n",
        "            if word in vocab:\n",
        "               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
        "            else:\n",
        "                f.read(binary_len)\n",
        "    return word_vecs\n",
        "\n",
        "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
        "    \"\"\"\n",
        "    For words that occur in at least min_df documents, create a separate word vector.    \n",
        "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
        "    \"\"\"\n",
        "    for word in vocab:\n",
        "        if word not in word_vecs and vocab[word] >= min_df:\n",
        "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
        "\n",
        "def clean_str(string, TREC=False):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Every dataset is lower cased except for TREC\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "    string = re.sub(r\",\", \" , \", string) \n",
        "    string = re.sub(r\"!\", \" ! \", string) \n",
        "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "    return string.strip() if TREC else string.strip().lower()\n",
        "\n",
        "def clean_str_sst(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for the SST dataset\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)   \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "    return string.strip().lower()\n",
        "\n",
        "if __name__==\"__main__\":    \n",
        "    w2v_file = baseFilepath + 'OriginalCode/GoogleNews-vectors-negative300.bin.gz'  # sys.argv[1]     \n",
        "    data_folder = [baseFilepath + \"rt-polarity.pos\", baseFilepath + \"rt-polarity.neg\"]    \n",
        "    print \"loading data...\",        \n",
        "    revs, vocab = build_data_cv(data_folder, cv=10, clean_string=True)\n",
        "    max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
        "    print \"data loaded!\"\n",
        "    print \"number of sentences: \" + str(len(revs))\n",
        "    print \"vocab size: \" + str(len(vocab))\n",
        "    print \"max sentence length: \" + str(max_l)\n",
        "    print \"loading word2vec vectors...\",\n",
        "    w2v = load_bin_vec(w2v_file, vocab)\n",
        "    print \"word2vec loaded!\"\n",
        "    print \"num words already in word2vec: \" + str(len(w2v))\n",
        "    add_unknown_words(w2v, vocab)\n",
        "    W, word_idx_map = get_W(w2v)\n",
        "    rand_vecs = {}\n",
        "    add_unknown_words(rand_vecs, vocab)\n",
        "    W2, _ = get_W(rand_vecs)\n",
        "    cPickle.dump([revs, W, W2, word_idx_map, vocab], open(baseFilepath + \"OriginalCode/mr.p\", \"wb\"))\n",
        "    print \"dataset created!\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data... data loaded!\n",
            "number of sentences: 10662\n",
            "vocab size: 18764\n",
            "max sentence length: 56\n",
            "loading word2vec vectors..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:84: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " word2vec loaded!\n",
            "num words already in word2vec: 16448\n",
            "dataset created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3nvmn9tDEBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "outputId": "867bebd3-ef18-45cf-f943-6617441717df"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/School/COMP551/Assignment4/OriginalCode\n",
        "!pip install -I theano\n",
        "!THEANO_FLAGS=mode=FAST_RUN,device=cpu,floatX=float32 python conv_net_sentence.py -nonstatic -word2vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/School/COMP551/Assignment4/OriginalCode\n",
            "Collecting theano\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/c4/6341148ad458b6cd8361b774d7ee6895c38eab88f05331f22304c484ed5d/Theano-1.0.4.tar.gz (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 4.8MB/s \n",
            "\u001b[?25hCollecting numpy>=1.9.1\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting scipy>=0.14\n",
            "  Using cached https://files.pythonhosted.org/packages/1d/f6/7c16d60aeb3694e5611976cb4f1eaf1c6b7f1e7c55771d691013405a02ea/scipy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting six>=1.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: theano\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-1.0.4-cp27-none-any.whl size=2667178 sha256=bc3d3fba0defda4e99da3d6fbef320a344febdeb6e4597119998db6fdc1b7148\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/fb/be/483910ff7e9f703f30a10605ad7605f3316493875c86637014\n",
            "Successfully built theano\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.3.1+cu100 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scipy, six, theano\n",
            "Successfully installed numpy-1.16.5 scipy-1.2.2 six-1.13.0 theano-1.0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loading data... data loaded!\n",
            "model architecture: CNN-non-static\n",
            "using: word2vec vectors\n",
            "[('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "... training\n",
            "epoch: 1, training time: 269.82 secs, train perf: 81.46 %, val perf: 79.79 %\n",
            "epoch: 2, training time: 269.56 secs, train perf: 86.00 %, val perf: 80.42 %\n",
            "epoch: 3, training time: 269.36 secs, train perf: 90.38 %, val perf: 81.05 %\n",
            "epoch: 4, training time: 269.83 secs, train perf: 94.80 %, val perf: 81.05 %\n",
            "epoch: 5, training time: 270.12 secs, train perf: 97.76 %, val perf: 83.16 %\n",
            "epoch: 6, training time: 269.17 secs, train perf: 99.09 %, val perf: 83.16 %\n",
            "epoch: 7, training time: 269.22 secs, train perf: 99.38 %, val perf: 82.95 %\n",
            "epoch: 8, training time: 269.30 secs, train perf: 99.80 %, val perf: 82.63 %\n",
            "epoch: 9, training time: 269.12 secs, train perf: 99.93 %, val perf: 82.63 %\n",
            "epoch: 10, training time: 269.12 secs, train perf: 99.94 %, val perf: 82.32 %\n",
            "epoch: 11, training time: 269.55 secs, train perf: 99.98 %, val perf: 82.21 %\n",
            "epoch: 12, training time: 268.98 secs, train perf: 99.99 %, val perf: 82.42 %\n",
            "epoch: 13, training time: 269.35 secs, train perf: 99.98 %, val perf: 82.95 %\n",
            "epoch: 14, training time: 269.60 secs, train perf: 99.99 %, val perf: 82.21 %\n",
            "epoch: 15, training time: 268.81 secs, train perf: 100.00 %, val perf: 82.11 %\n",
            "epoch: 16, training time: 268.71 secs, train perf: 100.00 %, val perf: 82.53 %\n",
            "epoch: 17, training time: 268.63 secs, train perf: 100.00 %, val perf: 83.26 %\n",
            "epoch: 18, training time: 268.92 secs, train perf: 99.99 %, val perf: 83.16 %\n",
            "epoch: 19, training time: 268.98 secs, train perf: 100.00 %, val perf: 83.16 %\n",
            "epoch: 20, training time: 268.96 secs, train perf: 100.00 %, val perf: 83.47 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}