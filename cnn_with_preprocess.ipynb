{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# loading the original MNIST hand-written digits\n",
    "mndata = MNIST('')\n",
    "mndata.gz = True\n",
    "\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = np.asarray(images)\n",
    "imgs = np.reshape(imgs, (-1, 28, 28))\n",
    "print(imgs.shape)\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(imgs[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 14)        140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 14)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 28)          9828      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 28)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 28)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 784)               352016    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 369,834\n",
      "Trainable params: 369,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(14, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(28, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(784, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.30, random_state=0)\n",
    "X_train = np.array(X_train)/255\n",
    "X_test = np.array(X_test)/255\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1,28,28,1)\n",
    "X_test = np.array(X_test).reshape(-1,28,28,1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(np.reshape(y_train, (-1,1)))\n",
    "y_test = onehot_encoder.transform(np.reshape(y_test, (-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - ETA: 25s - loss: 2.3619 - accuracy: 0.092 - ETA: 16s - loss: 2.3268 - accuracy: 0.105 - ETA: 13s - loss: 2.2968 - accuracy: 0.134 - ETA: 11s - loss: 2.2683 - accuracy: 0.166 - ETA: 10s - loss: 2.2420 - accuracy: 0.195 - ETA: 9s - loss: 2.2137 - accuracy: 0.222 - ETA: 9s - loss: 2.1848 - accuracy: 0.24 - ETA: 8s - loss: 2.1523 - accuracy: 0.27 - ETA: 7s - loss: 2.1205 - accuracy: 0.29 - ETA: 7s - loss: 2.0849 - accuracy: 0.30 - ETA: 7s - loss: 2.0494 - accuracy: 0.32 - ETA: 6s - loss: 2.0130 - accuracy: 0.33 - ETA: 6s - loss: 1.9742 - accuracy: 0.35 - ETA: 6s - loss: 1.9328 - accuracy: 0.36 - ETA: 6s - loss: 1.8905 - accuracy: 0.38 - ETA: 5s - loss: 1.8481 - accuracy: 0.39 - ETA: 5s - loss: 1.8086 - accuracy: 0.41 - ETA: 5s - loss: 1.7679 - accuracy: 0.42 - ETA: 5s - loss: 1.7288 - accuracy: 0.43 - ETA: 4s - loss: 1.6935 - accuracy: 0.44 - ETA: 4s - loss: 1.6591 - accuracy: 0.45 - ETA: 4s - loss: 1.6251 - accuracy: 0.46 - ETA: 4s - loss: 1.5952 - accuracy: 0.47 - ETA: 3s - loss: 1.5646 - accuracy: 0.48 - ETA: 3s - loss: 1.5385 - accuracy: 0.49 - ETA: 3s - loss: 1.5108 - accuracy: 0.50 - ETA: 3s - loss: 1.4848 - accuracy: 0.51 - ETA: 3s - loss: 1.4591 - accuracy: 0.52 - ETA: 2s - loss: 1.4332 - accuracy: 0.52 - ETA: 2s - loss: 1.4101 - accuracy: 0.53 - ETA: 2s - loss: 1.3873 - accuracy: 0.54 - ETA: 2s - loss: 1.3660 - accuracy: 0.55 - ETA: 1s - loss: 1.3453 - accuracy: 0.55 - ETA: 1s - loss: 1.3226 - accuracy: 0.56 - ETA: 1s - loss: 1.3032 - accuracy: 0.57 - ETA: 1s - loss: 1.2857 - accuracy: 0.57 - ETA: 1s - loss: 1.2666 - accuracy: 0.58 - ETA: 0s - loss: 1.2492 - accuracy: 0.59 - ETA: 0s - loss: 1.2325 - accuracy: 0.59 - ETA: 0s - loss: 1.2169 - accuracy: 0.60 - ETA: 0s - loss: 1.2010 - accuracy: 0.60 - 10s 249us/step - loss: 1.1863 - accuracy: 0.6114 - val_loss: 0.3874 - val_accuracy: 0.9012\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - ETA: 8s - loss: 0.5953 - accuracy: 0.80 - ETA: 8s - loss: 0.5604 - accuracy: 0.81 - ETA: 8s - loss: 0.5461 - accuracy: 0.81 - ETA: 8s - loss: 0.5427 - accuracy: 0.81 - ETA: 8s - loss: 0.5479 - accuracy: 0.81 - ETA: 8s - loss: 0.5391 - accuracy: 0.81 - ETA: 7s - loss: 0.5410 - accuracy: 0.82 - ETA: 7s - loss: 0.5375 - accuracy: 0.82 - ETA: 7s - loss: 0.5299 - accuracy: 0.82 - ETA: 7s - loss: 0.5237 - accuracy: 0.83 - ETA: 6s - loss: 0.5206 - accuracy: 0.83 - ETA: 6s - loss: 0.5188 - accuracy: 0.83 - ETA: 6s - loss: 0.5128 - accuracy: 0.83 - ETA: 6s - loss: 0.5062 - accuracy: 0.83 - ETA: 5s - loss: 0.5032 - accuracy: 0.83 - ETA: 5s - loss: 0.5019 - accuracy: 0.83 - ETA: 5s - loss: 0.4991 - accuracy: 0.83 - ETA: 5s - loss: 0.4971 - accuracy: 0.84 - ETA: 5s - loss: 0.4943 - accuracy: 0.84 - ETA: 4s - loss: 0.4917 - accuracy: 0.84 - ETA: 4s - loss: 0.4877 - accuracy: 0.84 - ETA: 4s - loss: 0.4825 - accuracy: 0.84 - ETA: 4s - loss: 0.4772 - accuracy: 0.84 - ETA: 3s - loss: 0.4737 - accuracy: 0.84 - ETA: 3s - loss: 0.4706 - accuracy: 0.84 - ETA: 3s - loss: 0.4680 - accuracy: 0.84 - ETA: 3s - loss: 0.4651 - accuracy: 0.85 - ETA: 3s - loss: 0.4614 - accuracy: 0.85 - ETA: 2s - loss: 0.4579 - accuracy: 0.85 - ETA: 2s - loss: 0.4549 - accuracy: 0.85 - ETA: 2s - loss: 0.4527 - accuracy: 0.85 - ETA: 2s - loss: 0.4496 - accuracy: 0.85 - ETA: 2s - loss: 0.4468 - accuracy: 0.85 - ETA: 1s - loss: 0.4449 - accuracy: 0.85 - ETA: 1s - loss: 0.4426 - accuracy: 0.85 - ETA: 1s - loss: 0.4416 - accuracy: 0.85 - ETA: 1s - loss: 0.4383 - accuracy: 0.85 - ETA: 0s - loss: 0.4351 - accuracy: 0.86 - ETA: 0s - loss: 0.4323 - accuracy: 0.86 - ETA: 0s - loss: 0.4301 - accuracy: 0.86 - ETA: 0s - loss: 0.4280 - accuracy: 0.86 - 11s 265us/step - loss: 0.4251 - accuracy: 0.8639 - val_loss: 0.2027 - val_accuracy: 0.9434\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - ETA: 12s - loss: 0.3150 - accuracy: 0.905 - ETA: 11s - loss: 0.3419 - accuracy: 0.896 - ETA: 10s - loss: 0.3176 - accuracy: 0.902 - ETA: 10s - loss: 0.3143 - accuracy: 0.901 - ETA: 10s - loss: 0.3204 - accuracy: 0.899 - ETA: 10s - loss: 0.3198 - accuracy: 0.898 - ETA: 10s - loss: 0.3272 - accuracy: 0.894 - ETA: 9s - loss: 0.3275 - accuracy: 0.893 - ETA: 9s - loss: 0.3225 - accuracy: 0.89 - ETA: 9s - loss: 0.3215 - accuracy: 0.89 - ETA: 8s - loss: 0.3202 - accuracy: 0.89 - ETA: 8s - loss: 0.3194 - accuracy: 0.89 - ETA: 8s - loss: 0.3198 - accuracy: 0.89 - ETA: 8s - loss: 0.3193 - accuracy: 0.89 - ETA: 7s - loss: 0.3183 - accuracy: 0.89 - ETA: 7s - loss: 0.3183 - accuracy: 0.89 - ETA: 7s - loss: 0.3153 - accuracy: 0.90 - ETA: 6s - loss: 0.3136 - accuracy: 0.90 - ETA: 6s - loss: 0.3132 - accuracy: 0.90 - ETA: 6s - loss: 0.3137 - accuracy: 0.90 - ETA: 6s - loss: 0.3122 - accuracy: 0.90 - ETA: 5s - loss: 0.3128 - accuracy: 0.90 - ETA: 5s - loss: 0.3122 - accuracy: 0.90 - ETA: 5s - loss: 0.3130 - accuracy: 0.90 - ETA: 4s - loss: 0.3107 - accuracy: 0.90 - ETA: 4s - loss: 0.3104 - accuracy: 0.90 - ETA: 4s - loss: 0.3104 - accuracy: 0.90 - ETA: 3s - loss: 0.3084 - accuracy: 0.90 - ETA: 3s - loss: 0.3060 - accuracy: 0.90 - ETA: 3s - loss: 0.3059 - accuracy: 0.90 - ETA: 3s - loss: 0.3049 - accuracy: 0.90 - ETA: 2s - loss: 0.3043 - accuracy: 0.90 - ETA: 2s - loss: 0.3031 - accuracy: 0.90 - ETA: 2s - loss: 0.3013 - accuracy: 0.90 - ETA: 1s - loss: 0.3019 - accuracy: 0.90 - ETA: 1s - loss: 0.3001 - accuracy: 0.90 - ETA: 1s - loss: 0.2996 - accuracy: 0.90 - ETA: 1s - loss: 0.2981 - accuracy: 0.90 - ETA: 0s - loss: 0.2975 - accuracy: 0.90 - ETA: 0s - loss: 0.2966 - accuracy: 0.90 - ETA: 0s - loss: 0.2957 - accuracy: 0.90 - 13s 312us/step - loss: 0.2950 - accuracy: 0.9089 - val_loss: 0.1475 - val_accuracy: 0.9583\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.3046 - accuracy: 0.90 - ETA: 10s - loss: 0.2604 - accuracy: 0.913 - ETA: 10s - loss: 0.2583 - accuracy: 0.916 - ETA: 10s - loss: 0.2542 - accuracy: 0.920 - ETA: 10s - loss: 0.2619 - accuracy: 0.916 - ETA: 9s - loss: 0.2648 - accuracy: 0.915 - ETA: 9s - loss: 0.2683 - accuracy: 0.91 - ETA: 9s - loss: 0.2639 - accuracy: 0.91 - ETA: 8s - loss: 0.2639 - accuracy: 0.91 - ETA: 8s - loss: 0.2649 - accuracy: 0.91 - ETA: 7s - loss: 0.2642 - accuracy: 0.91 - ETA: 7s - loss: 0.2659 - accuracy: 0.91 - ETA: 7s - loss: 0.2665 - accuracy: 0.91 - ETA: 7s - loss: 0.2676 - accuracy: 0.91 - ETA: 7s - loss: 0.2664 - accuracy: 0.91 - ETA: 6s - loss: 0.2647 - accuracy: 0.91 - ETA: 6s - loss: 0.2637 - accuracy: 0.91 - ETA: 6s - loss: 0.2639 - accuracy: 0.91 - ETA: 6s - loss: 0.2631 - accuracy: 0.91 - ETA: 5s - loss: 0.2623 - accuracy: 0.91 - ETA: 5s - loss: 0.2614 - accuracy: 0.92 - ETA: 5s - loss: 0.2584 - accuracy: 0.92 - ETA: 5s - loss: 0.2568 - accuracy: 0.92 - ETA: 4s - loss: 0.2554 - accuracy: 0.92 - ETA: 4s - loss: 0.2546 - accuracy: 0.92 - ETA: 4s - loss: 0.2537 - accuracy: 0.92 - ETA: 3s - loss: 0.2534 - accuracy: 0.92 - ETA: 3s - loss: 0.2521 - accuracy: 0.92 - ETA: 3s - loss: 0.2515 - accuracy: 0.92 - ETA: 3s - loss: 0.2503 - accuracy: 0.92 - ETA: 2s - loss: 0.2490 - accuracy: 0.92 - ETA: 2s - loss: 0.2480 - accuracy: 0.92 - ETA: 2s - loss: 0.2468 - accuracy: 0.92 - ETA: 2s - loss: 0.2462 - accuracy: 0.92 - ETA: 1s - loss: 0.2449 - accuracy: 0.92 - ETA: 1s - loss: 0.2441 - accuracy: 0.92 - ETA: 1s - loss: 0.2437 - accuracy: 0.92 - ETA: 1s - loss: 0.2432 - accuracy: 0.92 - ETA: 0s - loss: 0.2422 - accuracy: 0.92 - ETA: 0s - loss: 0.2422 - accuracy: 0.92 - ETA: 0s - loss: 0.2425 - accuracy: 0.92 - 12s 297us/step - loss: 0.2411 - accuracy: 0.9260 - val_loss: 0.1146 - val_accuracy: 0.9667\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.2411 - accuracy: 0.923 - ETA: 11s - loss: 0.2362 - accuracy: 0.922 - ETA: 10s - loss: 0.2239 - accuracy: 0.928 - ETA: 10s - loss: 0.2302 - accuracy: 0.928 - ETA: 9s - loss: 0.2326 - accuracy: 0.930 - ETA: 9s - loss: 0.2319 - accuracy: 0.93 - ETA: 9s - loss: 0.2277 - accuracy: 0.93 - ETA: 8s - loss: 0.2259 - accuracy: 0.93 - ETA: 8s - loss: 0.2245 - accuracy: 0.93 - ETA: 8s - loss: 0.2212 - accuracy: 0.93 - ETA: 8s - loss: 0.2223 - accuracy: 0.93 - ETA: 8s - loss: 0.2231 - accuracy: 0.93 - ETA: 7s - loss: 0.2210 - accuracy: 0.93 - ETA: 7s - loss: 0.2193 - accuracy: 0.93 - ETA: 7s - loss: 0.2190 - accuracy: 0.93 - ETA: 7s - loss: 0.2184 - accuracy: 0.93 - ETA: 6s - loss: 0.2201 - accuracy: 0.93 - ETA: 6s - loss: 0.2214 - accuracy: 0.93 - ETA: 6s - loss: 0.2192 - accuracy: 0.93 - ETA: 5s - loss: 0.2183 - accuracy: 0.93 - ETA: 5s - loss: 0.2183 - accuracy: 0.93 - ETA: 5s - loss: 0.2192 - accuracy: 0.93 - ETA: 5s - loss: 0.2191 - accuracy: 0.93 - ETA: 4s - loss: 0.2182 - accuracy: 0.93 - ETA: 4s - loss: 0.2176 - accuracy: 0.93 - ETA: 4s - loss: 0.2161 - accuracy: 0.93 - ETA: 4s - loss: 0.2158 - accuracy: 0.93 - ETA: 3s - loss: 0.2151 - accuracy: 0.93 - ETA: 3s - loss: 0.2142 - accuracy: 0.93 - ETA: 3s - loss: 0.2133 - accuracy: 0.93 - ETA: 3s - loss: 0.2130 - accuracy: 0.93 - ETA: 2s - loss: 0.2122 - accuracy: 0.93 - ETA: 2s - loss: 0.2116 - accuracy: 0.93 - ETA: 2s - loss: 0.2103 - accuracy: 0.93 - ETA: 1s - loss: 0.2094 - accuracy: 0.93 - ETA: 1s - loss: 0.2093 - accuracy: 0.93 - ETA: 1s - loss: 0.2091 - accuracy: 0.93 - ETA: 1s - loss: 0.2092 - accuracy: 0.93 - ETA: 0s - loss: 0.2086 - accuracy: 0.93 - ETA: 0s - loss: 0.2081 - accuracy: 0.93 - ETA: 0s - loss: 0.2081 - accuracy: 0.93 - 13s 309us/step - loss: 0.2078 - accuracy: 0.9364 - val_loss: 0.0958 - val_accuracy: 0.9712\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - ETA: 12s - loss: 0.1477 - accuracy: 0.951 - ETA: 11s - loss: 0.1587 - accuracy: 0.949 - ETA: 11s - loss: 0.1685 - accuracy: 0.948 - ETA: 10s - loss: 0.1736 - accuracy: 0.945 - ETA: 10s - loss: 0.1793 - accuracy: 0.944 - ETA: 10s - loss: 0.1786 - accuracy: 0.944 - ETA: 9s - loss: 0.1801 - accuracy: 0.943 - ETA: 9s - loss: 0.1807 - accuracy: 0.94 - ETA: 9s - loss: 0.1819 - accuracy: 0.94 - ETA: 9s - loss: 0.1852 - accuracy: 0.94 - ETA: 8s - loss: 0.1871 - accuracy: 0.94 - ETA: 8s - loss: 0.1874 - accuracy: 0.94 - ETA: 8s - loss: 0.1852 - accuracy: 0.94 - ETA: 8s - loss: 0.1825 - accuracy: 0.94 - ETA: 7s - loss: 0.1848 - accuracy: 0.94 - ETA: 7s - loss: 0.1860 - accuracy: 0.94 - ETA: 7s - loss: 0.1850 - accuracy: 0.94 - ETA: 6s - loss: 0.1864 - accuracy: 0.94 - ETA: 6s - loss: 0.1862 - accuracy: 0.94 - ETA: 6s - loss: 0.1859 - accuracy: 0.94 - ETA: 5s - loss: 0.1865 - accuracy: 0.94 - ETA: 5s - loss: 0.1862 - accuracy: 0.94 - ETA: 5s - loss: 0.1854 - accuracy: 0.94 - ETA: 5s - loss: 0.1865 - accuracy: 0.94 - ETA: 4s - loss: 0.1858 - accuracy: 0.94 - ETA: 4s - loss: 0.1863 - accuracy: 0.94 - ETA: 4s - loss: 0.1864 - accuracy: 0.94 - ETA: 3s - loss: 0.1866 - accuracy: 0.94 - ETA: 3s - loss: 0.1870 - accuracy: 0.94 - ETA: 3s - loss: 0.1878 - accuracy: 0.94 - ETA: 3s - loss: 0.1876 - accuracy: 0.94 - ETA: 2s - loss: 0.1880 - accuracy: 0.94 - ETA: 2s - loss: 0.1888 - accuracy: 0.94 - ETA: 2s - loss: 0.1887 - accuracy: 0.94 - ETA: 1s - loss: 0.1887 - accuracy: 0.94 - ETA: 1s - loss: 0.1890 - accuracy: 0.94 - ETA: 1s - loss: 0.1884 - accuracy: 0.94 - ETA: 1s - loss: 0.1876 - accuracy: 0.94 - ETA: 0s - loss: 0.1873 - accuracy: 0.94 - ETA: 0s - loss: 0.1872 - accuracy: 0.94 - ETA: 0s - loss: 0.1876 - accuracy: 0.94 - 13s 300us/step - loss: 0.1875 - accuracy: 0.9421 - val_loss: 0.0906 - val_accuracy: 0.9728\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1419 - accuracy: 0.960 - ETA: 10s - loss: 0.1415 - accuracy: 0.957 - ETA: 10s - loss: 0.1524 - accuracy: 0.953 - ETA: 9s - loss: 0.1606 - accuracy: 0.950 - ETA: 9s - loss: 0.1637 - accuracy: 0.94 - ETA: 9s - loss: 0.1711 - accuracy: 0.94 - ETA: 9s - loss: 0.1684 - accuracy: 0.94 - ETA: 8s - loss: 0.1658 - accuracy: 0.94 - ETA: 8s - loss: 0.1705 - accuracy: 0.94 - ETA: 8s - loss: 0.1694 - accuracy: 0.94 - ETA: 8s - loss: 0.1691 - accuracy: 0.94 - ETA: 7s - loss: 0.1702 - accuracy: 0.94 - ETA: 7s - loss: 0.1694 - accuracy: 0.94 - ETA: 7s - loss: 0.1680 - accuracy: 0.94 - ETA: 6s - loss: 0.1667 - accuracy: 0.94 - ETA: 6s - loss: 0.1662 - accuracy: 0.94 - ETA: 6s - loss: 0.1663 - accuracy: 0.94 - ETA: 6s - loss: 0.1668 - accuracy: 0.94 - ETA: 5s - loss: 0.1665 - accuracy: 0.94 - ETA: 5s - loss: 0.1660 - accuracy: 0.94 - ETA: 5s - loss: 0.1650 - accuracy: 0.94 - ETA: 5s - loss: 0.1661 - accuracy: 0.94 - ETA: 5s - loss: 0.1660 - accuracy: 0.94 - ETA: 4s - loss: 0.1661 - accuracy: 0.94 - ETA: 4s - loss: 0.1665 - accuracy: 0.94 - ETA: 4s - loss: 0.1661 - accuracy: 0.94 - ETA: 4s - loss: 0.1657 - accuracy: 0.94 - ETA: 3s - loss: 0.1670 - accuracy: 0.94 - ETA: 3s - loss: 0.1672 - accuracy: 0.94 - ETA: 3s - loss: 0.1669 - accuracy: 0.94 - ETA: 3s - loss: 0.1670 - accuracy: 0.94 - ETA: 2s - loss: 0.1678 - accuracy: 0.94 - ETA: 2s - loss: 0.1666 - accuracy: 0.94 - ETA: 2s - loss: 0.1664 - accuracy: 0.94 - ETA: 2s - loss: 0.1666 - accuracy: 0.94 - ETA: 1s - loss: 0.1665 - accuracy: 0.94 - ETA: 1s - loss: 0.1670 - accuracy: 0.94 - ETA: 1s - loss: 0.1670 - accuracy: 0.94 - ETA: 0s - loss: 0.1663 - accuracy: 0.94 - ETA: 0s - loss: 0.1665 - accuracy: 0.94 - ETA: 0s - loss: 0.1667 - accuracy: 0.94 - 14s 333us/step - loss: 0.1665 - accuracy: 0.9471 - val_loss: 0.0805 - val_accuracy: 0.9766\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.1451 - accuracy: 0.956 - ETA: 13s - loss: 0.1386 - accuracy: 0.957 - ETA: 12s - loss: 0.1495 - accuracy: 0.953 - ETA: 12s - loss: 0.1526 - accuracy: 0.951 - ETA: 11s - loss: 0.1573 - accuracy: 0.951 - ETA: 11s - loss: 0.1562 - accuracy: 0.951 - ETA: 10s - loss: 0.1521 - accuracy: 0.952 - ETA: 10s - loss: 0.1531 - accuracy: 0.953 - ETA: 9s - loss: 0.1541 - accuracy: 0.952 - ETA: 9s - loss: 0.1542 - accuracy: 0.95 - ETA: 9s - loss: 0.1531 - accuracy: 0.95 - ETA: 8s - loss: 0.1519 - accuracy: 0.95 - ETA: 8s - loss: 0.1554 - accuracy: 0.95 - ETA: 8s - loss: 0.1548 - accuracy: 0.95 - ETA: 8s - loss: 0.1545 - accuracy: 0.95 - ETA: 7s - loss: 0.1544 - accuracy: 0.95 - ETA: 7s - loss: 0.1543 - accuracy: 0.95 - ETA: 7s - loss: 0.1551 - accuracy: 0.95 - ETA: 6s - loss: 0.1563 - accuracy: 0.95 - ETA: 6s - loss: 0.1572 - accuracy: 0.94 - ETA: 6s - loss: 0.1566 - accuracy: 0.95 - ETA: 5s - loss: 0.1560 - accuracy: 0.95 - ETA: 5s - loss: 0.1564 - accuracy: 0.95 - ETA: 5s - loss: 0.1570 - accuracy: 0.95 - ETA: 5s - loss: 0.1573 - accuracy: 0.95 - ETA: 4s - loss: 0.1576 - accuracy: 0.95 - ETA: 4s - loss: 0.1583 - accuracy: 0.94 - ETA: 4s - loss: 0.1584 - accuracy: 0.95 - ETA: 3s - loss: 0.1583 - accuracy: 0.95 - ETA: 3s - loss: 0.1580 - accuracy: 0.95 - ETA: 3s - loss: 0.1582 - accuracy: 0.95 - ETA: 2s - loss: 0.1574 - accuracy: 0.95 - ETA: 2s - loss: 0.1579 - accuracy: 0.95 - ETA: 2s - loss: 0.1574 - accuracy: 0.95 - ETA: 2s - loss: 0.1571 - accuracy: 0.95 - ETA: 1s - loss: 0.1568 - accuracy: 0.95 - ETA: 1s - loss: 0.1567 - accuracy: 0.95 - ETA: 1s - loss: 0.1564 - accuracy: 0.95 - ETA: 0s - loss: 0.1559 - accuracy: 0.95 - ETA: 0s - loss: 0.1554 - accuracy: 0.95 - ETA: 0s - loss: 0.1561 - accuracy: 0.95 - 14s 331us/step - loss: 0.1555 - accuracy: 0.9511 - val_loss: 0.0717 - val_accuracy: 0.9784\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1713 - accuracy: 0.947 - ETA: 10s - loss: 0.1821 - accuracy: 0.943 - ETA: 10s - loss: 0.1718 - accuracy: 0.944 - ETA: 9s - loss: 0.1663 - accuracy: 0.947 - ETA: 9s - loss: 0.1619 - accuracy: 0.94 - ETA: 9s - loss: 0.1630 - accuracy: 0.94 - ETA: 8s - loss: 0.1609 - accuracy: 0.94 - ETA: 8s - loss: 0.1593 - accuracy: 0.94 - ETA: 8s - loss: 0.1560 - accuracy: 0.95 - ETA: 8s - loss: 0.1512 - accuracy: 0.95 - ETA: 8s - loss: 0.1518 - accuracy: 0.95 - ETA: 8s - loss: 0.1491 - accuracy: 0.95 - ETA: 8s - loss: 0.1496 - accuracy: 0.95 - ETA: 7s - loss: 0.1494 - accuracy: 0.95 - ETA: 7s - loss: 0.1496 - accuracy: 0.95 - ETA: 7s - loss: 0.1495 - accuracy: 0.95 - ETA: 6s - loss: 0.1513 - accuracy: 0.95 - ETA: 6s - loss: 0.1541 - accuracy: 0.95 - ETA: 6s - loss: 0.1528 - accuracy: 0.95 - ETA: 5s - loss: 0.1533 - accuracy: 0.95 - ETA: 5s - loss: 0.1539 - accuracy: 0.95 - ETA: 5s - loss: 0.1528 - accuracy: 0.95 - ETA: 5s - loss: 0.1509 - accuracy: 0.95 - ETA: 4s - loss: 0.1513 - accuracy: 0.95 - ETA: 4s - loss: 0.1511 - accuracy: 0.95 - ETA: 4s - loss: 0.1511 - accuracy: 0.95 - ETA: 4s - loss: 0.1509 - accuracy: 0.95 - ETA: 3s - loss: 0.1497 - accuracy: 0.95 - ETA: 3s - loss: 0.1487 - accuracy: 0.95 - ETA: 3s - loss: 0.1484 - accuracy: 0.95 - ETA: 3s - loss: 0.1483 - accuracy: 0.95 - ETA: 2s - loss: 0.1492 - accuracy: 0.95 - ETA: 2s - loss: 0.1493 - accuracy: 0.95 - ETA: 2s - loss: 0.1492 - accuracy: 0.95 - ETA: 1s - loss: 0.1490 - accuracy: 0.95 - ETA: 1s - loss: 0.1484 - accuracy: 0.95 - ETA: 1s - loss: 0.1495 - accuracy: 0.95 - ETA: 1s - loss: 0.1493 - accuracy: 0.95 - ETA: 0s - loss: 0.1491 - accuracy: 0.95 - ETA: 0s - loss: 0.1485 - accuracy: 0.95 - ETA: 0s - loss: 0.1479 - accuracy: 0.95 - 13s 311us/step - loss: 0.1486 - accuracy: 0.9534 - val_loss: 0.0655 - val_accuracy: 0.9801\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - ETA: 12s - loss: 0.1248 - accuracy: 0.957 - ETA: 12s - loss: 0.1185 - accuracy: 0.964 - ETA: 12s - loss: 0.1249 - accuracy: 0.961 - ETA: 11s - loss: 0.1241 - accuracy: 0.962 - ETA: 11s - loss: 0.1367 - accuracy: 0.958 - ETA: 10s - loss: 0.1344 - accuracy: 0.958 - ETA: 10s - loss: 0.1380 - accuracy: 0.957 - ETA: 9s - loss: 0.1365 - accuracy: 0.957 - ETA: 9s - loss: 0.1375 - accuracy: 0.95 - ETA: 9s - loss: 0.1373 - accuracy: 0.95 - ETA: 9s - loss: 0.1393 - accuracy: 0.95 - ETA: 8s - loss: 0.1384 - accuracy: 0.95 - ETA: 8s - loss: 0.1380 - accuracy: 0.95 - ETA: 8s - loss: 0.1383 - accuracy: 0.95 - ETA: 7s - loss: 0.1370 - accuracy: 0.95 - ETA: 7s - loss: 0.1378 - accuracy: 0.95 - ETA: 7s - loss: 0.1358 - accuracy: 0.95 - ETA: 6s - loss: 0.1354 - accuracy: 0.95 - ETA: 6s - loss: 0.1352 - accuracy: 0.95 - ETA: 6s - loss: 0.1358 - accuracy: 0.95 - ETA: 5s - loss: 0.1359 - accuracy: 0.95 - ETA: 5s - loss: 0.1359 - accuracy: 0.95 - ETA: 5s - loss: 0.1357 - accuracy: 0.95 - ETA: 5s - loss: 0.1359 - accuracy: 0.95 - ETA: 4s - loss: 0.1360 - accuracy: 0.95 - ETA: 4s - loss: 0.1372 - accuracy: 0.95 - ETA: 4s - loss: 0.1369 - accuracy: 0.95 - ETA: 3s - loss: 0.1367 - accuracy: 0.95 - ETA: 3s - loss: 0.1365 - accuracy: 0.95 - ETA: 3s - loss: 0.1367 - accuracy: 0.95 - ETA: 3s - loss: 0.1360 - accuracy: 0.95 - ETA: 2s - loss: 0.1362 - accuracy: 0.95 - ETA: 2s - loss: 0.1358 - accuracy: 0.95 - ETA: 2s - loss: 0.1356 - accuracy: 0.95 - ETA: 1s - loss: 0.1354 - accuracy: 0.95 - ETA: 1s - loss: 0.1356 - accuracy: 0.95 - ETA: 1s - loss: 0.1355 - accuracy: 0.95 - ETA: 1s - loss: 0.1356 - accuracy: 0.95 - ETA: 0s - loss: 0.1363 - accuracy: 0.95 - ETA: 0s - loss: 0.1368 - accuracy: 0.95 - ETA: 0s - loss: 0.1366 - accuracy: 0.95 - 13s 304us/step - loss: 0.1371 - accuracy: 0.9565 - val_loss: 0.0611 - val_accuracy: 0.9816\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1287 - accuracy: 0.960 - ETA: 9s - loss: 0.1135 - accuracy: 0.964 - ETA: 9s - loss: 0.1203 - accuracy: 0.96 - ETA: 9s - loss: 0.1251 - accuracy: 0.96 - ETA: 9s - loss: 0.1206 - accuracy: 0.96 - ETA: 9s - loss: 0.1191 - accuracy: 0.96 - ETA: 9s - loss: 0.1222 - accuracy: 0.96 - ETA: 8s - loss: 0.1276 - accuracy: 0.96 - ETA: 8s - loss: 0.1288 - accuracy: 0.96 - ETA: 8s - loss: 0.1290 - accuracy: 0.96 - ETA: 8s - loss: 0.1293 - accuracy: 0.96 - ETA: 8s - loss: 0.1285 - accuracy: 0.96 - ETA: 7s - loss: 0.1273 - accuracy: 0.96 - ETA: 7s - loss: 0.1268 - accuracy: 0.96 - ETA: 7s - loss: 0.1252 - accuracy: 0.96 - ETA: 7s - loss: 0.1258 - accuracy: 0.96 - ETA: 6s - loss: 0.1262 - accuracy: 0.96 - ETA: 6s - loss: 0.1260 - accuracy: 0.96 - ETA: 6s - loss: 0.1262 - accuracy: 0.96 - ETA: 5s - loss: 0.1273 - accuracy: 0.96 - ETA: 5s - loss: 0.1277 - accuracy: 0.96 - ETA: 5s - loss: 0.1285 - accuracy: 0.96 - ETA: 5s - loss: 0.1294 - accuracy: 0.96 - ETA: 4s - loss: 0.1286 - accuracy: 0.96 - ETA: 4s - loss: 0.1290 - accuracy: 0.96 - ETA: 4s - loss: 0.1284 - accuracy: 0.96 - ETA: 4s - loss: 0.1293 - accuracy: 0.96 - ETA: 3s - loss: 0.1292 - accuracy: 0.96 - ETA: 3s - loss: 0.1292 - accuracy: 0.96 - ETA: 3s - loss: 0.1288 - accuracy: 0.96 - ETA: 3s - loss: 0.1284 - accuracy: 0.96 - ETA: 2s - loss: 0.1284 - accuracy: 0.96 - ETA: 2s - loss: 0.1289 - accuracy: 0.96 - ETA: 2s - loss: 0.1285 - accuracy: 0.96 - ETA: 1s - loss: 0.1287 - accuracy: 0.96 - ETA: 1s - loss: 0.1283 - accuracy: 0.95 - ETA: 1s - loss: 0.1284 - accuracy: 0.95 - ETA: 1s - loss: 0.1281 - accuracy: 0.96 - ETA: 0s - loss: 0.1281 - accuracy: 0.96 - ETA: 0s - loss: 0.1280 - accuracy: 0.96 - ETA: 0s - loss: 0.1283 - accuracy: 0.96 - 13s 318us/step - loss: 0.1290 - accuracy: 0.9599 - val_loss: 0.0570 - val_accuracy: 0.9829\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1272 - accuracy: 0.956 - ETA: 10s - loss: 0.1323 - accuracy: 0.954 - ETA: 10s - loss: 0.1336 - accuracy: 0.955 - ETA: 10s - loss: 0.1361 - accuracy: 0.955 - ETA: 10s - loss: 0.1280 - accuracy: 0.959 - ETA: 9s - loss: 0.1309 - accuracy: 0.957 - ETA: 9s - loss: 0.1300 - accuracy: 0.95 - ETA: 9s - loss: 0.1321 - accuracy: 0.95 - ETA: 9s - loss: 0.1335 - accuracy: 0.95 - ETA: 8s - loss: 0.1313 - accuracy: 0.95 - ETA: 8s - loss: 0.1315 - accuracy: 0.95 - ETA: 8s - loss: 0.1338 - accuracy: 0.95 - ETA: 7s - loss: 0.1331 - accuracy: 0.95 - ETA: 7s - loss: 0.1308 - accuracy: 0.95 - ETA: 7s - loss: 0.1311 - accuracy: 0.95 - ETA: 7s - loss: 0.1302 - accuracy: 0.95 - ETA: 6s - loss: 0.1302 - accuracy: 0.95 - ETA: 6s - loss: 0.1295 - accuracy: 0.95 - ETA: 6s - loss: 0.1286 - accuracy: 0.95 - ETA: 5s - loss: 0.1281 - accuracy: 0.95 - ETA: 5s - loss: 0.1281 - accuracy: 0.95 - ETA: 5s - loss: 0.1281 - accuracy: 0.95 - ETA: 5s - loss: 0.1276 - accuracy: 0.95 - ETA: 4s - loss: 0.1273 - accuracy: 0.95 - ETA: 4s - loss: 0.1268 - accuracy: 0.95 - ETA: 4s - loss: 0.1282 - accuracy: 0.95 - ETA: 4s - loss: 0.1283 - accuracy: 0.95 - ETA: 3s - loss: 0.1275 - accuracy: 0.95 - ETA: 3s - loss: 0.1281 - accuracy: 0.95 - ETA: 3s - loss: 0.1286 - accuracy: 0.95 - ETA: 2s - loss: 0.1284 - accuracy: 0.95 - ETA: 2s - loss: 0.1280 - accuracy: 0.95 - ETA: 2s - loss: 0.1277 - accuracy: 0.95 - ETA: 2s - loss: 0.1283 - accuracy: 0.95 - ETA: 1s - loss: 0.1281 - accuracy: 0.95 - ETA: 1s - loss: 0.1279 - accuracy: 0.95 - ETA: 1s - loss: 0.1281 - accuracy: 0.95 - ETA: 1s - loss: 0.1282 - accuracy: 0.95 - ETA: 0s - loss: 0.1284 - accuracy: 0.95 - ETA: 0s - loss: 0.1280 - accuracy: 0.95 - ETA: 0s - loss: 0.1276 - accuracy: 0.95 - 13s 301us/step - loss: 0.1275 - accuracy: 0.9595 - val_loss: 0.0552 - val_accuracy: 0.9828\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1537 - accuracy: 0.960 - ETA: 10s - loss: 0.1521 - accuracy: 0.954 - ETA: 10s - loss: 0.1439 - accuracy: 0.955 - ETA: 10s - loss: 0.1380 - accuracy: 0.954 - ETA: 10s - loss: 0.1297 - accuracy: 0.956 - ETA: 9s - loss: 0.1307 - accuracy: 0.957 - ETA: 9s - loss: 0.1295 - accuracy: 0.95 - ETA: 9s - loss: 0.1285 - accuracy: 0.95 - ETA: 9s - loss: 0.1295 - accuracy: 0.95 - ETA: 8s - loss: 0.1319 - accuracy: 0.95 - ETA: 8s - loss: 0.1297 - accuracy: 0.95 - ETA: 8s - loss: 0.1282 - accuracy: 0.95 - ETA: 8s - loss: 0.1281 - accuracy: 0.95 - ETA: 7s - loss: 0.1271 - accuracy: 0.95 - ETA: 7s - loss: 0.1262 - accuracy: 0.95 - ETA: 7s - loss: 0.1257 - accuracy: 0.96 - ETA: 6s - loss: 0.1243 - accuracy: 0.96 - ETA: 6s - loss: 0.1235 - accuracy: 0.96 - ETA: 6s - loss: 0.1235 - accuracy: 0.96 - ETA: 5s - loss: 0.1229 - accuracy: 0.96 - ETA: 5s - loss: 0.1225 - accuracy: 0.96 - ETA: 5s - loss: 0.1240 - accuracy: 0.96 - ETA: 5s - loss: 0.1228 - accuracy: 0.96 - ETA: 4s - loss: 0.1225 - accuracy: 0.96 - ETA: 4s - loss: 0.1228 - accuracy: 0.96 - ETA: 4s - loss: 0.1228 - accuracy: 0.96 - ETA: 4s - loss: 0.1220 - accuracy: 0.96 - ETA: 3s - loss: 0.1223 - accuracy: 0.96 - ETA: 3s - loss: 0.1222 - accuracy: 0.96 - ETA: 3s - loss: 0.1226 - accuracy: 0.96 - ETA: 2s - loss: 0.1231 - accuracy: 0.96 - ETA: 2s - loss: 0.1224 - accuracy: 0.96 - ETA: 2s - loss: 0.1226 - accuracy: 0.96 - ETA: 2s - loss: 0.1223 - accuracy: 0.96 - ETA: 1s - loss: 0.1221 - accuracy: 0.96 - ETA: 1s - loss: 0.1221 - accuracy: 0.96 - ETA: 1s - loss: 0.1221 - accuracy: 0.96 - ETA: 1s - loss: 0.1220 - accuracy: 0.96 - ETA: 0s - loss: 0.1229 - accuracy: 0.96 - ETA: 0s - loss: 0.1221 - accuracy: 0.96 - ETA: 0s - loss: 0.1216 - accuracy: 0.96 - 13s 299us/step - loss: 0.1213 - accuracy: 0.9619 - val_loss: 0.0528 - val_accuracy: 0.9849\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - ETA: 12s - loss: 0.1210 - accuracy: 0.960 - ETA: 12s - loss: 0.1207 - accuracy: 0.962 - ETA: 11s - loss: 0.1209 - accuracy: 0.961 - ETA: 10s - loss: 0.1230 - accuracy: 0.960 - ETA: 10s - loss: 0.1216 - accuracy: 0.960 - ETA: 10s - loss: 0.1170 - accuracy: 0.962 - ETA: 9s - loss: 0.1162 - accuracy: 0.961 - ETA: 9s - loss: 0.1161 - accuracy: 0.96 - ETA: 9s - loss: 0.1161 - accuracy: 0.96 - ETA: 8s - loss: 0.1153 - accuracy: 0.96 - ETA: 8s - loss: 0.1162 - accuracy: 0.96 - ETA: 8s - loss: 0.1147 - accuracy: 0.96 - ETA: 8s - loss: 0.1144 - accuracy: 0.96 - ETA: 7s - loss: 0.1165 - accuracy: 0.96 - ETA: 7s - loss: 0.1165 - accuracy: 0.96 - ETA: 7s - loss: 0.1164 - accuracy: 0.96 - ETA: 6s - loss: 0.1160 - accuracy: 0.96 - ETA: 6s - loss: 0.1158 - accuracy: 0.96 - ETA: 6s - loss: 0.1151 - accuracy: 0.96 - ETA: 5s - loss: 0.1161 - accuracy: 0.96 - ETA: 5s - loss: 0.1168 - accuracy: 0.96 - ETA: 5s - loss: 0.1164 - accuracy: 0.96 - ETA: 5s - loss: 0.1157 - accuracy: 0.96 - ETA: 4s - loss: 0.1144 - accuracy: 0.96 - ETA: 4s - loss: 0.1136 - accuracy: 0.96 - ETA: 4s - loss: 0.1127 - accuracy: 0.96 - ETA: 4s - loss: 0.1127 - accuracy: 0.96 - ETA: 3s - loss: 0.1128 - accuracy: 0.96 - ETA: 3s - loss: 0.1124 - accuracy: 0.96 - ETA: 3s - loss: 0.1122 - accuracy: 0.96 - ETA: 3s - loss: 0.1125 - accuracy: 0.96 - ETA: 2s - loss: 0.1125 - accuracy: 0.96 - ETA: 2s - loss: 0.1127 - accuracy: 0.96 - ETA: 2s - loss: 0.1126 - accuracy: 0.96 - ETA: 1s - loss: 0.1131 - accuracy: 0.96 - ETA: 1s - loss: 0.1128 - accuracy: 0.96 - ETA: 1s - loss: 0.1124 - accuracy: 0.96 - ETA: 1s - loss: 0.1123 - accuracy: 0.96 - ETA: 0s - loss: 0.1120 - accuracy: 0.96 - ETA: 0s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1122 - accuracy: 0.96 - 13s 317us/step - loss: 0.1123 - accuracy: 0.9637 - val_loss: 0.0510 - val_accuracy: 0.9851\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - ETA: 12s - loss: 0.1037 - accuracy: 0.971 - ETA: 11s - loss: 0.1127 - accuracy: 0.964 - ETA: 11s - loss: 0.1140 - accuracy: 0.964 - ETA: 11s - loss: 0.1153 - accuracy: 0.963 - ETA: 11s - loss: 0.1168 - accuracy: 0.964 - ETA: 10s - loss: 0.1156 - accuracy: 0.964 - ETA: 10s - loss: 0.1174 - accuracy: 0.963 - ETA: 10s - loss: 0.1180 - accuracy: 0.963 - ETA: 9s - loss: 0.1185 - accuracy: 0.962 - ETA: 9s - loss: 0.1178 - accuracy: 0.96 - ETA: 8s - loss: 0.1191 - accuracy: 0.96 - ETA: 8s - loss: 0.1172 - accuracy: 0.96 - ETA: 8s - loss: 0.1172 - accuracy: 0.96 - ETA: 8s - loss: 0.1171 - accuracy: 0.96 - ETA: 7s - loss: 0.1148 - accuracy: 0.96 - ETA: 7s - loss: 0.1155 - accuracy: 0.96 - ETA: 7s - loss: 0.1157 - accuracy: 0.96 - ETA: 6s - loss: 0.1148 - accuracy: 0.96 - ETA: 6s - loss: 0.1145 - accuracy: 0.96 - ETA: 6s - loss: 0.1159 - accuracy: 0.96 - ETA: 6s - loss: 0.1150 - accuracy: 0.96 - ETA: 5s - loss: 0.1145 - accuracy: 0.96 - ETA: 5s - loss: 0.1147 - accuracy: 0.96 - ETA: 5s - loss: 0.1135 - accuracy: 0.96 - ETA: 4s - loss: 0.1129 - accuracy: 0.96 - ETA: 4s - loss: 0.1127 - accuracy: 0.96 - ETA: 4s - loss: 0.1124 - accuracy: 0.96 - ETA: 4s - loss: 0.1122 - accuracy: 0.96 - ETA: 3s - loss: 0.1121 - accuracy: 0.96 - ETA: 3s - loss: 0.1119 - accuracy: 0.96 - ETA: 3s - loss: 0.1121 - accuracy: 0.96 - ETA: 2s - loss: 0.1123 - accuracy: 0.96 - ETA: 2s - loss: 0.1124 - accuracy: 0.96 - ETA: 2s - loss: 0.1127 - accuracy: 0.96 - ETA: 2s - loss: 0.1122 - accuracy: 0.96 - ETA: 1s - loss: 0.1123 - accuracy: 0.96 - ETA: 1s - loss: 0.1122 - accuracy: 0.96 - ETA: 1s - loss: 0.1123 - accuracy: 0.96 - ETA: 0s - loss: 0.1133 - accuracy: 0.96 - ETA: 0s - loss: 0.1133 - accuracy: 0.96 - ETA: 0s - loss: 0.1127 - accuracy: 0.96 - 14s 326us/step - loss: 0.1123 - accuracy: 0.9645 - val_loss: 0.0519 - val_accuracy: 0.9844\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1205 - accuracy: 0.965 - ETA: 10s - loss: 0.1271 - accuracy: 0.960 - ETA: 10s - loss: 0.1257 - accuracy: 0.962 - ETA: 10s - loss: 0.1180 - accuracy: 0.963 - ETA: 9s - loss: 0.1186 - accuracy: 0.963 - ETA: 9s - loss: 0.1175 - accuracy: 0.96 - ETA: 9s - loss: 0.1139 - accuracy: 0.96 - ETA: 9s - loss: 0.1143 - accuracy: 0.96 - ETA: 8s - loss: 0.1125 - accuracy: 0.96 - ETA: 8s - loss: 0.1141 - accuracy: 0.96 - ETA: 8s - loss: 0.1121 - accuracy: 0.96 - ETA: 8s - loss: 0.1120 - accuracy: 0.96 - ETA: 7s - loss: 0.1110 - accuracy: 0.96 - ETA: 7s - loss: 0.1103 - accuracy: 0.96 - ETA: 7s - loss: 0.1103 - accuracy: 0.96 - ETA: 6s - loss: 0.1112 - accuracy: 0.96 - ETA: 6s - loss: 0.1139 - accuracy: 0.96 - ETA: 6s - loss: 0.1131 - accuracy: 0.96 - ETA: 6s - loss: 0.1115 - accuracy: 0.96 - ETA: 5s - loss: 0.1109 - accuracy: 0.96 - ETA: 5s - loss: 0.1114 - accuracy: 0.96 - ETA: 5s - loss: 0.1120 - accuracy: 0.96 - ETA: 5s - loss: 0.1107 - accuracy: 0.96 - ETA: 4s - loss: 0.1103 - accuracy: 0.96 - ETA: 4s - loss: 0.1106 - accuracy: 0.96 - ETA: 4s - loss: 0.1104 - accuracy: 0.96 - ETA: 3s - loss: 0.1108 - accuracy: 0.96 - ETA: 3s - loss: 0.1105 - accuracy: 0.96 - ETA: 3s - loss: 0.1108 - accuracy: 0.96 - ETA: 3s - loss: 0.1106 - accuracy: 0.96 - ETA: 2s - loss: 0.1109 - accuracy: 0.96 - ETA: 2s - loss: 0.1108 - accuracy: 0.96 - ETA: 2s - loss: 0.1108 - accuracy: 0.96 - ETA: 2s - loss: 0.1102 - accuracy: 0.96 - ETA: 1s - loss: 0.1107 - accuracy: 0.96 - ETA: 1s - loss: 0.1102 - accuracy: 0.96 - ETA: 1s - loss: 0.1091 - accuracy: 0.96 - ETA: 1s - loss: 0.1107 - accuracy: 0.96 - ETA: 0s - loss: 0.1111 - accuracy: 0.96 - ETA: 0s - loss: 0.1113 - accuracy: 0.96 - ETA: 0s - loss: 0.1108 - accuracy: 0.96 - 12s 293us/step - loss: 0.1101 - accuracy: 0.9652 - val_loss: 0.0479 - val_accuracy: 0.9854\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1282 - accuracy: 0.955 - ETA: 10s - loss: 0.1299 - accuracy: 0.958 - ETA: 9s - loss: 0.1171 - accuracy: 0.963 - ETA: 9s - loss: 0.1054 - accuracy: 0.96 - ETA: 9s - loss: 0.1097 - accuracy: 0.96 - ETA: 9s - loss: 0.1069 - accuracy: 0.96 - ETA: 8s - loss: 0.1064 - accuracy: 0.96 - ETA: 8s - loss: 0.1049 - accuracy: 0.96 - ETA: 8s - loss: 0.1052 - accuracy: 0.96 - ETA: 8s - loss: 0.1062 - accuracy: 0.96 - ETA: 8s - loss: 0.1048 - accuracy: 0.96 - ETA: 8s - loss: 0.1051 - accuracy: 0.96 - ETA: 7s - loss: 0.1038 - accuracy: 0.96 - ETA: 7s - loss: 0.1030 - accuracy: 0.96 - ETA: 7s - loss: 0.1025 - accuracy: 0.96 - ETA: 6s - loss: 0.1027 - accuracy: 0.96 - ETA: 6s - loss: 0.1014 - accuracy: 0.96 - ETA: 6s - loss: 0.1006 - accuracy: 0.97 - ETA: 6s - loss: 0.1005 - accuracy: 0.97 - ETA: 6s - loss: 0.0999 - accuracy: 0.97 - ETA: 5s - loss: 0.1004 - accuracy: 0.97 - ETA: 5s - loss: 0.1016 - accuracy: 0.96 - ETA: 5s - loss: 0.1015 - accuracy: 0.96 - ETA: 4s - loss: 0.1013 - accuracy: 0.96 - ETA: 4s - loss: 0.1021 - accuracy: 0.96 - ETA: 4s - loss: 0.1018 - accuracy: 0.96 - ETA: 4s - loss: 0.1017 - accuracy: 0.96 - ETA: 3s - loss: 0.1012 - accuracy: 0.96 - ETA: 3s - loss: 0.1008 - accuracy: 0.96 - ETA: 3s - loss: 0.1006 - accuracy: 0.96 - ETA: 2s - loss: 0.1003 - accuracy: 0.96 - ETA: 2s - loss: 0.1002 - accuracy: 0.96 - ETA: 2s - loss: 0.1002 - accuracy: 0.96 - ETA: 2s - loss: 0.1010 - accuracy: 0.96 - ETA: 1s - loss: 0.1008 - accuracy: 0.96 - ETA: 1s - loss: 0.1004 - accuracy: 0.96 - ETA: 1s - loss: 0.1003 - accuracy: 0.96 - ETA: 1s - loss: 0.1011 - accuracy: 0.96 - ETA: 0s - loss: 0.1006 - accuracy: 0.96 - ETA: 0s - loss: 0.1003 - accuracy: 0.96 - ETA: 0s - loss: 0.1000 - accuracy: 0.96 - 13s 306us/step - loss: 0.1001 - accuracy: 0.9688 - val_loss: 0.0449 - val_accuracy: 0.9862\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1173 - accuracy: 0.960 - ETA: 10s - loss: 0.1032 - accuracy: 0.968 - ETA: 11s - loss: 0.0995 - accuracy: 0.970 - ETA: 10s - loss: 0.0981 - accuracy: 0.969 - ETA: 10s - loss: 0.0997 - accuracy: 0.969 - ETA: 10s - loss: 0.1002 - accuracy: 0.969 - ETA: 9s - loss: 0.0989 - accuracy: 0.969 - ETA: 9s - loss: 0.0966 - accuracy: 0.97 - ETA: 9s - loss: 0.0978 - accuracy: 0.97 - ETA: 8s - loss: 0.0999 - accuracy: 0.97 - ETA: 8s - loss: 0.0996 - accuracy: 0.96 - ETA: 8s - loss: 0.1002 - accuracy: 0.96 - ETA: 7s - loss: 0.0990 - accuracy: 0.96 - ETA: 7s - loss: 0.0969 - accuracy: 0.96 - ETA: 7s - loss: 0.0955 - accuracy: 0.97 - ETA: 7s - loss: 0.0973 - accuracy: 0.96 - ETA: 6s - loss: 0.0965 - accuracy: 0.96 - ETA: 6s - loss: 0.0950 - accuracy: 0.97 - ETA: 6s - loss: 0.0950 - accuracy: 0.97 - ETA: 6s - loss: 0.0949 - accuracy: 0.97 - ETA: 5s - loss: 0.0934 - accuracy: 0.97 - ETA: 5s - loss: 0.0941 - accuracy: 0.97 - ETA: 5s - loss: 0.0938 - accuracy: 0.97 - ETA: 4s - loss: 0.0948 - accuracy: 0.97 - ETA: 4s - loss: 0.0957 - accuracy: 0.97 - ETA: 4s - loss: 0.0952 - accuracy: 0.97 - ETA: 4s - loss: 0.0950 - accuracy: 0.97 - ETA: 3s - loss: 0.0950 - accuracy: 0.96 - ETA: 3s - loss: 0.0948 - accuracy: 0.97 - ETA: 3s - loss: 0.0957 - accuracy: 0.96 - ETA: 3s - loss: 0.0954 - accuracy: 0.96 - ETA: 2s - loss: 0.0955 - accuracy: 0.96 - ETA: 2s - loss: 0.0959 - accuracy: 0.96 - ETA: 2s - loss: 0.0966 - accuracy: 0.96 - ETA: 1s - loss: 0.0961 - accuracy: 0.96 - ETA: 1s - loss: 0.0964 - accuracy: 0.96 - ETA: 1s - loss: 0.0973 - accuracy: 0.96 - ETA: 1s - loss: 0.0974 - accuracy: 0.96 - ETA: 0s - loss: 0.0975 - accuracy: 0.96 - ETA: 0s - loss: 0.0971 - accuracy: 0.96 - ETA: 0s - loss: 0.0972 - accuracy: 0.96 - 13s 315us/step - loss: 0.0979 - accuracy: 0.9689 - val_loss: 0.0450 - val_accuracy: 0.9867\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.0641 - accuracy: 0.976 - ETA: 11s - loss: 0.0780 - accuracy: 0.973 - ETA: 11s - loss: 0.0904 - accuracy: 0.967 - ETA: 10s - loss: 0.0964 - accuracy: 0.966 - ETA: 10s - loss: 0.0934 - accuracy: 0.967 - ETA: 10s - loss: 0.0965 - accuracy: 0.967 - ETA: 10s - loss: 0.0961 - accuracy: 0.968 - ETA: 9s - loss: 0.0984 - accuracy: 0.967 - ETA: 9s - loss: 0.0987 - accuracy: 0.96 - ETA: 9s - loss: 0.0982 - accuracy: 0.96 - ETA: 9s - loss: 0.0998 - accuracy: 0.96 - ETA: 8s - loss: 0.0983 - accuracy: 0.96 - ETA: 8s - loss: 0.0989 - accuracy: 0.96 - ETA: 8s - loss: 0.0990 - accuracy: 0.96 - ETA: 7s - loss: 0.0976 - accuracy: 0.96 - ETA: 7s - loss: 0.0973 - accuracy: 0.96 - ETA: 7s - loss: 0.0976 - accuracy: 0.96 - ETA: 6s - loss: 0.0967 - accuracy: 0.96 - ETA: 6s - loss: 0.0966 - accuracy: 0.96 - ETA: 6s - loss: 0.0961 - accuracy: 0.96 - ETA: 6s - loss: 0.0967 - accuracy: 0.96 - ETA: 5s - loss: 0.0966 - accuracy: 0.96 - ETA: 5s - loss: 0.0962 - accuracy: 0.96 - ETA: 5s - loss: 0.0966 - accuracy: 0.96 - ETA: 4s - loss: 0.0966 - accuracy: 0.96 - ETA: 4s - loss: 0.0966 - accuracy: 0.96 - ETA: 4s - loss: 0.0971 - accuracy: 0.96 - ETA: 4s - loss: 0.0967 - accuracy: 0.96 - ETA: 3s - loss: 0.0967 - accuracy: 0.96 - ETA: 3s - loss: 0.0981 - accuracy: 0.96 - ETA: 3s - loss: 0.0980 - accuracy: 0.96 - ETA: 2s - loss: 0.0986 - accuracy: 0.96 - ETA: 2s - loss: 0.0980 - accuracy: 0.96 - ETA: 2s - loss: 0.0974 - accuracy: 0.96 - ETA: 2s - loss: 0.0975 - accuracy: 0.96 - ETA: 1s - loss: 0.0972 - accuracy: 0.96 - ETA: 1s - loss: 0.0973 - accuracy: 0.96 - ETA: 1s - loss: 0.0971 - accuracy: 0.96 - ETA: 0s - loss: 0.0979 - accuracy: 0.96 - ETA: 0s - loss: 0.0984 - accuracy: 0.96 - ETA: 0s - loss: 0.0979 - accuracy: 0.96 - 14s 331us/step - loss: 0.0978 - accuracy: 0.9685 - val_loss: 0.0446 - val_accuracy: 0.9869\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0829 - accuracy: 0.975 - ETA: 10s - loss: 0.0896 - accuracy: 0.971 - ETA: 9s - loss: 0.0954 - accuracy: 0.969 - ETA: 9s - loss: 0.0958 - accuracy: 0.96 - ETA: 9s - loss: 0.0915 - accuracy: 0.96 - ETA: 9s - loss: 0.0915 - accuracy: 0.97 - ETA: 9s - loss: 0.0900 - accuracy: 0.97 - ETA: 9s - loss: 0.0895 - accuracy: 0.97 - ETA: 9s - loss: 0.0901 - accuracy: 0.97 - ETA: 8s - loss: 0.0918 - accuracy: 0.97 - ETA: 8s - loss: 0.0925 - accuracy: 0.97 - ETA: 8s - loss: 0.0945 - accuracy: 0.97 - ETA: 8s - loss: 0.0940 - accuracy: 0.97 - ETA: 7s - loss: 0.0938 - accuracy: 0.97 - ETA: 7s - loss: 0.0938 - accuracy: 0.97 - ETA: 7s - loss: 0.0945 - accuracy: 0.96 - ETA: 6s - loss: 0.0928 - accuracy: 0.97 - ETA: 6s - loss: 0.0928 - accuracy: 0.97 - ETA: 6s - loss: 0.0920 - accuracy: 0.97 - ETA: 6s - loss: 0.0919 - accuracy: 0.97 - ETA: 5s - loss: 0.0920 - accuracy: 0.97 - ETA: 5s - loss: 0.0931 - accuracy: 0.97 - ETA: 5s - loss: 0.0929 - accuracy: 0.97 - ETA: 5s - loss: 0.0935 - accuracy: 0.96 - ETA: 4s - loss: 0.0931 - accuracy: 0.97 - ETA: 4s - loss: 0.0923 - accuracy: 0.97 - ETA: 4s - loss: 0.0920 - accuracy: 0.97 - ETA: 3s - loss: 0.0922 - accuracy: 0.97 - ETA: 3s - loss: 0.0930 - accuracy: 0.97 - ETA: 3s - loss: 0.0929 - accuracy: 0.97 - ETA: 3s - loss: 0.0928 - accuracy: 0.97 - ETA: 2s - loss: 0.0926 - accuracy: 0.97 - ETA: 2s - loss: 0.0929 - accuracy: 0.97 - ETA: 2s - loss: 0.0928 - accuracy: 0.97 - ETA: 1s - loss: 0.0921 - accuracy: 0.97 - ETA: 1s - loss: 0.0924 - accuracy: 0.97 - ETA: 1s - loss: 0.0929 - accuracy: 0.97 - ETA: 1s - loss: 0.0935 - accuracy: 0.97 - ETA: 0s - loss: 0.0936 - accuracy: 0.97 - ETA: 0s - loss: 0.0933 - accuracy: 0.97 - ETA: 0s - loss: 0.0938 - accuracy: 0.97 - 13s 320us/step - loss: 0.0934 - accuracy: 0.9706 - val_loss: 0.0421 - val_accuracy: 0.9872\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 13s - loss: 0.0744 - accuracy: 0.979 - ETA: 11s - loss: 0.0889 - accuracy: 0.974 - ETA: 10s - loss: 0.0942 - accuracy: 0.970 - ETA: 10s - loss: 0.0950 - accuracy: 0.969 - ETA: 10s - loss: 0.0923 - accuracy: 0.970 - ETA: 10s - loss: 0.0936 - accuracy: 0.971 - ETA: 10s - loss: 0.0909 - accuracy: 0.971 - ETA: 9s - loss: 0.0890 - accuracy: 0.971 - ETA: 9s - loss: 0.0864 - accuracy: 0.97 - ETA: 8s - loss: 0.0858 - accuracy: 0.97 - ETA: 8s - loss: 0.0865 - accuracy: 0.97 - ETA: 8s - loss: 0.0864 - accuracy: 0.97 - ETA: 8s - loss: 0.0881 - accuracy: 0.97 - ETA: 7s - loss: 0.0877 - accuracy: 0.97 - ETA: 7s - loss: 0.0881 - accuracy: 0.97 - ETA: 7s - loss: 0.0891 - accuracy: 0.97 - ETA: 6s - loss: 0.0892 - accuracy: 0.97 - ETA: 6s - loss: 0.0892 - accuracy: 0.97 - ETA: 6s - loss: 0.0895 - accuracy: 0.97 - ETA: 6s - loss: 0.0891 - accuracy: 0.97 - ETA: 5s - loss: 0.0886 - accuracy: 0.97 - ETA: 5s - loss: 0.0902 - accuracy: 0.97 - ETA: 5s - loss: 0.0898 - accuracy: 0.97 - ETA: 4s - loss: 0.0895 - accuracy: 0.97 - ETA: 4s - loss: 0.0889 - accuracy: 0.97 - ETA: 4s - loss: 0.0882 - accuracy: 0.97 - ETA: 4s - loss: 0.0888 - accuracy: 0.97 - ETA: 3s - loss: 0.0894 - accuracy: 0.97 - ETA: 3s - loss: 0.0891 - accuracy: 0.97 - ETA: 3s - loss: 0.0895 - accuracy: 0.97 - ETA: 3s - loss: 0.0890 - accuracy: 0.97 - ETA: 2s - loss: 0.0892 - accuracy: 0.97 - ETA: 2s - loss: 0.0893 - accuracy: 0.97 - ETA: 2s - loss: 0.0898 - accuracy: 0.97 - ETA: 1s - loss: 0.0899 - accuracy: 0.97 - ETA: 1s - loss: 0.0892 - accuracy: 0.97 - ETA: 1s - loss: 0.0902 - accuracy: 0.97 - ETA: 1s - loss: 0.0903 - accuracy: 0.97 - ETA: 0s - loss: 0.0906 - accuracy: 0.97 - ETA: 0s - loss: 0.0907 - accuracy: 0.97 - ETA: 0s - loss: 0.0910 - accuracy: 0.97 - 13s 317us/step - loss: 0.0908 - accuracy: 0.9720 - val_loss: 0.0408 - val_accuracy: 0.9881\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0901 - accuracy: 0.970 - ETA: 10s - loss: 0.0867 - accuracy: 0.969 - ETA: 9s - loss: 0.0814 - accuracy: 0.973 - ETA: 9s - loss: 0.0788 - accuracy: 0.97 - ETA: 9s - loss: 0.0780 - accuracy: 0.97 - ETA: 9s - loss: 0.0773 - accuracy: 0.97 - ETA: 8s - loss: 0.0811 - accuracy: 0.97 - ETA: 8s - loss: 0.0819 - accuracy: 0.97 - ETA: 8s - loss: 0.0839 - accuracy: 0.97 - ETA: 8s - loss: 0.0829 - accuracy: 0.97 - ETA: 7s - loss: 0.0824 - accuracy: 0.97 - ETA: 7s - loss: 0.0818 - accuracy: 0.97 - ETA: 7s - loss: 0.0836 - accuracy: 0.97 - ETA: 7s - loss: 0.0814 - accuracy: 0.97 - ETA: 7s - loss: 0.0824 - accuracy: 0.97 - ETA: 6s - loss: 0.0827 - accuracy: 0.97 - ETA: 6s - loss: 0.0825 - accuracy: 0.97 - ETA: 6s - loss: 0.0830 - accuracy: 0.97 - ETA: 6s - loss: 0.0834 - accuracy: 0.97 - ETA: 6s - loss: 0.0836 - accuracy: 0.97 - ETA: 5s - loss: 0.0831 - accuracy: 0.97 - ETA: 5s - loss: 0.0833 - accuracy: 0.97 - ETA: 5s - loss: 0.0836 - accuracy: 0.97 - ETA: 4s - loss: 0.0833 - accuracy: 0.97 - ETA: 4s - loss: 0.0840 - accuracy: 0.97 - ETA: 4s - loss: 0.0836 - accuracy: 0.97 - ETA: 4s - loss: 0.0843 - accuracy: 0.97 - ETA: 3s - loss: 0.0845 - accuracy: 0.97 - ETA: 3s - loss: 0.0847 - accuracy: 0.97 - ETA: 3s - loss: 0.0856 - accuracy: 0.97 - ETA: 3s - loss: 0.0861 - accuracy: 0.97 - ETA: 2s - loss: 0.0857 - accuracy: 0.97 - ETA: 2s - loss: 0.0862 - accuracy: 0.97 - ETA: 2s - loss: 0.0866 - accuracy: 0.97 - ETA: 1s - loss: 0.0870 - accuracy: 0.97 - ETA: 1s - loss: 0.0872 - accuracy: 0.97 - ETA: 1s - loss: 0.0874 - accuracy: 0.97 - ETA: 1s - loss: 0.0871 - accuracy: 0.97 - ETA: 0s - loss: 0.0871 - accuracy: 0.97 - ETA: 0s - loss: 0.0873 - accuracy: 0.97 - ETA: 0s - loss: 0.0882 - accuracy: 0.97 - 13s 308us/step - loss: 0.0875 - accuracy: 0.9724 - val_loss: 0.0395 - val_accuracy: 0.9888\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0730 - accuracy: 0.976 - ETA: 10s - loss: 0.0935 - accuracy: 0.970 - ETA: 10s - loss: 0.0978 - accuracy: 0.968 - ETA: 9s - loss: 0.0921 - accuracy: 0.970 - ETA: 9s - loss: 0.0878 - accuracy: 0.97 - ETA: 9s - loss: 0.0894 - accuracy: 0.97 - ETA: 9s - loss: 0.0926 - accuracy: 0.97 - ETA: 9s - loss: 0.0894 - accuracy: 0.97 - ETA: 9s - loss: 0.0916 - accuracy: 0.97 - ETA: 8s - loss: 0.0917 - accuracy: 0.97 - ETA: 8s - loss: 0.0892 - accuracy: 0.97 - ETA: 8s - loss: 0.0887 - accuracy: 0.97 - ETA: 7s - loss: 0.0882 - accuracy: 0.97 - ETA: 7s - loss: 0.0885 - accuracy: 0.97 - ETA: 7s - loss: 0.0892 - accuracy: 0.97 - ETA: 7s - loss: 0.0882 - accuracy: 0.97 - ETA: 6s - loss: 0.0899 - accuracy: 0.97 - ETA: 6s - loss: 0.0898 - accuracy: 0.97 - ETA: 6s - loss: 0.0894 - accuracy: 0.97 - ETA: 5s - loss: 0.0887 - accuracy: 0.97 - ETA: 5s - loss: 0.0902 - accuracy: 0.97 - ETA: 5s - loss: 0.0893 - accuracy: 0.97 - ETA: 5s - loss: 0.0887 - accuracy: 0.97 - ETA: 4s - loss: 0.0893 - accuracy: 0.97 - ETA: 4s - loss: 0.0883 - accuracy: 0.97 - ETA: 4s - loss: 0.0882 - accuracy: 0.97 - ETA: 4s - loss: 0.0872 - accuracy: 0.97 - ETA: 3s - loss: 0.0883 - accuracy: 0.97 - ETA: 3s - loss: 0.0876 - accuracy: 0.97 - ETA: 3s - loss: 0.0879 - accuracy: 0.97 - ETA: 2s - loss: 0.0875 - accuracy: 0.97 - ETA: 2s - loss: 0.0873 - accuracy: 0.97 - ETA: 2s - loss: 0.0879 - accuracy: 0.97 - ETA: 2s - loss: 0.0876 - accuracy: 0.97 - ETA: 1s - loss: 0.0881 - accuracy: 0.97 - ETA: 1s - loss: 0.0879 - accuracy: 0.97 - ETA: 1s - loss: 0.0883 - accuracy: 0.97 - ETA: 1s - loss: 0.0879 - accuracy: 0.97 - ETA: 0s - loss: 0.0878 - accuracy: 0.97 - ETA: 0s - loss: 0.0875 - accuracy: 0.97 - ETA: 0s - loss: 0.0871 - accuracy: 0.97 - 13s 314us/step - loss: 0.0864 - accuracy: 0.9726 - val_loss: 0.0397 - val_accuracy: 0.9878\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0831 - accuracy: 0.972 - ETA: 10s - loss: 0.0900 - accuracy: 0.969 - ETA: 10s - loss: 0.0908 - accuracy: 0.970 - ETA: 10s - loss: 0.0918 - accuracy: 0.970 - ETA: 10s - loss: 0.0856 - accuracy: 0.972 - ETA: 10s - loss: 0.0857 - accuracy: 0.973 - ETA: 9s - loss: 0.0841 - accuracy: 0.973 - ETA: 9s - loss: 0.0810 - accuracy: 0.97 - ETA: 8s - loss: 0.0808 - accuracy: 0.97 - ETA: 8s - loss: 0.0831 - accuracy: 0.97 - ETA: 8s - loss: 0.0845 - accuracy: 0.97 - ETA: 8s - loss: 0.0875 - accuracy: 0.97 - ETA: 7s - loss: 0.0878 - accuracy: 0.97 - ETA: 7s - loss: 0.0877 - accuracy: 0.97 - ETA: 7s - loss: 0.0877 - accuracy: 0.97 - ETA: 7s - loss: 0.0869 - accuracy: 0.97 - ETA: 6s - loss: 0.0865 - accuracy: 0.97 - ETA: 6s - loss: 0.0880 - accuracy: 0.97 - ETA: 6s - loss: 0.0875 - accuracy: 0.97 - ETA: 6s - loss: 0.0890 - accuracy: 0.97 - ETA: 5s - loss: 0.0875 - accuracy: 0.97 - ETA: 5s - loss: 0.0874 - accuracy: 0.97 - ETA: 5s - loss: 0.0865 - accuracy: 0.97 - ETA: 5s - loss: 0.0859 - accuracy: 0.97 - ETA: 4s - loss: 0.0853 - accuracy: 0.97 - ETA: 4s - loss: 0.0852 - accuracy: 0.97 - ETA: 4s - loss: 0.0855 - accuracy: 0.97 - ETA: 3s - loss: 0.0865 - accuracy: 0.97 - ETA: 3s - loss: 0.0858 - accuracy: 0.97 - ETA: 3s - loss: 0.0864 - accuracy: 0.97 - ETA: 3s - loss: 0.0862 - accuracy: 0.97 - ETA: 2s - loss: 0.0868 - accuracy: 0.97 - ETA: 2s - loss: 0.0877 - accuracy: 0.97 - ETA: 2s - loss: 0.0877 - accuracy: 0.97 - ETA: 1s - loss: 0.0878 - accuracy: 0.97 - ETA: 1s - loss: 0.0875 - accuracy: 0.97 - ETA: 1s - loss: 0.0876 - accuracy: 0.97 - ETA: 1s - loss: 0.0880 - accuracy: 0.97 - ETA: 0s - loss: 0.0879 - accuracy: 0.97 - ETA: 0s - loss: 0.0877 - accuracy: 0.97 - ETA: 0s - loss: 0.0873 - accuracy: 0.97 - 13s 313us/step - loss: 0.0873 - accuracy: 0.9720 - val_loss: 0.0386 - val_accuracy: 0.9886\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0887 - accuracy: 0.964 - ETA: 11s - loss: 0.0826 - accuracy: 0.968 - ETA: 11s - loss: 0.0876 - accuracy: 0.971 - ETA: 11s - loss: 0.0845 - accuracy: 0.971 - ETA: 10s - loss: 0.0826 - accuracy: 0.974 - ETA: 10s - loss: 0.0824 - accuracy: 0.974 - ETA: 10s - loss: 0.0816 - accuracy: 0.973 - ETA: 9s - loss: 0.0781 - accuracy: 0.974 - ETA: 9s - loss: 0.0810 - accuracy: 0.97 - ETA: 8s - loss: 0.0794 - accuracy: 0.97 - ETA: 8s - loss: 0.0799 - accuracy: 0.97 - ETA: 8s - loss: 0.0799 - accuracy: 0.97 - ETA: 8s - loss: 0.0794 - accuracy: 0.97 - ETA: 7s - loss: 0.0794 - accuracy: 0.97 - ETA: 7s - loss: 0.0790 - accuracy: 0.97 - ETA: 7s - loss: 0.0785 - accuracy: 0.97 - ETA: 6s - loss: 0.0786 - accuracy: 0.97 - ETA: 6s - loss: 0.0792 - accuracy: 0.97 - ETA: 6s - loss: 0.0789 - accuracy: 0.97 - ETA: 6s - loss: 0.0799 - accuracy: 0.97 - ETA: 5s - loss: 0.0806 - accuracy: 0.97 - ETA: 5s - loss: 0.0814 - accuracy: 0.97 - ETA: 5s - loss: 0.0804 - accuracy: 0.97 - ETA: 4s - loss: 0.0804 - accuracy: 0.97 - ETA: 4s - loss: 0.0803 - accuracy: 0.97 - ETA: 4s - loss: 0.0801 - accuracy: 0.97 - ETA: 4s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0799 - accuracy: 0.97 - ETA: 3s - loss: 0.0811 - accuracy: 0.97 - ETA: 3s - loss: 0.0816 - accuracy: 0.97 - ETA: 3s - loss: 0.0816 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0821 - accuracy: 0.97 - ETA: 1s - loss: 0.0835 - accuracy: 0.97 - ETA: 1s - loss: 0.0844 - accuracy: 0.97 - ETA: 1s - loss: 0.0840 - accuracy: 0.97 - ETA: 1s - loss: 0.0835 - accuracy: 0.97 - ETA: 0s - loss: 0.0837 - accuracy: 0.97 - ETA: 0s - loss: 0.0834 - accuracy: 0.97 - ETA: 0s - loss: 0.0835 - accuracy: 0.97 - 13s 308us/step - loss: 0.0832 - accuracy: 0.9739 - val_loss: 0.0378 - val_accuracy: 0.9884\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0734 - accuracy: 0.974 - ETA: 12s - loss: 0.0709 - accuracy: 0.973 - ETA: 11s - loss: 0.0696 - accuracy: 0.975 - ETA: 10s - loss: 0.0768 - accuracy: 0.973 - ETA: 10s - loss: 0.0742 - accuracy: 0.974 - ETA: 10s - loss: 0.0735 - accuracy: 0.974 - ETA: 9s - loss: 0.0757 - accuracy: 0.973 - ETA: 9s - loss: 0.0783 - accuracy: 0.97 - ETA: 9s - loss: 0.0773 - accuracy: 0.97 - ETA: 9s - loss: 0.0780 - accuracy: 0.97 - ETA: 8s - loss: 0.0771 - accuracy: 0.97 - ETA: 8s - loss: 0.0766 - accuracy: 0.97 - ETA: 8s - loss: 0.0764 - accuracy: 0.97 - ETA: 8s - loss: 0.0761 - accuracy: 0.97 - ETA: 7s - loss: 0.0757 - accuracy: 0.97 - ETA: 7s - loss: 0.0760 - accuracy: 0.97 - ETA: 7s - loss: 0.0744 - accuracy: 0.97 - ETA: 6s - loss: 0.0743 - accuracy: 0.97 - ETA: 6s - loss: 0.0760 - accuracy: 0.97 - ETA: 6s - loss: 0.0765 - accuracy: 0.97 - ETA: 6s - loss: 0.0762 - accuracy: 0.97 - ETA: 5s - loss: 0.0765 - accuracy: 0.97 - ETA: 5s - loss: 0.0782 - accuracy: 0.97 - ETA: 5s - loss: 0.0782 - accuracy: 0.97 - ETA: 4s - loss: 0.0781 - accuracy: 0.97 - ETA: 4s - loss: 0.0791 - accuracy: 0.97 - ETA: 4s - loss: 0.0789 - accuracy: 0.97 - ETA: 3s - loss: 0.0788 - accuracy: 0.97 - ETA: 3s - loss: 0.0795 - accuracy: 0.97 - ETA: 3s - loss: 0.0795 - accuracy: 0.97 - ETA: 3s - loss: 0.0796 - accuracy: 0.97 - ETA: 2s - loss: 0.0795 - accuracy: 0.97 - ETA: 2s - loss: 0.0797 - accuracy: 0.97 - ETA: 2s - loss: 0.0797 - accuracy: 0.97 - ETA: 1s - loss: 0.0796 - accuracy: 0.97 - ETA: 1s - loss: 0.0792 - accuracy: 0.97 - ETA: 1s - loss: 0.0799 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0798 - accuracy: 0.97 - ETA: 0s - loss: 0.0804 - accuracy: 0.97 - 13s 312us/step - loss: 0.0804 - accuracy: 0.9749 - val_loss: 0.0375 - val_accuracy: 0.9885\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.0736 - accuracy: 0.980 - ETA: 14s - loss: 0.0735 - accuracy: 0.979 - ETA: 13s - loss: 0.0719 - accuracy: 0.976 - ETA: 13s - loss: 0.0771 - accuracy: 0.975 - ETA: 13s - loss: 0.0750 - accuracy: 0.975 - ETA: 12s - loss: 0.0773 - accuracy: 0.975 - ETA: 12s - loss: 0.0799 - accuracy: 0.975 - ETA: 11s - loss: 0.0800 - accuracy: 0.975 - ETA: 11s - loss: 0.0801 - accuracy: 0.975 - ETA: 11s - loss: 0.0795 - accuracy: 0.975 - ETA: 10s - loss: 0.0818 - accuracy: 0.973 - ETA: 10s - loss: 0.0835 - accuracy: 0.973 - ETA: 10s - loss: 0.0831 - accuracy: 0.973 - ETA: 9s - loss: 0.0842 - accuracy: 0.973 - ETA: 9s - loss: 0.0841 - accuracy: 0.97 - ETA: 8s - loss: 0.0842 - accuracy: 0.97 - ETA: 8s - loss: 0.0838 - accuracy: 0.97 - ETA: 8s - loss: 0.0836 - accuracy: 0.97 - ETA: 7s - loss: 0.0824 - accuracy: 0.97 - ETA: 7s - loss: 0.0832 - accuracy: 0.97 - ETA: 6s - loss: 0.0845 - accuracy: 0.97 - ETA: 6s - loss: 0.0842 - accuracy: 0.97 - ETA: 6s - loss: 0.0841 - accuracy: 0.97 - ETA: 5s - loss: 0.0840 - accuracy: 0.97 - ETA: 5s - loss: 0.0834 - accuracy: 0.97 - ETA: 5s - loss: 0.0824 - accuracy: 0.97 - ETA: 4s - loss: 0.0831 - accuracy: 0.97 - ETA: 4s - loss: 0.0830 - accuracy: 0.97 - ETA: 4s - loss: 0.0824 - accuracy: 0.97 - ETA: 3s - loss: 0.0827 - accuracy: 0.97 - ETA: 3s - loss: 0.0824 - accuracy: 0.97 - ETA: 3s - loss: 0.0818 - accuracy: 0.97 - ETA: 2s - loss: 0.0820 - accuracy: 0.97 - ETA: 2s - loss: 0.0824 - accuracy: 0.97 - ETA: 2s - loss: 0.0820 - accuracy: 0.97 - ETA: 1s - loss: 0.0812 - accuracy: 0.97 - ETA: 1s - loss: 0.0811 - accuracy: 0.97 - ETA: 1s - loss: 0.0810 - accuracy: 0.97 - ETA: 0s - loss: 0.0815 - accuracy: 0.97 - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - ETA: 0s - loss: 0.0809 - accuracy: 0.97 - 15s 350us/step - loss: 0.0810 - accuracy: 0.9745 - val_loss: 0.0371 - val_accuracy: 0.9890\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0630 - accuracy: 0.977 - ETA: 9s - loss: 0.0748 - accuracy: 0.975 - ETA: 9s - loss: 0.0746 - accuracy: 0.97 - ETA: 9s - loss: 0.0775 - accuracy: 0.97 - ETA: 9s - loss: 0.0772 - accuracy: 0.97 - ETA: 9s - loss: 0.0778 - accuracy: 0.97 - ETA: 8s - loss: 0.0765 - accuracy: 0.97 - ETA: 8s - loss: 0.0796 - accuracy: 0.97 - ETA: 8s - loss: 0.0823 - accuracy: 0.97 - ETA: 8s - loss: 0.0831 - accuracy: 0.97 - ETA: 8s - loss: 0.0827 - accuracy: 0.97 - ETA: 8s - loss: 0.0823 - accuracy: 0.97 - ETA: 7s - loss: 0.0811 - accuracy: 0.97 - ETA: 7s - loss: 0.0808 - accuracy: 0.97 - ETA: 7s - loss: 0.0801 - accuracy: 0.97 - ETA: 7s - loss: 0.0802 - accuracy: 0.97 - ETA: 6s - loss: 0.0806 - accuracy: 0.97 - ETA: 6s - loss: 0.0801 - accuracy: 0.97 - ETA: 6s - loss: 0.0790 - accuracy: 0.97 - ETA: 6s - loss: 0.0784 - accuracy: 0.97 - ETA: 5s - loss: 0.0781 - accuracy: 0.97 - ETA: 5s - loss: 0.0783 - accuracy: 0.97 - ETA: 5s - loss: 0.0775 - accuracy: 0.97 - ETA: 4s - loss: 0.0778 - accuracy: 0.97 - ETA: 4s - loss: 0.0776 - accuracy: 0.97 - ETA: 4s - loss: 0.0773 - accuracy: 0.97 - ETA: 4s - loss: 0.0769 - accuracy: 0.97 - ETA: 3s - loss: 0.0767 - accuracy: 0.97 - ETA: 3s - loss: 0.0762 - accuracy: 0.97 - ETA: 3s - loss: 0.0763 - accuracy: 0.97 - ETA: 2s - loss: 0.0771 - accuracy: 0.97 - ETA: 2s - loss: 0.0763 - accuracy: 0.97 - ETA: 2s - loss: 0.0767 - accuracy: 0.97 - ETA: 2s - loss: 0.0769 - accuracy: 0.97 - ETA: 1s - loss: 0.0763 - accuracy: 0.97 - ETA: 1s - loss: 0.0761 - accuracy: 0.97 - ETA: 1s - loss: 0.0763 - accuracy: 0.97 - ETA: 1s - loss: 0.0759 - accuracy: 0.97 - ETA: 0s - loss: 0.0761 - accuracy: 0.97 - ETA: 0s - loss: 0.0763 - accuracy: 0.97 - ETA: 0s - loss: 0.0759 - accuracy: 0.97 - 13s 310us/step - loss: 0.0763 - accuracy: 0.9746 - val_loss: 0.0362 - val_accuracy: 0.9883\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1075 - accuracy: 0.96 - ETA: 10s - loss: 0.0877 - accuracy: 0.972 - ETA: 10s - loss: 0.0831 - accuracy: 0.974 - ETA: 9s - loss: 0.0756 - accuracy: 0.975 - ETA: 9s - loss: 0.0784 - accuracy: 0.97 - ETA: 9s - loss: 0.0788 - accuracy: 0.97 - ETA: 9s - loss: 0.0801 - accuracy: 0.97 - ETA: 8s - loss: 0.0804 - accuracy: 0.97 - ETA: 8s - loss: 0.0777 - accuracy: 0.97 - ETA: 8s - loss: 0.0756 - accuracy: 0.97 - ETA: 7s - loss: 0.0765 - accuracy: 0.97 - ETA: 7s - loss: 0.0773 - accuracy: 0.97 - ETA: 7s - loss: 0.0783 - accuracy: 0.97 - ETA: 7s - loss: 0.0780 - accuracy: 0.97 - ETA: 7s - loss: 0.0772 - accuracy: 0.97 - ETA: 6s - loss: 0.0771 - accuracy: 0.97 - ETA: 6s - loss: 0.0766 - accuracy: 0.97 - ETA: 6s - loss: 0.0770 - accuracy: 0.97 - ETA: 6s - loss: 0.0771 - accuracy: 0.97 - ETA: 5s - loss: 0.0780 - accuracy: 0.97 - ETA: 5s - loss: 0.0787 - accuracy: 0.97 - ETA: 5s - loss: 0.0783 - accuracy: 0.97 - ETA: 5s - loss: 0.0783 - accuracy: 0.97 - ETA: 4s - loss: 0.0784 - accuracy: 0.97 - ETA: 4s - loss: 0.0792 - accuracy: 0.97 - ETA: 4s - loss: 0.0789 - accuracy: 0.97 - ETA: 3s - loss: 0.0795 - accuracy: 0.97 - ETA: 3s - loss: 0.0797 - accuracy: 0.97 - ETA: 3s - loss: 0.0800 - accuracy: 0.97 - ETA: 3s - loss: 0.0801 - accuracy: 0.97 - ETA: 2s - loss: 0.0801 - accuracy: 0.97 - ETA: 2s - loss: 0.0796 - accuracy: 0.97 - ETA: 2s - loss: 0.0802 - accuracy: 0.97 - ETA: 2s - loss: 0.0801 - accuracy: 0.97 - ETA: 1s - loss: 0.0797 - accuracy: 0.97 - ETA: 1s - loss: 0.0793 - accuracy: 0.97 - ETA: 1s - loss: 0.0793 - accuracy: 0.97 - ETA: 1s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0782 - accuracy: 0.97 - ETA: 0s - loss: 0.0778 - accuracy: 0.97 - ETA: 0s - loss: 0.0777 - accuracy: 0.97 - 13s 301us/step - loss: 0.0779 - accuracy: 0.9755 - val_loss: 0.0359 - val_accuracy: 0.9887\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0757 - accuracy: 0.97 - ETA: 10s - loss: 0.0766 - accuracy: 0.977 - ETA: 10s - loss: 0.0770 - accuracy: 0.977 - ETA: 10s - loss: 0.0712 - accuracy: 0.979 - ETA: 9s - loss: 0.0727 - accuracy: 0.977 - ETA: 9s - loss: 0.0735 - accuracy: 0.97 - ETA: 9s - loss: 0.0744 - accuracy: 0.97 - ETA: 9s - loss: 0.0770 - accuracy: 0.97 - ETA: 8s - loss: 0.0755 - accuracy: 0.97 - ETA: 8s - loss: 0.0760 - accuracy: 0.97 - ETA: 8s - loss: 0.0750 - accuracy: 0.97 - ETA: 8s - loss: 0.0748 - accuracy: 0.97 - ETA: 8s - loss: 0.0752 - accuracy: 0.97 - ETA: 7s - loss: 0.0749 - accuracy: 0.97 - ETA: 7s - loss: 0.0769 - accuracy: 0.97 - ETA: 7s - loss: 0.0769 - accuracy: 0.97 - ETA: 7s - loss: 0.0755 - accuracy: 0.97 - ETA: 6s - loss: 0.0762 - accuracy: 0.97 - ETA: 6s - loss: 0.0762 - accuracy: 0.97 - ETA: 6s - loss: 0.0760 - accuracy: 0.97 - ETA: 5s - loss: 0.0759 - accuracy: 0.97 - ETA: 5s - loss: 0.0760 - accuracy: 0.97 - ETA: 5s - loss: 0.0752 - accuracy: 0.97 - ETA: 5s - loss: 0.0751 - accuracy: 0.97 - ETA: 4s - loss: 0.0751 - accuracy: 0.97 - ETA: 4s - loss: 0.0754 - accuracy: 0.97 - ETA: 4s - loss: 0.0760 - accuracy: 0.97 - ETA: 3s - loss: 0.0752 - accuracy: 0.97 - ETA: 3s - loss: 0.0749 - accuracy: 0.97 - ETA: 3s - loss: 0.0752 - accuracy: 0.97 - ETA: 3s - loss: 0.0748 - accuracy: 0.97 - ETA: 2s - loss: 0.0751 - accuracy: 0.97 - ETA: 2s - loss: 0.0747 - accuracy: 0.97 - ETA: 2s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0741 - accuracy: 0.97 - ETA: 1s - loss: 0.0743 - accuracy: 0.97 - ETA: 1s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0746 - accuracy: 0.97 - ETA: 0s - loss: 0.0744 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - 13s 311us/step - loss: 0.0742 - accuracy: 0.9768 - val_loss: 0.0342 - val_accuracy: 0.9898\n",
      "18000/18000 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 80us/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=1000, epochs=30, validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Label\n",
      "0   0      6\n",
      "1   1      7\n",
      "2   2      2\n",
      "3   3      9\n",
      "4   4      7\n",
      "(10000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_max_y.csv')\n",
    "train_images = torch.load('digitData.pkl')\n",
    "test_images = pd.read_pickle('test_max_x')\n",
    "\n",
    "print(train_labels.head())\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "y_pred = []\n",
    "for img in train_images:\n",
    "    digit_pred = [];\n",
    "    for digit in img:\n",
    "        digit = 255 - digit\n",
    "        digit = digit/255;\n",
    "#         print(np.argmax(model.predict(np.reshape(digit, (-1, 28, 28, 1)))))\n",
    "#         plt.imshow(digit, cmap='gray')\n",
    "#         plt.show()\n",
    "        \n",
    "        digit_pred.append(np.argmax(model.predict(np.reshape(digit, (-1, 28, 28, 1)))))\n",
    "    y_pred.append(np.amax(digit_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_labels['Label'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
