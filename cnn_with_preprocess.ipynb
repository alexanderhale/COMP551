{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# loading the original MNIST hand-written digits\n",
    "mndata = MNIST('')\n",
    "mndata.gz = True\n",
    "\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = np.asarray(images)\n",
    "imgs = np.reshape(imgs, (-1, 28, 28))\n",
    "# thresholds imgs\n",
    "imgs = np.where(imgs < 100, 0, 255)\n",
    "print(imgs.shape)\n",
    "\n",
    "plt.imshow(imgs[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3013: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1259: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2880: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 14)        140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 14)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 28)          9828      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 28)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 28)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 784)               352016    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 369,834\n",
      "Trainable params: 369,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(14, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(28, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(784, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(imgs).reshape(-1,28,28,1)\n",
    "\n",
    "# onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "# y_train = onehot_encoder.fit_transform(np.reshape(labels, (-1, 1)))\n",
    "\n",
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.30, random_state=0, stratify=labels)\n",
    "X_train = np.array(X_train)/255\n",
    "X_test = np.array(X_test)/255\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1,28,28,1)\n",
    "X_test = np.array(X_test).reshape(-1,28,28,1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(np.reshape(y_train, (-1,1)))\n",
    "y_test = onehot_encoder.transform(np.reshape(y_test, (-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:953: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - ETA: 25s - loss: 2.3569 - acc: 0.11 - ETA: 16s - loss: 2.3266 - acc: 0.12 - ETA: 13s - loss: 2.2983 - acc: 0.14 - ETA: 12s - loss: 2.2777 - acc: 0.15 - ETA: 11s - loss: 2.2551 - acc: 0.17 - ETA: 10s - loss: 2.2299 - acc: 0.19 - ETA: 9s - loss: 2.2050 - acc: 0.2201 - ETA: 9s - loss: 2.1738 - acc: 0.246 - ETA: 8s - loss: 2.1429 - acc: 0.266 - ETA: 8s - loss: 2.1116 - acc: 0.284 - ETA: 8s - loss: 2.0782 - acc: 0.303 - ETA: 7s - loss: 2.0413 - acc: 0.321 - ETA: 7s - loss: 2.0030 - acc: 0.337 - ETA: 7s - loss: 1.9633 - acc: 0.354 - ETA: 6s - loss: 1.9228 - acc: 0.368 - ETA: 6s - loss: 1.8868 - acc: 0.380 - ETA: 6s - loss: 1.8486 - acc: 0.393 - ETA: 6s - loss: 1.8116 - acc: 0.405 - ETA: 5s - loss: 1.7780 - acc: 0.416 - ETA: 5s - loss: 1.7451 - acc: 0.427 - ETA: 5s - loss: 1.7090 - acc: 0.437 - ETA: 4s - loss: 1.6738 - acc: 0.449 - ETA: 4s - loss: 1.6442 - acc: 0.458 - ETA: 4s - loss: 1.6123 - acc: 0.468 - ETA: 4s - loss: 1.5834 - acc: 0.477 - ETA: 3s - loss: 1.5555 - acc: 0.485 - ETA: 3s - loss: 1.5307 - acc: 0.494 - ETA: 3s - loss: 1.5070 - acc: 0.502 - ETA: 3s - loss: 1.4818 - acc: 0.510 - ETA: 2s - loss: 1.4598 - acc: 0.518 - ETA: 2s - loss: 1.4361 - acc: 0.526 - ETA: 2s - loss: 1.4127 - acc: 0.533 - ETA: 2s - loss: 1.3915 - acc: 0.540 - ETA: 2s - loss: 1.3714 - acc: 0.547 - ETA: 1s - loss: 1.3539 - acc: 0.553 - ETA: 1s - loss: 1.3354 - acc: 0.559 - ETA: 1s - loss: 1.3170 - acc: 0.565 - ETA: 1s - loss: 1.2994 - acc: 0.571 - ETA: 0s - loss: 1.2818 - acc: 0.577 - ETA: 0s - loss: 1.2659 - acc: 0.582 - ETA: 0s - loss: 1.2497 - acc: 0.587 - 15s 362us/step - loss: 1.2344 - acc: 0.5929\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - ETA: 31s - loss: 0.5787 - acc: 0.81 - ETA: 30s - loss: 0.5979 - acc: 0.81 - ETA: 29s - loss: 0.5798 - acc: 0.81 - ETA: 28s - loss: 0.5725 - acc: 0.81 - ETA: 27s - loss: 0.5629 - acc: 0.81 - ETA: 26s - loss: 0.5614 - acc: 0.81 - ETA: 25s - loss: 0.5602 - acc: 0.81 - ETA: 24s - loss: 0.5595 - acc: 0.82 - ETA: 23s - loss: 0.5515 - acc: 0.82 - ETA: 22s - loss: 0.5509 - acc: 0.82 - ETA: 21s - loss: 0.5482 - acc: 0.82 - ETA: 20s - loss: 0.5446 - acc: 0.82 - ETA: 19s - loss: 0.5408 - acc: 0.82 - ETA: 18s - loss: 0.5417 - acc: 0.82 - ETA: 17s - loss: 0.5388 - acc: 0.82 - ETA: 16s - loss: 0.5364 - acc: 0.82 - ETA: 15s - loss: 0.5323 - acc: 0.82 - ETA: 15s - loss: 0.5288 - acc: 0.82 - ETA: 14s - loss: 0.5231 - acc: 0.83 - ETA: 13s - loss: 0.5188 - acc: 0.83 - ETA: 12s - loss: 0.5179 - acc: 0.83 - ETA: 12s - loss: 0.5122 - acc: 0.83 - ETA: 11s - loss: 0.5095 - acc: 0.83 - ETA: 10s - loss: 0.5061 - acc: 0.83 - ETA: 9s - loss: 0.5034 - acc: 0.8390 - ETA: 9s - loss: 0.5015 - acc: 0.840 - ETA: 8s - loss: 0.4983 - acc: 0.841 - ETA: 7s - loss: 0.4960 - acc: 0.842 - ETA: 7s - loss: 0.4932 - acc: 0.843 - ETA: 6s - loss: 0.4902 - acc: 0.844 - ETA: 6s - loss: 0.4875 - acc: 0.846 - ETA: 5s - loss: 0.4842 - acc: 0.846 - ETA: 4s - loss: 0.4807 - acc: 0.848 - ETA: 4s - loss: 0.4784 - acc: 0.848 - ETA: 3s - loss: 0.4763 - acc: 0.849 - ETA: 3s - loss: 0.4728 - acc: 0.850 - ETA: 2s - loss: 0.4709 - acc: 0.851 - ETA: 2s - loss: 0.4684 - acc: 0.852 - ETA: 1s - loss: 0.4670 - acc: 0.852 - ETA: 1s - loss: 0.4661 - acc: 0.853 - ETA: 0s - loss: 0.4635 - acc: 0.853 - 21s 501us/step - loss: 0.4616 - acc: 0.8544\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.3320 - acc: 0.895 - ETA: 9s - loss: 0.3492 - acc: 0.891 - ETA: 8s - loss: 0.3526 - acc: 0.887 - ETA: 8s - loss: 0.3537 - acc: 0.888 - ETA: 8s - loss: 0.3600 - acc: 0.883 - ETA: 8s - loss: 0.3605 - acc: 0.882 - ETA: 8s - loss: 0.3633 - acc: 0.881 - ETA: 7s - loss: 0.3603 - acc: 0.884 - ETA: 7s - loss: 0.3572 - acc: 0.886 - ETA: 7s - loss: 0.3521 - acc: 0.887 - ETA: 7s - loss: 0.3534 - acc: 0.886 - ETA: 7s - loss: 0.3510 - acc: 0.887 - ETA: 7s - loss: 0.3533 - acc: 0.887 - ETA: 8s - loss: 0.3491 - acc: 0.887 - ETA: 8s - loss: 0.3480 - acc: 0.887 - ETA: 8s - loss: 0.3453 - acc: 0.888 - ETA: 9s - loss: 0.3448 - acc: 0.888 - ETA: 9s - loss: 0.3431 - acc: 0.889 - ETA: 9s - loss: 0.3411 - acc: 0.890 - ETA: 9s - loss: 0.3396 - acc: 0.890 - ETA: 9s - loss: 0.3356 - acc: 0.892 - ETA: 9s - loss: 0.3352 - acc: 0.892 - ETA: 8s - loss: 0.3351 - acc: 0.892 - ETA: 8s - loss: 0.3336 - acc: 0.893 - ETA: 8s - loss: 0.3317 - acc: 0.894 - ETA: 8s - loss: 0.3309 - acc: 0.894 - ETA: 7s - loss: 0.3301 - acc: 0.895 - ETA: 7s - loss: 0.3297 - acc: 0.895 - ETA: 6s - loss: 0.3265 - acc: 0.896 - ETA: 6s - loss: 0.3255 - acc: 0.896 - ETA: 5s - loss: 0.3255 - acc: 0.896 - ETA: 5s - loss: 0.3242 - acc: 0.896 - ETA: 4s - loss: 0.3234 - acc: 0.897 - ETA: 4s - loss: 0.3233 - acc: 0.897 - ETA: 3s - loss: 0.3214 - acc: 0.897 - ETA: 3s - loss: 0.3203 - acc: 0.898 - ETA: 2s - loss: 0.3198 - acc: 0.898 - ETA: 2s - loss: 0.3197 - acc: 0.898 - ETA: 1s - loss: 0.3187 - acc: 0.898 - ETA: 1s - loss: 0.3188 - acc: 0.899 - ETA: 0s - loss: 0.3177 - acc: 0.899 - 22s 533us/step - loss: 0.3173 - acc: 0.8996\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - ETA: 20s - loss: 0.2515 - acc: 0.91 - ETA: 19s - loss: 0.2497 - acc: 0.91 - ETA: 18s - loss: 0.2654 - acc: 0.91 - ETA: 17s - loss: 0.2629 - acc: 0.91 - ETA: 17s - loss: 0.2579 - acc: 0.91 - ETA: 16s - loss: 0.2639 - acc: 0.91 - ETA: 15s - loss: 0.2710 - acc: 0.91 - ETA: 15s - loss: 0.2695 - acc: 0.91 - ETA: 14s - loss: 0.2697 - acc: 0.91 - ETA: 14s - loss: 0.2715 - acc: 0.91 - ETA: 13s - loss: 0.2679 - acc: 0.91 - ETA: 13s - loss: 0.2687 - acc: 0.91 - ETA: 12s - loss: 0.2687 - acc: 0.91 - ETA: 12s - loss: 0.2713 - acc: 0.91 - ETA: 11s - loss: 0.2723 - acc: 0.91 - ETA: 11s - loss: 0.2721 - acc: 0.91 - ETA: 10s - loss: 0.2707 - acc: 0.91 - ETA: 10s - loss: 0.2705 - acc: 0.91 - ETA: 9s - loss: 0.2717 - acc: 0.9156 - ETA: 9s - loss: 0.2716 - acc: 0.915 - ETA: 8s - loss: 0.2697 - acc: 0.916 - ETA: 8s - loss: 0.2677 - acc: 0.916 - ETA: 7s - loss: 0.2653 - acc: 0.917 - ETA: 7s - loss: 0.2621 - acc: 0.918 - ETA: 6s - loss: 0.2608 - acc: 0.918 - ETA: 6s - loss: 0.2608 - acc: 0.918 - ETA: 5s - loss: 0.2608 - acc: 0.918 - ETA: 5s - loss: 0.2606 - acc: 0.918 - ETA: 4s - loss: 0.2597 - acc: 0.918 - ETA: 4s - loss: 0.2598 - acc: 0.918 - ETA: 3s - loss: 0.2598 - acc: 0.918 - ETA: 3s - loss: 0.2598 - acc: 0.918 - ETA: 3s - loss: 0.2603 - acc: 0.918 - ETA: 2s - loss: 0.2586 - acc: 0.918 - ETA: 2s - loss: 0.2585 - acc: 0.918 - ETA: 2s - loss: 0.2585 - acc: 0.918 - ETA: 1s - loss: 0.2578 - acc: 0.919 - ETA: 1s - loss: 0.2577 - acc: 0.919 - ETA: 1s - loss: 0.2571 - acc: 0.919 - ETA: 0s - loss: 0.2574 - acc: 0.919 - ETA: 0s - loss: 0.2562 - acc: 0.920 - 17s 416us/step - loss: 0.2562 - acc: 0.9198\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 31s - loss: 0.2384 - acc: 0.93 - ETA: 30s - loss: 0.2507 - acc: 0.92 - ETA: 29s - loss: 0.2341 - acc: 0.93 - ETA: 29s - loss: 0.2318 - acc: 0.93 - ETA: 28s - loss: 0.2459 - acc: 0.92 - ETA: 27s - loss: 0.2469 - acc: 0.92 - ETA: 26s - loss: 0.2453 - acc: 0.92 - ETA: 25s - loss: 0.2466 - acc: 0.92 - ETA: 24s - loss: 0.2411 - acc: 0.92 - ETA: 23s - loss: 0.2388 - acc: 0.92 - ETA: 22s - loss: 0.2366 - acc: 0.92 - ETA: 21s - loss: 0.2397 - acc: 0.92 - ETA: 20s - loss: 0.2381 - acc: 0.92 - ETA: 19s - loss: 0.2358 - acc: 0.92 - ETA: 18s - loss: 0.2339 - acc: 0.92 - ETA: 17s - loss: 0.2320 - acc: 0.92 - ETA: 16s - loss: 0.2321 - acc: 0.92 - ETA: 16s - loss: 0.2334 - acc: 0.92 - ETA: 15s - loss: 0.2337 - acc: 0.92 - ETA: 14s - loss: 0.2328 - acc: 0.92 - ETA: 13s - loss: 0.2318 - acc: 0.92 - ETA: 12s - loss: 0.2317 - acc: 0.92 - ETA: 11s - loss: 0.2327 - acc: 0.92 - ETA: 11s - loss: 0.2315 - acc: 0.92 - ETA: 10s - loss: 0.2311 - acc: 0.92 - ETA: 9s - loss: 0.2299 - acc: 0.9295 - ETA: 9s - loss: 0.2296 - acc: 0.929 - ETA: 8s - loss: 0.2288 - acc: 0.929 - ETA: 7s - loss: 0.2283 - acc: 0.929 - ETA: 7s - loss: 0.2286 - acc: 0.929 - ETA: 6s - loss: 0.2276 - acc: 0.930 - ETA: 5s - loss: 0.2276 - acc: 0.930 - ETA: 5s - loss: 0.2280 - acc: 0.929 - ETA: 4s - loss: 0.2286 - acc: 0.929 - ETA: 3s - loss: 0.2276 - acc: 0.930 - ETA: 3s - loss: 0.2271 - acc: 0.930 - ETA: 2s - loss: 0.2274 - acc: 0.930 - ETA: 2s - loss: 0.2278 - acc: 0.930 - ETA: 1s - loss: 0.2272 - acc: 0.930 - ETA: 1s - loss: 0.2269 - acc: 0.930 - ETA: 0s - loss: 0.2263 - acc: 0.930 - 23s 543us/step - loss: 0.2261 - acc: 0.9303\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.2086 - acc: 0.94 - ETA: 12s - loss: 0.2153 - acc: 0.93 - ETA: 11s - loss: 0.2118 - acc: 0.93 - ETA: 10s - loss: 0.2111 - acc: 0.93 - ETA: 9s - loss: 0.2100 - acc: 0.9350 - ETA: 9s - loss: 0.2054 - acc: 0.936 - ETA: 9s - loss: 0.2027 - acc: 0.936 - ETA: 8s - loss: 0.2062 - acc: 0.934 - ETA: 8s - loss: 0.2040 - acc: 0.935 - ETA: 8s - loss: 0.2048 - acc: 0.935 - ETA: 7s - loss: 0.2049 - acc: 0.934 - ETA: 7s - loss: 0.2046 - acc: 0.934 - ETA: 7s - loss: 0.2078 - acc: 0.934 - ETA: 6s - loss: 0.2091 - acc: 0.934 - ETA: 6s - loss: 0.2071 - acc: 0.935 - ETA: 6s - loss: 0.2047 - acc: 0.935 - ETA: 6s - loss: 0.2023 - acc: 0.936 - ETA: 5s - loss: 0.2019 - acc: 0.936 - ETA: 5s - loss: 0.2012 - acc: 0.936 - ETA: 5s - loss: 0.2014 - acc: 0.936 - ETA: 5s - loss: 0.2005 - acc: 0.936 - ETA: 5s - loss: 0.1997 - acc: 0.937 - ETA: 5s - loss: 0.2006 - acc: 0.937 - ETA: 4s - loss: 0.2016 - acc: 0.937 - ETA: 4s - loss: 0.2011 - acc: 0.936 - ETA: 4s - loss: 0.2007 - acc: 0.936 - ETA: 4s - loss: 0.2001 - acc: 0.936 - ETA: 3s - loss: 0.2001 - acc: 0.936 - ETA: 3s - loss: 0.1988 - acc: 0.937 - ETA: 3s - loss: 0.1993 - acc: 0.936 - ETA: 2s - loss: 0.2002 - acc: 0.936 - ETA: 2s - loss: 0.2002 - acc: 0.936 - ETA: 2s - loss: 0.2000 - acc: 0.936 - ETA: 2s - loss: 0.1995 - acc: 0.936 - ETA: 1s - loss: 0.2000 - acc: 0.936 - ETA: 1s - loss: 0.1990 - acc: 0.937 - ETA: 1s - loss: 0.1981 - acc: 0.937 - ETA: 1s - loss: 0.1972 - acc: 0.937 - ETA: 0s - loss: 0.1977 - acc: 0.937 - ETA: 0s - loss: 0.1991 - acc: 0.937 - ETA: 0s - loss: 0.1991 - acc: 0.937 - 11s 258us/step - loss: 0.1994 - acc: 0.9374\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.2245 - acc: 0.929 - ETA: 9s - loss: 0.1983 - acc: 0.938 - ETA: 9s - loss: 0.1958 - acc: 0.940 - ETA: 9s - loss: 0.1961 - acc: 0.940 - ETA: 8s - loss: 0.1936 - acc: 0.940 - ETA: 8s - loss: 0.1888 - acc: 0.941 - ETA: 8s - loss: 0.1864 - acc: 0.943 - ETA: 10s - loss: 0.1895 - acc: 0.94 - ETA: 11s - loss: 0.1892 - acc: 0.94 - ETA: 12s - loss: 0.1859 - acc: 0.94 - ETA: 12s - loss: 0.1837 - acc: 0.94 - ETA: 13s - loss: 0.1861 - acc: 0.94 - ETA: 13s - loss: 0.1830 - acc: 0.94 - ETA: 13s - loss: 0.1840 - acc: 0.94 - ETA: 13s - loss: 0.1835 - acc: 0.94 - ETA: 13s - loss: 0.1827 - acc: 0.94 - ETA: 12s - loss: 0.1818 - acc: 0.94 - ETA: 12s - loss: 0.1821 - acc: 0.94 - ETA: 12s - loss: 0.1813 - acc: 0.94 - ETA: 12s - loss: 0.1823 - acc: 0.94 - ETA: 11s - loss: 0.1815 - acc: 0.94 - ETA: 11s - loss: 0.1806 - acc: 0.94 - ETA: 10s - loss: 0.1813 - acc: 0.94 - ETA: 10s - loss: 0.1810 - acc: 0.94 - ETA: 9s - loss: 0.1811 - acc: 0.9440 - ETA: 9s - loss: 0.1817 - acc: 0.944 - ETA: 8s - loss: 0.1836 - acc: 0.943 - ETA: 8s - loss: 0.1836 - acc: 0.943 - ETA: 7s - loss: 0.1844 - acc: 0.942 - ETA: 6s - loss: 0.1848 - acc: 0.942 - ETA: 6s - loss: 0.1860 - acc: 0.942 - ETA: 5s - loss: 0.1864 - acc: 0.942 - ETA: 5s - loss: 0.1860 - acc: 0.942 - ETA: 4s - loss: 0.1860 - acc: 0.942 - ETA: 4s - loss: 0.1848 - acc: 0.942 - ETA: 3s - loss: 0.1845 - acc: 0.942 - ETA: 2s - loss: 0.1846 - acc: 0.942 - ETA: 2s - loss: 0.1843 - acc: 0.942 - ETA: 1s - loss: 0.1840 - acc: 0.942 - ETA: 1s - loss: 0.1843 - acc: 0.942 - ETA: 0s - loss: 0.1844 - acc: 0.942 - 23s 554us/step - loss: 0.1845 - acc: 0.9425\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - ETA: 17s - loss: 0.1736 - acc: 0.94 - ETA: 17s - loss: 0.1674 - acc: 0.94 - ETA: 16s - loss: 0.1747 - acc: 0.94 - ETA: 16s - loss: 0.1795 - acc: 0.94 - ETA: 15s - loss: 0.1774 - acc: 0.94 - ETA: 15s - loss: 0.1779 - acc: 0.94 - ETA: 14s - loss: 0.1781 - acc: 0.94 - ETA: 14s - loss: 0.1770 - acc: 0.94 - ETA: 13s - loss: 0.1768 - acc: 0.94 - ETA: 13s - loss: 0.1741 - acc: 0.94 - ETA: 12s - loss: 0.1757 - acc: 0.94 - ETA: 12s - loss: 0.1766 - acc: 0.94 - ETA: 11s - loss: 0.1762 - acc: 0.94 - ETA: 11s - loss: 0.1765 - acc: 0.94 - ETA: 10s - loss: 0.1761 - acc: 0.94 - ETA: 10s - loss: 0.1761 - acc: 0.94 - ETA: 9s - loss: 0.1740 - acc: 0.9442 - ETA: 9s - loss: 0.1734 - acc: 0.944 - ETA: 8s - loss: 0.1725 - acc: 0.944 - ETA: 8s - loss: 0.1728 - acc: 0.945 - ETA: 7s - loss: 0.1731 - acc: 0.945 - ETA: 7s - loss: 0.1730 - acc: 0.945 - ETA: 6s - loss: 0.1730 - acc: 0.945 - ETA: 6s - loss: 0.1728 - acc: 0.945 - ETA: 5s - loss: 0.1730 - acc: 0.945 - ETA: 5s - loss: 0.1717 - acc: 0.945 - ETA: 4s - loss: 0.1708 - acc: 0.945 - ETA: 4s - loss: 0.1718 - acc: 0.945 - ETA: 4s - loss: 0.1714 - acc: 0.945 - ETA: 3s - loss: 0.1709 - acc: 0.945 - ETA: 3s - loss: 0.1702 - acc: 0.945 - ETA: 3s - loss: 0.1708 - acc: 0.945 - ETA: 2s - loss: 0.1704 - acc: 0.946 - ETA: 2s - loss: 0.1703 - acc: 0.945 - ETA: 2s - loss: 0.1709 - acc: 0.945 - ETA: 1s - loss: 0.1724 - acc: 0.945 - ETA: 1s - loss: 0.1731 - acc: 0.945 - ETA: 1s - loss: 0.1731 - acc: 0.945 - ETA: 0s - loss: 0.1725 - acc: 0.945 - ETA: 0s - loss: 0.1732 - acc: 0.945 - ETA: 0s - loss: 0.1728 - acc: 0.945 - 13s 304us/step - loss: 0.1726 - acc: 0.9454\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 15s - loss: 0.1516 - acc: 0.95 - ETA: 14s - loss: 0.1603 - acc: 0.94 - ETA: 14s - loss: 0.1613 - acc: 0.94 - ETA: 13s - loss: 0.1598 - acc: 0.94 - ETA: 12s - loss: 0.1612 - acc: 0.94 - ETA: 11s - loss: 0.1618 - acc: 0.95 - ETA: 11s - loss: 0.1559 - acc: 0.95 - ETA: 10s - loss: 0.1577 - acc: 0.95 - ETA: 9s - loss: 0.1555 - acc: 0.9524 - ETA: 9s - loss: 0.1543 - acc: 0.952 - ETA: 8s - loss: 0.1541 - acc: 0.952 - ETA: 8s - loss: 0.1544 - acc: 0.952 - ETA: 8s - loss: 0.1543 - acc: 0.952 - ETA: 7s - loss: 0.1557 - acc: 0.951 - ETA: 7s - loss: 0.1553 - acc: 0.951 - ETA: 6s - loss: 0.1533 - acc: 0.952 - ETA: 6s - loss: 0.1529 - acc: 0.952 - ETA: 6s - loss: 0.1541 - acc: 0.951 - ETA: 6s - loss: 0.1538 - acc: 0.951 - ETA: 6s - loss: 0.1537 - acc: 0.951 - ETA: 5s - loss: 0.1542 - acc: 0.951 - ETA: 5s - loss: 0.1552 - acc: 0.951 - ETA: 5s - loss: 0.1554 - acc: 0.950 - ETA: 5s - loss: 0.1560 - acc: 0.950 - ETA: 5s - loss: 0.1578 - acc: 0.950 - ETA: 4s - loss: 0.1591 - acc: 0.949 - ETA: 4s - loss: 0.1593 - acc: 0.949 - ETA: 4s - loss: 0.1590 - acc: 0.949 - ETA: 3s - loss: 0.1595 - acc: 0.950 - ETA: 3s - loss: 0.1584 - acc: 0.950 - ETA: 3s - loss: 0.1585 - acc: 0.950 - ETA: 2s - loss: 0.1581 - acc: 0.950 - ETA: 2s - loss: 0.1586 - acc: 0.950 - ETA: 2s - loss: 0.1580 - acc: 0.950 - ETA: 1s - loss: 0.1583 - acc: 0.950 - ETA: 1s - loss: 0.1582 - acc: 0.950 - ETA: 1s - loss: 0.1587 - acc: 0.950 - ETA: 1s - loss: 0.1578 - acc: 0.950 - ETA: 0s - loss: 0.1583 - acc: 0.950 - ETA: 0s - loss: 0.1579 - acc: 0.950 - ETA: 0s - loss: 0.1579 - acc: 0.950 - 11s 271us/step - loss: 0.1574 - acc: 0.9509\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1334 - acc: 0.954 - ETA: 9s - loss: 0.1509 - acc: 0.949 - ETA: 8s - loss: 0.1516 - acc: 0.948 - ETA: 8s - loss: 0.1572 - acc: 0.949 - ETA: 8s - loss: 0.1541 - acc: 0.951 - ETA: 9s - loss: 0.1543 - acc: 0.952 - ETA: 9s - loss: 0.1545 - acc: 0.953 - ETA: 9s - loss: 0.1550 - acc: 0.953 - ETA: 9s - loss: 0.1533 - acc: 0.953 - ETA: 9s - loss: 0.1512 - acc: 0.954 - ETA: 9s - loss: 0.1530 - acc: 0.953 - ETA: 9s - loss: 0.1547 - acc: 0.952 - ETA: 8s - loss: 0.1556 - acc: 0.952 - ETA: 8s - loss: 0.1541 - acc: 0.952 - ETA: 7s - loss: 0.1534 - acc: 0.952 - ETA: 7s - loss: 0.1547 - acc: 0.951 - ETA: 7s - loss: 0.1552 - acc: 0.951 - ETA: 6s - loss: 0.1546 - acc: 0.951 - ETA: 6s - loss: 0.1546 - acc: 0.951 - ETA: 6s - loss: 0.1537 - acc: 0.951 - ETA: 5s - loss: 0.1541 - acc: 0.951 - ETA: 5s - loss: 0.1531 - acc: 0.951 - ETA: 5s - loss: 0.1535 - acc: 0.951 - ETA: 4s - loss: 0.1554 - acc: 0.950 - ETA: 4s - loss: 0.1547 - acc: 0.951 - ETA: 4s - loss: 0.1553 - acc: 0.950 - ETA: 4s - loss: 0.1560 - acc: 0.951 - ETA: 3s - loss: 0.1555 - acc: 0.951 - ETA: 3s - loss: 0.1549 - acc: 0.951 - ETA: 3s - loss: 0.1540 - acc: 0.951 - ETA: 2s - loss: 0.1541 - acc: 0.951 - ETA: 2s - loss: 0.1551 - acc: 0.951 - ETA: 2s - loss: 0.1548 - acc: 0.951 - ETA: 2s - loss: 0.1544 - acc: 0.951 - ETA: 1s - loss: 0.1546 - acc: 0.951 - ETA: 1s - loss: 0.1538 - acc: 0.951 - ETA: 1s - loss: 0.1547 - acc: 0.951 - ETA: 1s - loss: 0.1538 - acc: 0.951 - ETA: 0s - loss: 0.1531 - acc: 0.951 - ETA: 0s - loss: 0.1528 - acc: 0.952 - ETA: 0s - loss: 0.1529 - acc: 0.952 - 12s 278us/step - loss: 0.1527 - acc: 0.9522\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1531 - acc: 0.95 - ETA: 10s - loss: 0.1703 - acc: 0.94 - ETA: 9s - loss: 0.1519 - acc: 0.9550 - ETA: 9s - loss: 0.1484 - acc: 0.955 - ETA: 8s - loss: 0.1549 - acc: 0.953 - ETA: 8s - loss: 0.1477 - acc: 0.955 - ETA: 9s - loss: 0.1438 - acc: 0.955 - ETA: 9s - loss: 0.1441 - acc: 0.955 - ETA: 9s - loss: 0.1430 - acc: 0.955 - ETA: 9s - loss: 0.1452 - acc: 0.954 - ETA: 9s - loss: 0.1453 - acc: 0.954 - ETA: 9s - loss: 0.1448 - acc: 0.955 - ETA: 8s - loss: 0.1437 - acc: 0.955 - ETA: 8s - loss: 0.1432 - acc: 0.955 - ETA: 8s - loss: 0.1438 - acc: 0.955 - ETA: 7s - loss: 0.1447 - acc: 0.954 - ETA: 7s - loss: 0.1435 - acc: 0.954 - ETA: 6s - loss: 0.1426 - acc: 0.954 - ETA: 6s - loss: 0.1434 - acc: 0.954 - ETA: 6s - loss: 0.1457 - acc: 0.953 - ETA: 6s - loss: 0.1456 - acc: 0.953 - ETA: 6s - loss: 0.1451 - acc: 0.954 - ETA: 5s - loss: 0.1440 - acc: 0.953 - ETA: 5s - loss: 0.1450 - acc: 0.953 - ETA: 5s - loss: 0.1440 - acc: 0.954 - ETA: 4s - loss: 0.1442 - acc: 0.953 - ETA: 4s - loss: 0.1449 - acc: 0.953 - ETA: 4s - loss: 0.1452 - acc: 0.953 - ETA: 3s - loss: 0.1454 - acc: 0.953 - ETA: 3s - loss: 0.1440 - acc: 0.953 - ETA: 3s - loss: 0.1440 - acc: 0.953 - ETA: 2s - loss: 0.1434 - acc: 0.953 - ETA: 2s - loss: 0.1425 - acc: 0.953 - ETA: 2s - loss: 0.1421 - acc: 0.954 - ETA: 2s - loss: 0.1428 - acc: 0.953 - ETA: 1s - loss: 0.1428 - acc: 0.954 - ETA: 1s - loss: 0.1432 - acc: 0.954 - ETA: 1s - loss: 0.1425 - acc: 0.954 - ETA: 0s - loss: 0.1432 - acc: 0.953 - ETA: 0s - loss: 0.1427 - acc: 0.954 - ETA: 0s - loss: 0.1430 - acc: 0.954 - 13s 303us/step - loss: 0.1429 - acc: 0.9541\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1350 - acc: 0.958 - ETA: 10s - loss: 0.1325 - acc: 0.95 - ETA: 11s - loss: 0.1498 - acc: 0.95 - ETA: 11s - loss: 0.1387 - acc: 0.95 - ETA: 11s - loss: 0.1438 - acc: 0.95 - ETA: 11s - loss: 0.1478 - acc: 0.95 - ETA: 11s - loss: 0.1462 - acc: 0.95 - ETA: 11s - loss: 0.1472 - acc: 0.95 - ETA: 11s - loss: 0.1454 - acc: 0.95 - ETA: 10s - loss: 0.1441 - acc: 0.95 - ETA: 10s - loss: 0.1433 - acc: 0.95 - ETA: 9s - loss: 0.1431 - acc: 0.9527 - ETA: 9s - loss: 0.1462 - acc: 0.951 - ETA: 8s - loss: 0.1454 - acc: 0.952 - ETA: 8s - loss: 0.1442 - acc: 0.952 - ETA: 7s - loss: 0.1427 - acc: 0.953 - ETA: 7s - loss: 0.1412 - acc: 0.953 - ETA: 6s - loss: 0.1407 - acc: 0.953 - ETA: 6s - loss: 0.1407 - acc: 0.953 - ETA: 6s - loss: 0.1386 - acc: 0.954 - ETA: 5s - loss: 0.1389 - acc: 0.954 - ETA: 5s - loss: 0.1383 - acc: 0.955 - ETA: 5s - loss: 0.1379 - acc: 0.955 - ETA: 4s - loss: 0.1377 - acc: 0.955 - ETA: 4s - loss: 0.1375 - acc: 0.955 - ETA: 4s - loss: 0.1377 - acc: 0.955 - ETA: 4s - loss: 0.1373 - acc: 0.955 - ETA: 4s - loss: 0.1378 - acc: 0.955 - ETA: 3s - loss: 0.1386 - acc: 0.955 - ETA: 3s - loss: 0.1389 - acc: 0.955 - ETA: 3s - loss: 0.1380 - acc: 0.955 - ETA: 2s - loss: 0.1373 - acc: 0.955 - ETA: 2s - loss: 0.1376 - acc: 0.955 - ETA: 2s - loss: 0.1383 - acc: 0.955 - ETA: 2s - loss: 0.1378 - acc: 0.955 - ETA: 1s - loss: 0.1382 - acc: 0.956 - ETA: 1s - loss: 0.1383 - acc: 0.955 - ETA: 1s - loss: 0.1385 - acc: 0.956 - ETA: 0s - loss: 0.1388 - acc: 0.955 - ETA: 0s - loss: 0.1393 - acc: 0.955 - ETA: 0s - loss: 0.1395 - acc: 0.955 - 12s 292us/step - loss: 0.1385 - acc: 0.9560\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 14s - loss: 0.1073 - acc: 0.96 - ETA: 14s - loss: 0.1385 - acc: 0.95 - ETA: 14s - loss: 0.1396 - acc: 0.95 - ETA: 12s - loss: 0.1403 - acc: 0.95 - ETA: 11s - loss: 0.1358 - acc: 0.95 - ETA: 10s - loss: 0.1330 - acc: 0.95 - ETA: 9s - loss: 0.1378 - acc: 0.9569 - ETA: 9s - loss: 0.1387 - acc: 0.955 - ETA: 8s - loss: 0.1382 - acc: 0.956 - ETA: 8s - loss: 0.1359 - acc: 0.957 - ETA: 8s - loss: 0.1358 - acc: 0.956 - ETA: 7s - loss: 0.1368 - acc: 0.956 - ETA: 7s - loss: 0.1349 - acc: 0.956 - ETA: 7s - loss: 0.1364 - acc: 0.956 - ETA: 6s - loss: 0.1346 - acc: 0.956 - ETA: 6s - loss: 0.1365 - acc: 0.956 - ETA: 6s - loss: 0.1360 - acc: 0.956 - ETA: 6s - loss: 0.1365 - acc: 0.956 - ETA: 5s - loss: 0.1352 - acc: 0.956 - ETA: 5s - loss: 0.1379 - acc: 0.956 - ETA: 5s - loss: 0.1371 - acc: 0.956 - ETA: 4s - loss: 0.1370 - acc: 0.956 - ETA: 4s - loss: 0.1362 - acc: 0.957 - ETA: 4s - loss: 0.1376 - acc: 0.957 - ETA: 4s - loss: 0.1379 - acc: 0.957 - ETA: 4s - loss: 0.1371 - acc: 0.957 - ETA: 3s - loss: 0.1366 - acc: 0.957 - ETA: 3s - loss: 0.1362 - acc: 0.958 - ETA: 3s - loss: 0.1359 - acc: 0.957 - ETA: 3s - loss: 0.1351 - acc: 0.958 - ETA: 2s - loss: 0.1352 - acc: 0.958 - ETA: 2s - loss: 0.1349 - acc: 0.958 - ETA: 2s - loss: 0.1347 - acc: 0.958 - ETA: 2s - loss: 0.1359 - acc: 0.958 - ETA: 1s - loss: 0.1357 - acc: 0.958 - ETA: 1s - loss: 0.1358 - acc: 0.958 - ETA: 1s - loss: 0.1351 - acc: 0.958 - ETA: 1s - loss: 0.1343 - acc: 0.958 - ETA: 0s - loss: 0.1335 - acc: 0.958 - ETA: 0s - loss: 0.1338 - acc: 0.958 - ETA: 0s - loss: 0.1340 - acc: 0.958 - 11s 266us/step - loss: 0.1331 - acc: 0.9585\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1275 - acc: 0.954 - ETA: 9s - loss: 0.1231 - acc: 0.959 - ETA: 8s - loss: 0.1246 - acc: 0.961 - ETA: 8s - loss: 0.1234 - acc: 0.959 - ETA: 8s - loss: 0.1233 - acc: 0.960 - ETA: 8s - loss: 0.1221 - acc: 0.959 - ETA: 8s - loss: 0.1232 - acc: 0.959 - ETA: 7s - loss: 0.1255 - acc: 0.959 - ETA: 7s - loss: 0.1279 - acc: 0.959 - ETA: 7s - loss: 0.1263 - acc: 0.960 - ETA: 7s - loss: 0.1256 - acc: 0.960 - ETA: 6s - loss: 0.1240 - acc: 0.960 - ETA: 7s - loss: 0.1251 - acc: 0.960 - ETA: 7s - loss: 0.1246 - acc: 0.960 - ETA: 6s - loss: 0.1232 - acc: 0.960 - ETA: 6s - loss: 0.1221 - acc: 0.961 - ETA: 6s - loss: 0.1218 - acc: 0.961 - ETA: 6s - loss: 0.1205 - acc: 0.961 - ETA: 6s - loss: 0.1219 - acc: 0.961 - ETA: 6s - loss: 0.1220 - acc: 0.961 - ETA: 5s - loss: 0.1217 - acc: 0.961 - ETA: 5s - loss: 0.1226 - acc: 0.961 - ETA: 5s - loss: 0.1222 - acc: 0.960 - ETA: 4s - loss: 0.1232 - acc: 0.960 - ETA: 4s - loss: 0.1224 - acc: 0.961 - ETA: 4s - loss: 0.1232 - acc: 0.960 - ETA: 4s - loss: 0.1228 - acc: 0.961 - ETA: 3s - loss: 0.1230 - acc: 0.960 - ETA: 3s - loss: 0.1232 - acc: 0.960 - ETA: 3s - loss: 0.1235 - acc: 0.960 - ETA: 2s - loss: 0.1235 - acc: 0.960 - ETA: 2s - loss: 0.1235 - acc: 0.961 - ETA: 2s - loss: 0.1236 - acc: 0.961 - ETA: 2s - loss: 0.1234 - acc: 0.961 - ETA: 1s - loss: 0.1236 - acc: 0.961 - ETA: 1s - loss: 0.1236 - acc: 0.960 - ETA: 1s - loss: 0.1234 - acc: 0.961 - ETA: 1s - loss: 0.1241 - acc: 0.960 - ETA: 0s - loss: 0.1246 - acc: 0.960 - ETA: 0s - loss: 0.1245 - acc: 0.960 - ETA: 0s - loss: 0.1245 - acc: 0.960 - 11s 257us/step - loss: 0.1246 - acc: 0.9608\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.1309 - acc: 0.95 - ETA: 14s - loss: 0.1244 - acc: 0.96 - ETA: 14s - loss: 0.1288 - acc: 0.96 - ETA: 14s - loss: 0.1261 - acc: 0.96 - ETA: 13s - loss: 0.1218 - acc: 0.96 - ETA: 13s - loss: 0.1174 - acc: 0.96 - ETA: 13s - loss: 0.1194 - acc: 0.96 - ETA: 12s - loss: 0.1204 - acc: 0.96 - ETA: 11s - loss: 0.1192 - acc: 0.96 - ETA: 10s - loss: 0.1167 - acc: 0.96 - ETA: 10s - loss: 0.1182 - acc: 0.96 - ETA: 9s - loss: 0.1176 - acc: 0.9607 - ETA: 9s - loss: 0.1179 - acc: 0.960 - ETA: 8s - loss: 0.1175 - acc: 0.960 - ETA: 8s - loss: 0.1172 - acc: 0.961 - ETA: 7s - loss: 0.1180 - acc: 0.961 - ETA: 7s - loss: 0.1167 - acc: 0.961 - ETA: 7s - loss: 0.1158 - acc: 0.961 - ETA: 6s - loss: 0.1157 - acc: 0.962 - ETA: 6s - loss: 0.1164 - acc: 0.962 - ETA: 6s - loss: 0.1168 - acc: 0.961 - ETA: 5s - loss: 0.1163 - acc: 0.962 - ETA: 5s - loss: 0.1168 - acc: 0.961 - ETA: 5s - loss: 0.1164 - acc: 0.962 - ETA: 4s - loss: 0.1171 - acc: 0.961 - ETA: 4s - loss: 0.1169 - acc: 0.961 - ETA: 4s - loss: 0.1180 - acc: 0.961 - ETA: 3s - loss: 0.1184 - acc: 0.961 - ETA: 3s - loss: 0.1185 - acc: 0.961 - ETA: 3s - loss: 0.1184 - acc: 0.961 - ETA: 3s - loss: 0.1190 - acc: 0.961 - ETA: 2s - loss: 0.1191 - acc: 0.961 - ETA: 2s - loss: 0.1196 - acc: 0.961 - ETA: 2s - loss: 0.1200 - acc: 0.961 - ETA: 2s - loss: 0.1207 - acc: 0.961 - ETA: 1s - loss: 0.1197 - acc: 0.961 - ETA: 1s - loss: 0.1201 - acc: 0.961 - ETA: 1s - loss: 0.1198 - acc: 0.961 - ETA: 0s - loss: 0.1209 - acc: 0.961 - ETA: 0s - loss: 0.1210 - acc: 0.961 - ETA: 0s - loss: 0.1213 - acc: 0.961 - 12s 287us/step - loss: 0.1215 - acc: 0.9610\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - ETA: 8s - loss: 0.1182 - acc: 0.964 - ETA: 9s - loss: 0.1156 - acc: 0.964 - ETA: 9s - loss: 0.1086 - acc: 0.967 - ETA: 8s - loss: 0.1044 - acc: 0.967 - ETA: 8s - loss: 0.1114 - acc: 0.965 - ETA: 8s - loss: 0.1101 - acc: 0.964 - ETA: 8s - loss: 0.1092 - acc: 0.964 - ETA: 7s - loss: 0.1085 - acc: 0.965 - ETA: 7s - loss: 0.1094 - acc: 0.964 - ETA: 7s - loss: 0.1098 - acc: 0.964 - ETA: 7s - loss: 0.1099 - acc: 0.964 - ETA: 7s - loss: 0.1090 - acc: 0.964 - ETA: 6s - loss: 0.1094 - acc: 0.964 - ETA: 6s - loss: 0.1098 - acc: 0.964 - ETA: 6s - loss: 0.1116 - acc: 0.964 - ETA: 6s - loss: 0.1122 - acc: 0.964 - ETA: 6s - loss: 0.1128 - acc: 0.964 - ETA: 6s - loss: 0.1129 - acc: 0.964 - ETA: 6s - loss: 0.1131 - acc: 0.964 - ETA: 5s - loss: 0.1140 - acc: 0.963 - ETA: 5s - loss: 0.1138 - acc: 0.963 - ETA: 5s - loss: 0.1141 - acc: 0.963 - ETA: 5s - loss: 0.1132 - acc: 0.963 - ETA: 4s - loss: 0.1151 - acc: 0.963 - ETA: 4s - loss: 0.1141 - acc: 0.963 - ETA: 4s - loss: 0.1145 - acc: 0.963 - ETA: 4s - loss: 0.1147 - acc: 0.963 - ETA: 3s - loss: 0.1149 - acc: 0.963 - ETA: 3s - loss: 0.1149 - acc: 0.963 - ETA: 3s - loss: 0.1149 - acc: 0.963 - ETA: 2s - loss: 0.1150 - acc: 0.963 - ETA: 2s - loss: 0.1149 - acc: 0.963 - ETA: 2s - loss: 0.1163 - acc: 0.963 - ETA: 2s - loss: 0.1167 - acc: 0.963 - ETA: 1s - loss: 0.1162 - acc: 0.963 - ETA: 1s - loss: 0.1171 - acc: 0.963 - ETA: 1s - loss: 0.1167 - acc: 0.963 - ETA: 1s - loss: 0.1165 - acc: 0.963 - ETA: 0s - loss: 0.1172 - acc: 0.963 - ETA: 0s - loss: 0.1175 - acc: 0.963 - ETA: 0s - loss: 0.1174 - acc: 0.963 - 11s 256us/step - loss: 0.1170 - acc: 0.9634\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1240 - acc: 0.953 - ETA: 9s - loss: 0.1279 - acc: 0.956 - ETA: 9s - loss: 0.1159 - acc: 0.963 - ETA: 10s - loss: 0.1213 - acc: 0.96 - ETA: 10s - loss: 0.1199 - acc: 0.96 - ETA: 11s - loss: 0.1174 - acc: 0.96 - ETA: 11s - loss: 0.1151 - acc: 0.96 - ETA: 10s - loss: 0.1139 - acc: 0.96 - ETA: 10s - loss: 0.1135 - acc: 0.96 - ETA: 10s - loss: 0.1159 - acc: 0.96 - ETA: 9s - loss: 0.1163 - acc: 0.9623 - ETA: 9s - loss: 0.1173 - acc: 0.961 - ETA: 8s - loss: 0.1176 - acc: 0.961 - ETA: 8s - loss: 0.1178 - acc: 0.962 - ETA: 8s - loss: 0.1163 - acc: 0.962 - ETA: 7s - loss: 0.1167 - acc: 0.962 - ETA: 7s - loss: 0.1157 - acc: 0.963 - ETA: 6s - loss: 0.1161 - acc: 0.962 - ETA: 6s - loss: 0.1177 - acc: 0.962 - ETA: 6s - loss: 0.1187 - acc: 0.962 - ETA: 5s - loss: 0.1185 - acc: 0.962 - ETA: 5s - loss: 0.1183 - acc: 0.962 - ETA: 5s - loss: 0.1173 - acc: 0.962 - ETA: 4s - loss: 0.1176 - acc: 0.962 - ETA: 4s - loss: 0.1167 - acc: 0.962 - ETA: 4s - loss: 0.1166 - acc: 0.962 - ETA: 4s - loss: 0.1156 - acc: 0.963 - ETA: 3s - loss: 0.1150 - acc: 0.963 - ETA: 3s - loss: 0.1148 - acc: 0.963 - ETA: 3s - loss: 0.1147 - acc: 0.963 - ETA: 2s - loss: 0.1144 - acc: 0.963 - ETA: 2s - loss: 0.1141 - acc: 0.963 - ETA: 2s - loss: 0.1146 - acc: 0.963 - ETA: 2s - loss: 0.1146 - acc: 0.963 - ETA: 1s - loss: 0.1145 - acc: 0.963 - ETA: 1s - loss: 0.1145 - acc: 0.963 - ETA: 1s - loss: 0.1140 - acc: 0.963 - ETA: 1s - loss: 0.1140 - acc: 0.963 - ETA: 0s - loss: 0.1143 - acc: 0.963 - ETA: 0s - loss: 0.1141 - acc: 0.964 - ETA: 0s - loss: 0.1140 - acc: 0.964 - 12s 280us/step - loss: 0.1136 - acc: 0.9642\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0969 - acc: 0.966 - ETA: 9s - loss: 0.1074 - acc: 0.966 - ETA: 9s - loss: 0.1108 - acc: 0.964 - ETA: 8s - loss: 0.1050 - acc: 0.968 - ETA: 8s - loss: 0.1046 - acc: 0.968 - ETA: 8s - loss: 0.1065 - acc: 0.967 - ETA: 8s - loss: 0.1050 - acc: 0.967 - ETA: 7s - loss: 0.1080 - acc: 0.966 - ETA: 7s - loss: 0.1085 - acc: 0.966 - ETA: 7s - loss: 0.1082 - acc: 0.965 - ETA: 7s - loss: 0.1068 - acc: 0.965 - ETA: 6s - loss: 0.1068 - acc: 0.966 - ETA: 6s - loss: 0.1067 - acc: 0.966 - ETA: 6s - loss: 0.1080 - acc: 0.965 - ETA: 6s - loss: 0.1084 - acc: 0.965 - ETA: 6s - loss: 0.1075 - acc: 0.965 - ETA: 5s - loss: 0.1092 - acc: 0.965 - ETA: 5s - loss: 0.1090 - acc: 0.964 - ETA: 5s - loss: 0.1084 - acc: 0.965 - ETA: 5s - loss: 0.1071 - acc: 0.965 - ETA: 5s - loss: 0.1077 - acc: 0.965 - ETA: 5s - loss: 0.1076 - acc: 0.965 - ETA: 4s - loss: 0.1080 - acc: 0.965 - ETA: 4s - loss: 0.1078 - acc: 0.965 - ETA: 4s - loss: 0.1094 - acc: 0.965 - ETA: 4s - loss: 0.1098 - acc: 0.965 - ETA: 4s - loss: 0.1090 - acc: 0.965 - ETA: 3s - loss: 0.1085 - acc: 0.965 - ETA: 3s - loss: 0.1087 - acc: 0.965 - ETA: 3s - loss: 0.1083 - acc: 0.965 - ETA: 2s - loss: 0.1082 - acc: 0.965 - ETA: 2s - loss: 0.1081 - acc: 0.965 - ETA: 2s - loss: 0.1088 - acc: 0.965 - ETA: 2s - loss: 0.1094 - acc: 0.965 - ETA: 1s - loss: 0.1099 - acc: 0.964 - ETA: 1s - loss: 0.1095 - acc: 0.964 - ETA: 1s - loss: 0.1095 - acc: 0.964 - ETA: 1s - loss: 0.1090 - acc: 0.964 - ETA: 0s - loss: 0.1092 - acc: 0.964 - ETA: 0s - loss: 0.1092 - acc: 0.965 - ETA: 0s - loss: 0.1094 - acc: 0.965 - 11s 256us/step - loss: 0.1091 - acc: 0.9652\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0922 - acc: 0.973 - ETA: 9s - loss: 0.1050 - acc: 0.972 - ETA: 8s - loss: 0.1000 - acc: 0.971 - ETA: 8s - loss: 0.0946 - acc: 0.973 - ETA: 8s - loss: 0.0923 - acc: 0.972 - ETA: 8s - loss: 0.0944 - acc: 0.971 - ETA: 8s - loss: 0.0978 - acc: 0.970 - ETA: 8s - loss: 0.0987 - acc: 0.970 - ETA: 8s - loss: 0.1013 - acc: 0.969 - ETA: 8s - loss: 0.1031 - acc: 0.968 - ETA: 8s - loss: 0.1042 - acc: 0.967 - ETA: 8s - loss: 0.1029 - acc: 0.967 - ETA: 8s - loss: 0.1023 - acc: 0.967 - ETA: 8s - loss: 0.1017 - acc: 0.967 - ETA: 7s - loss: 0.1029 - acc: 0.967 - ETA: 7s - loss: 0.1053 - acc: 0.966 - ETA: 7s - loss: 0.1051 - acc: 0.966 - ETA: 6s - loss: 0.1043 - acc: 0.966 - ETA: 6s - loss: 0.1055 - acc: 0.966 - ETA: 6s - loss: 0.1072 - acc: 0.965 - ETA: 5s - loss: 0.1079 - acc: 0.965 - ETA: 5s - loss: 0.1082 - acc: 0.965 - ETA: 5s - loss: 0.1080 - acc: 0.965 - ETA: 4s - loss: 0.1072 - acc: 0.965 - ETA: 4s - loss: 0.1082 - acc: 0.965 - ETA: 4s - loss: 0.1094 - acc: 0.964 - ETA: 3s - loss: 0.1092 - acc: 0.964 - ETA: 3s - loss: 0.1087 - acc: 0.965 - ETA: 3s - loss: 0.1094 - acc: 0.964 - ETA: 3s - loss: 0.1100 - acc: 0.964 - ETA: 2s - loss: 0.1097 - acc: 0.964 - ETA: 2s - loss: 0.1086 - acc: 0.964 - ETA: 2s - loss: 0.1083 - acc: 0.964 - ETA: 2s - loss: 0.1083 - acc: 0.965 - ETA: 1s - loss: 0.1082 - acc: 0.964 - ETA: 1s - loss: 0.1091 - acc: 0.964 - ETA: 1s - loss: 0.1097 - acc: 0.964 - ETA: 1s - loss: 0.1091 - acc: 0.964 - ETA: 0s - loss: 0.1087 - acc: 0.964 - ETA: 0s - loss: 0.1085 - acc: 0.964 - ETA: 0s - loss: 0.1089 - acc: 0.964 - 11s 273us/step - loss: 0.1087 - acc: 0.9649\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.0826 - acc: 0.97 - ETA: 13s - loss: 0.0928 - acc: 0.97 - ETA: 11s - loss: 0.0977 - acc: 0.97 - ETA: 10s - loss: 0.1040 - acc: 0.96 - ETA: 9s - loss: 0.1024 - acc: 0.9678 - ETA: 9s - loss: 0.1064 - acc: 0.967 - ETA: 9s - loss: 0.1053 - acc: 0.968 - ETA: 8s - loss: 0.1067 - acc: 0.966 - ETA: 8s - loss: 0.1087 - acc: 0.965 - ETA: 8s - loss: 0.1084 - acc: 0.965 - ETA: 7s - loss: 0.1072 - acc: 0.965 - ETA: 7s - loss: 0.1055 - acc: 0.966 - ETA: 7s - loss: 0.1062 - acc: 0.966 - ETA: 6s - loss: 0.1074 - acc: 0.965 - ETA: 6s - loss: 0.1069 - acc: 0.965 - ETA: 6s - loss: 0.1081 - acc: 0.965 - ETA: 6s - loss: 0.1079 - acc: 0.965 - ETA: 5s - loss: 0.1060 - acc: 0.966 - ETA: 5s - loss: 0.1066 - acc: 0.965 - ETA: 5s - loss: 0.1055 - acc: 0.965 - ETA: 5s - loss: 0.1049 - acc: 0.966 - ETA: 4s - loss: 0.1055 - acc: 0.965 - ETA: 4s - loss: 0.1067 - acc: 0.965 - ETA: 4s - loss: 0.1067 - acc: 0.965 - ETA: 4s - loss: 0.1061 - acc: 0.965 - ETA: 4s - loss: 0.1067 - acc: 0.965 - ETA: 3s - loss: 0.1070 - acc: 0.965 - ETA: 3s - loss: 0.1072 - acc: 0.965 - ETA: 3s - loss: 0.1075 - acc: 0.965 - ETA: 3s - loss: 0.1075 - acc: 0.965 - ETA: 2s - loss: 0.1070 - acc: 0.965 - ETA: 2s - loss: 0.1064 - acc: 0.965 - ETA: 2s - loss: 0.1061 - acc: 0.965 - ETA: 2s - loss: 0.1059 - acc: 0.965 - ETA: 1s - loss: 0.1063 - acc: 0.965 - ETA: 1s - loss: 0.1054 - acc: 0.965 - ETA: 1s - loss: 0.1054 - acc: 0.965 - ETA: 1s - loss: 0.1051 - acc: 0.965 - ETA: 0s - loss: 0.1045 - acc: 0.965 - ETA: 0s - loss: 0.1048 - acc: 0.965 - ETA: 0s - loss: 0.1048 - acc: 0.965 - 11s 262us/step - loss: 0.1048 - acc: 0.9660\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0994 - acc: 0.969 - ETA: 9s - loss: 0.1021 - acc: 0.968 - ETA: 9s - loss: 0.1006 - acc: 0.969 - ETA: 8s - loss: 0.1010 - acc: 0.967 - ETA: 8s - loss: 0.0979 - acc: 0.969 - ETA: 8s - loss: 0.0982 - acc: 0.970 - ETA: 8s - loss: 0.1021 - acc: 0.968 - ETA: 8s - loss: 0.1050 - acc: 0.967 - ETA: 7s - loss: 0.1041 - acc: 0.967 - ETA: 7s - loss: 0.1080 - acc: 0.965 - ETA: 7s - loss: 0.1081 - acc: 0.966 - ETA: 7s - loss: 0.1091 - acc: 0.965 - ETA: 7s - loss: 0.1074 - acc: 0.966 - ETA: 7s - loss: 0.1076 - acc: 0.965 - ETA: 7s - loss: 0.1067 - acc: 0.965 - ETA: 7s - loss: 0.1061 - acc: 0.966 - ETA: 7s - loss: 0.1063 - acc: 0.966 - ETA: 6s - loss: 0.1068 - acc: 0.965 - ETA: 6s - loss: 0.1060 - acc: 0.966 - ETA: 6s - loss: 0.1067 - acc: 0.966 - ETA: 5s - loss: 0.1069 - acc: 0.966 - ETA: 5s - loss: 0.1070 - acc: 0.966 - ETA: 5s - loss: 0.1063 - acc: 0.966 - ETA: 4s - loss: 0.1064 - acc: 0.966 - ETA: 4s - loss: 0.1058 - acc: 0.966 - ETA: 4s - loss: 0.1058 - acc: 0.966 - ETA: 4s - loss: 0.1066 - acc: 0.966 - ETA: 3s - loss: 0.1058 - acc: 0.966 - ETA: 3s - loss: 0.1063 - acc: 0.966 - ETA: 3s - loss: 0.1059 - acc: 0.966 - ETA: 2s - loss: 0.1062 - acc: 0.966 - ETA: 2s - loss: 0.1058 - acc: 0.966 - ETA: 2s - loss: 0.1054 - acc: 0.966 - ETA: 2s - loss: 0.1055 - acc: 0.966 - ETA: 1s - loss: 0.1060 - acc: 0.966 - ETA: 1s - loss: 0.1067 - acc: 0.966 - ETA: 1s - loss: 0.1067 - acc: 0.966 - ETA: 1s - loss: 0.1060 - acc: 0.966 - ETA: 0s - loss: 0.1057 - acc: 0.966 - ETA: 0s - loss: 0.1055 - acc: 0.966 - ETA: 0s - loss: 0.1047 - acc: 0.967 - 11s 266us/step - loss: 0.1046 - acc: 0.9669\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.1455 - acc: 0.95 - ETA: 14s - loss: 0.1230 - acc: 0.96 - ETA: 13s - loss: 0.1242 - acc: 0.96 - ETA: 13s - loss: 0.1194 - acc: 0.96 - ETA: 13s - loss: 0.1161 - acc: 0.96 - ETA: 12s - loss: 0.1196 - acc: 0.96 - ETA: 11s - loss: 0.1160 - acc: 0.96 - ETA: 10s - loss: 0.1148 - acc: 0.96 - ETA: 9s - loss: 0.1135 - acc: 0.9653 - ETA: 9s - loss: 0.1119 - acc: 0.965 - ETA: 8s - loss: 0.1102 - acc: 0.966 - ETA: 8s - loss: 0.1084 - acc: 0.966 - ETA: 8s - loss: 0.1061 - acc: 0.967 - ETA: 7s - loss: 0.1061 - acc: 0.967 - ETA: 7s - loss: 0.1071 - acc: 0.966 - ETA: 7s - loss: 0.1069 - acc: 0.966 - ETA: 6s - loss: 0.1074 - acc: 0.966 - ETA: 6s - loss: 0.1062 - acc: 0.967 - ETA: 6s - loss: 0.1057 - acc: 0.966 - ETA: 5s - loss: 0.1053 - acc: 0.967 - ETA: 5s - loss: 0.1049 - acc: 0.967 - ETA: 5s - loss: 0.1042 - acc: 0.967 - ETA: 4s - loss: 0.1027 - acc: 0.967 - ETA: 4s - loss: 0.1028 - acc: 0.967 - ETA: 4s - loss: 0.1030 - acc: 0.967 - ETA: 4s - loss: 0.1023 - acc: 0.967 - ETA: 3s - loss: 0.1016 - acc: 0.968 - ETA: 3s - loss: 0.1003 - acc: 0.968 - ETA: 3s - loss: 0.1008 - acc: 0.968 - ETA: 3s - loss: 0.1016 - acc: 0.967 - ETA: 2s - loss: 0.1014 - acc: 0.967 - ETA: 2s - loss: 0.1008 - acc: 0.967 - ETA: 2s - loss: 0.1013 - acc: 0.967 - ETA: 2s - loss: 0.1008 - acc: 0.967 - ETA: 1s - loss: 0.1007 - acc: 0.967 - ETA: 1s - loss: 0.1008 - acc: 0.967 - ETA: 1s - loss: 0.1007 - acc: 0.967 - ETA: 1s - loss: 0.1011 - acc: 0.967 - ETA: 0s - loss: 0.1004 - acc: 0.967 - ETA: 0s - loss: 0.0997 - acc: 0.967 - ETA: 0s - loss: 0.1000 - acc: 0.967 - 11s 271us/step - loss: 0.1002 - acc: 0.9679\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1123 - acc: 0.964 - ETA: 9s - loss: 0.1012 - acc: 0.967 - ETA: 8s - loss: 0.0958 - acc: 0.968 - ETA: 8s - loss: 0.0956 - acc: 0.969 - ETA: 8s - loss: 0.0922 - acc: 0.970 - ETA: 8s - loss: 0.0899 - acc: 0.970 - ETA: 8s - loss: 0.0916 - acc: 0.970 - ETA: 7s - loss: 0.0954 - acc: 0.969 - ETA: 7s - loss: 0.0950 - acc: 0.969 - ETA: 7s - loss: 0.0966 - acc: 0.969 - ETA: 7s - loss: 0.0969 - acc: 0.969 - ETA: 6s - loss: 0.0979 - acc: 0.968 - ETA: 6s - loss: 0.0984 - acc: 0.968 - ETA: 6s - loss: 0.0987 - acc: 0.968 - ETA: 6s - loss: 0.0977 - acc: 0.968 - ETA: 6s - loss: 0.0989 - acc: 0.967 - ETA: 6s - loss: 0.0984 - acc: 0.967 - ETA: 6s - loss: 0.0992 - acc: 0.967 - ETA: 5s - loss: 0.0999 - acc: 0.967 - ETA: 5s - loss: 0.0992 - acc: 0.967 - ETA: 5s - loss: 0.0980 - acc: 0.968 - ETA: 5s - loss: 0.0975 - acc: 0.968 - ETA: 5s - loss: 0.0986 - acc: 0.968 - ETA: 4s - loss: 0.0995 - acc: 0.968 - ETA: 4s - loss: 0.0990 - acc: 0.968 - ETA: 4s - loss: 0.0990 - acc: 0.968 - ETA: 4s - loss: 0.0981 - acc: 0.968 - ETA: 3s - loss: 0.0981 - acc: 0.968 - ETA: 3s - loss: 0.0971 - acc: 0.968 - ETA: 3s - loss: 0.0966 - acc: 0.968 - ETA: 3s - loss: 0.0968 - acc: 0.968 - ETA: 2s - loss: 0.0973 - acc: 0.968 - ETA: 2s - loss: 0.0977 - acc: 0.968 - ETA: 2s - loss: 0.0979 - acc: 0.968 - ETA: 2s - loss: 0.0976 - acc: 0.968 - ETA: 1s - loss: 0.0978 - acc: 0.968 - ETA: 1s - loss: 0.0978 - acc: 0.968 - ETA: 1s - loss: 0.0982 - acc: 0.968 - ETA: 0s - loss: 0.0987 - acc: 0.968 - ETA: 0s - loss: 0.0988 - acc: 0.968 - ETA: 0s - loss: 0.0986 - acc: 0.968 - 12s 286us/step - loss: 0.0986 - acc: 0.9686\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - ETA: 15s - loss: 0.1030 - acc: 0.96 - ETA: 14s - loss: 0.1092 - acc: 0.96 - ETA: 14s - loss: 0.1006 - acc: 0.96 - ETA: 13s - loss: 0.0921 - acc: 0.97 - ETA: 13s - loss: 0.0923 - acc: 0.97 - ETA: 12s - loss: 0.0939 - acc: 0.97 - ETA: 12s - loss: 0.0903 - acc: 0.97 - ETA: 11s - loss: 0.0941 - acc: 0.97 - ETA: 10s - loss: 0.0944 - acc: 0.97 - ETA: 10s - loss: 0.0950 - acc: 0.97 - ETA: 10s - loss: 0.0962 - acc: 0.97 - ETA: 9s - loss: 0.0960 - acc: 0.9696 - ETA: 9s - loss: 0.0939 - acc: 0.970 - ETA: 9s - loss: 0.0929 - acc: 0.970 - ETA: 9s - loss: 0.0926 - acc: 0.971 - ETA: 9s - loss: 0.0928 - acc: 0.970 - ETA: 8s - loss: 0.0929 - acc: 0.970 - ETA: 8s - loss: 0.0956 - acc: 0.970 - ETA: 7s - loss: 0.0957 - acc: 0.969 - ETA: 7s - loss: 0.0946 - acc: 0.970 - ETA: 6s - loss: 0.0951 - acc: 0.970 - ETA: 6s - loss: 0.0959 - acc: 0.970 - ETA: 6s - loss: 0.0960 - acc: 0.969 - ETA: 5s - loss: 0.0951 - acc: 0.970 - ETA: 5s - loss: 0.0955 - acc: 0.969 - ETA: 5s - loss: 0.0956 - acc: 0.969 - ETA: 5s - loss: 0.0960 - acc: 0.969 - ETA: 4s - loss: 0.0950 - acc: 0.969 - ETA: 4s - loss: 0.0942 - acc: 0.969 - ETA: 4s - loss: 0.0946 - acc: 0.969 - ETA: 3s - loss: 0.0941 - acc: 0.969 - ETA: 3s - loss: 0.0941 - acc: 0.969 - ETA: 2s - loss: 0.0941 - acc: 0.969 - ETA: 2s - loss: 0.0942 - acc: 0.969 - ETA: 2s - loss: 0.0945 - acc: 0.969 - ETA: 1s - loss: 0.0943 - acc: 0.969 - ETA: 1s - loss: 0.0947 - acc: 0.969 - ETA: 1s - loss: 0.0950 - acc: 0.969 - ETA: 0s - loss: 0.0946 - acc: 0.969 - ETA: 0s - loss: 0.0943 - acc: 0.969 - ETA: 0s - loss: 0.0944 - acc: 0.969 - 14s 331us/step - loss: 0.0943 - acc: 0.9698\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 8s - loss: 0.1107 - acc: 0.964 - ETA: 8s - loss: 0.0986 - acc: 0.966 - ETA: 8s - loss: 0.1033 - acc: 0.967 - ETA: 8s - loss: 0.1108 - acc: 0.965 - ETA: 7s - loss: 0.1098 - acc: 0.966 - ETA: 8s - loss: 0.1064 - acc: 0.967 - ETA: 8s - loss: 0.1089 - acc: 0.967 - ETA: 8s - loss: 0.1060 - acc: 0.968 - ETA: 8s - loss: 0.1020 - acc: 0.969 - ETA: 8s - loss: 0.1031 - acc: 0.968 - ETA: 8s - loss: 0.1040 - acc: 0.969 - ETA: 8s - loss: 0.1030 - acc: 0.969 - ETA: 8s - loss: 0.1030 - acc: 0.968 - ETA: 8s - loss: 0.1033 - acc: 0.967 - ETA: 7s - loss: 0.1022 - acc: 0.968 - ETA: 7s - loss: 0.1010 - acc: 0.968 - ETA: 6s - loss: 0.0998 - acc: 0.968 - ETA: 6s - loss: 0.0989 - acc: 0.969 - ETA: 6s - loss: 0.0993 - acc: 0.968 - ETA: 6s - loss: 0.0991 - acc: 0.969 - ETA: 5s - loss: 0.0980 - acc: 0.969 - ETA: 5s - loss: 0.0984 - acc: 0.969 - ETA: 5s - loss: 0.0985 - acc: 0.968 - ETA: 5s - loss: 0.0989 - acc: 0.968 - ETA: 4s - loss: 0.0983 - acc: 0.968 - ETA: 4s - loss: 0.0993 - acc: 0.968 - ETA: 4s - loss: 0.0989 - acc: 0.968 - ETA: 4s - loss: 0.0979 - acc: 0.968 - ETA: 3s - loss: 0.0981 - acc: 0.968 - ETA: 3s - loss: 0.0977 - acc: 0.969 - ETA: 3s - loss: 0.0972 - acc: 0.969 - ETA: 2s - loss: 0.0973 - acc: 0.969 - ETA: 2s - loss: 0.0966 - acc: 0.969 - ETA: 2s - loss: 0.0970 - acc: 0.969 - ETA: 2s - loss: 0.0972 - acc: 0.969 - ETA: 1s - loss: 0.0972 - acc: 0.968 - ETA: 1s - loss: 0.0968 - acc: 0.969 - ETA: 1s - loss: 0.0970 - acc: 0.968 - ETA: 0s - loss: 0.0972 - acc: 0.968 - ETA: 0s - loss: 0.0971 - acc: 0.968 - ETA: 0s - loss: 0.0971 - acc: 0.968 - 12s 290us/step - loss: 0.0969 - acc: 0.9690\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - ETA: 8s - loss: 0.0995 - acc: 0.970 - ETA: 8s - loss: 0.0996 - acc: 0.972 - ETA: 8s - loss: 0.0920 - acc: 0.972 - ETA: 9s - loss: 0.0925 - acc: 0.971 - ETA: 9s - loss: 0.0972 - acc: 0.969 - ETA: 10s - loss: 0.0980 - acc: 0.96 - ETA: 10s - loss: 0.0953 - acc: 0.97 - ETA: 10s - loss: 0.0963 - acc: 0.97 - ETA: 9s - loss: 0.0974 - acc: 0.9699 - ETA: 9s - loss: 0.1007 - acc: 0.969 - ETA: 9s - loss: 0.0984 - acc: 0.970 - ETA: 8s - loss: 0.0974 - acc: 0.970 - ETA: 8s - loss: 0.0967 - acc: 0.970 - ETA: 8s - loss: 0.0971 - acc: 0.969 - ETA: 7s - loss: 0.0957 - acc: 0.970 - ETA: 7s - loss: 0.0958 - acc: 0.970 - ETA: 6s - loss: 0.0955 - acc: 0.970 - ETA: 6s - loss: 0.0949 - acc: 0.970 - ETA: 6s - loss: 0.0959 - acc: 0.970 - ETA: 5s - loss: 0.0957 - acc: 0.970 - ETA: 5s - loss: 0.0963 - acc: 0.970 - ETA: 5s - loss: 0.0964 - acc: 0.970 - ETA: 5s - loss: 0.0969 - acc: 0.969 - ETA: 4s - loss: 0.0967 - acc: 0.970 - ETA: 4s - loss: 0.0966 - acc: 0.969 - ETA: 4s - loss: 0.0957 - acc: 0.970 - ETA: 4s - loss: 0.0963 - acc: 0.969 - ETA: 3s - loss: 0.0961 - acc: 0.969 - ETA: 3s - loss: 0.0964 - acc: 0.969 - ETA: 3s - loss: 0.0962 - acc: 0.969 - ETA: 3s - loss: 0.0961 - acc: 0.969 - ETA: 2s - loss: 0.0958 - acc: 0.969 - ETA: 2s - loss: 0.0960 - acc: 0.969 - ETA: 2s - loss: 0.0958 - acc: 0.969 - ETA: 1s - loss: 0.0954 - acc: 0.969 - ETA: 1s - loss: 0.0949 - acc: 0.969 - ETA: 1s - loss: 0.0948 - acc: 0.969 - ETA: 1s - loss: 0.0949 - acc: 0.969 - ETA: 0s - loss: 0.0949 - acc: 0.969 - ETA: 0s - loss: 0.0947 - acc: 0.969 - ETA: 0s - loss: 0.0945 - acc: 0.970 - 12s 291us/step - loss: 0.0937 - acc: 0.9704\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - ETA: 14s - loss: 0.0936 - acc: 0.96 - ETA: 14s - loss: 0.0864 - acc: 0.97 - ETA: 13s - loss: 0.0894 - acc: 0.97 - ETA: 12s - loss: 0.0910 - acc: 0.97 - ETA: 11s - loss: 0.0862 - acc: 0.97 - ETA: 10s - loss: 0.0905 - acc: 0.97 - ETA: 9s - loss: 0.0900 - acc: 0.9707 - ETA: 9s - loss: 0.0892 - acc: 0.971 - ETA: 8s - loss: 0.0896 - acc: 0.970 - ETA: 8s - loss: 0.0888 - acc: 0.971 - ETA: 8s - loss: 0.0896 - acc: 0.970 - ETA: 7s - loss: 0.0920 - acc: 0.970 - ETA: 7s - loss: 0.0925 - acc: 0.970 - ETA: 7s - loss: 0.0926 - acc: 0.970 - ETA: 6s - loss: 0.0919 - acc: 0.970 - ETA: 6s - loss: 0.0909 - acc: 0.970 - ETA: 6s - loss: 0.0911 - acc: 0.970 - ETA: 6s - loss: 0.0918 - acc: 0.970 - ETA: 6s - loss: 0.0922 - acc: 0.970 - ETA: 6s - loss: 0.0917 - acc: 0.970 - ETA: 5s - loss: 0.0922 - acc: 0.970 - ETA: 5s - loss: 0.0923 - acc: 0.970 - ETA: 5s - loss: 0.0919 - acc: 0.970 - ETA: 5s - loss: 0.0912 - acc: 0.970 - ETA: 4s - loss: 0.0903 - acc: 0.970 - ETA: 4s - loss: 0.0901 - acc: 0.970 - ETA: 4s - loss: 0.0895 - acc: 0.971 - ETA: 3s - loss: 0.0894 - acc: 0.971 - ETA: 3s - loss: 0.0894 - acc: 0.971 - ETA: 3s - loss: 0.0894 - acc: 0.971 - ETA: 3s - loss: 0.0890 - acc: 0.971 - ETA: 2s - loss: 0.0888 - acc: 0.971 - ETA: 2s - loss: 0.0884 - acc: 0.971 - ETA: 2s - loss: 0.0882 - acc: 0.971 - ETA: 2s - loss: 0.0891 - acc: 0.971 - ETA: 1s - loss: 0.0886 - acc: 0.971 - ETA: 1s - loss: 0.0893 - acc: 0.971 - ETA: 1s - loss: 0.0888 - acc: 0.971 - ETA: 0s - loss: 0.0884 - acc: 0.971 - ETA: 0s - loss: 0.0888 - acc: 0.971 - ETA: 0s - loss: 0.0890 - acc: 0.971 - 12s 295us/step - loss: 0.0890 - acc: 0.9713\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - ETA: 31s - loss: 0.0873 - acc: 0.97 - ETA: 29s - loss: 0.0852 - acc: 0.97 - ETA: 28s - loss: 0.0863 - acc: 0.97 - ETA: 27s - loss: 0.0853 - acc: 0.97 - ETA: 26s - loss: 0.0858 - acc: 0.97 - ETA: 25s - loss: 0.0873 - acc: 0.97 - ETA: 25s - loss: 0.0902 - acc: 0.97 - ETA: 24s - loss: 0.0912 - acc: 0.97 - ETA: 23s - loss: 0.0897 - acc: 0.97 - ETA: 22s - loss: 0.0906 - acc: 0.97 - ETA: 22s - loss: 0.0887 - acc: 0.97 - ETA: 21s - loss: 0.0890 - acc: 0.97 - ETA: 20s - loss: 0.0884 - acc: 0.97 - ETA: 20s - loss: 0.0879 - acc: 0.97 - ETA: 19s - loss: 0.0882 - acc: 0.97 - ETA: 18s - loss: 0.0883 - acc: 0.97 - ETA: 17s - loss: 0.0884 - acc: 0.97 - ETA: 17s - loss: 0.0880 - acc: 0.97 - ETA: 16s - loss: 0.0883 - acc: 0.97 - ETA: 15s - loss: 0.0890 - acc: 0.97 - ETA: 14s - loss: 0.0902 - acc: 0.97 - ETA: 13s - loss: 0.0901 - acc: 0.97 - ETA: 12s - loss: 0.0897 - acc: 0.97 - ETA: 12s - loss: 0.0887 - acc: 0.97 - ETA: 11s - loss: 0.0883 - acc: 0.97 - ETA: 10s - loss: 0.0879 - acc: 0.97 - ETA: 9s - loss: 0.0871 - acc: 0.9713 - ETA: 9s - loss: 0.0888 - acc: 0.971 - ETA: 8s - loss: 0.0891 - acc: 0.970 - ETA: 7s - loss: 0.0892 - acc: 0.970 - ETA: 6s - loss: 0.0896 - acc: 0.970 - ETA: 6s - loss: 0.0897 - acc: 0.970 - ETA: 5s - loss: 0.0895 - acc: 0.970 - ETA: 4s - loss: 0.0892 - acc: 0.970 - ETA: 4s - loss: 0.0886 - acc: 0.971 - ETA: 3s - loss: 0.0887 - acc: 0.971 - ETA: 3s - loss: 0.0886 - acc: 0.971 - ETA: 2s - loss: 0.0887 - acc: 0.971 - ETA: 1s - loss: 0.0893 - acc: 0.970 - ETA: 1s - loss: 0.0898 - acc: 0.970 - ETA: 0s - loss: 0.0897 - acc: 0.970 - 24s 577us/step - loss: 0.0896 - acc: 0.9709\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 15s - loss: 0.1056 - acc: 0.96 - ETA: 15s - loss: 0.0984 - acc: 0.96 - ETA: 15s - loss: 0.0893 - acc: 0.97 - ETA: 14s - loss: 0.0907 - acc: 0.96 - ETA: 14s - loss: 0.0860 - acc: 0.97 - ETA: 13s - loss: 0.0904 - acc: 0.97 - ETA: 13s - loss: 0.0902 - acc: 0.97 - ETA: 12s - loss: 0.0931 - acc: 0.97 - ETA: 12s - loss: 0.0933 - acc: 0.97 - ETA: 11s - loss: 0.0932 - acc: 0.97 - ETA: 10s - loss: 0.0940 - acc: 0.97 - ETA: 10s - loss: 0.0936 - acc: 0.97 - ETA: 9s - loss: 0.0937 - acc: 0.9708 - ETA: 8s - loss: 0.0939 - acc: 0.970 - ETA: 8s - loss: 0.0939 - acc: 0.970 - ETA: 8s - loss: 0.0937 - acc: 0.970 - ETA: 7s - loss: 0.0934 - acc: 0.970 - ETA: 7s - loss: 0.0937 - acc: 0.970 - ETA: 6s - loss: 0.0934 - acc: 0.970 - ETA: 6s - loss: 0.0938 - acc: 0.971 - ETA: 6s - loss: 0.0933 - acc: 0.971 - ETA: 5s - loss: 0.0924 - acc: 0.971 - ETA: 5s - loss: 0.0919 - acc: 0.971 - ETA: 5s - loss: 0.0923 - acc: 0.971 - ETA: 4s - loss: 0.0909 - acc: 0.971 - ETA: 4s - loss: 0.0904 - acc: 0.971 - ETA: 4s - loss: 0.0900 - acc: 0.972 - ETA: 4s - loss: 0.0906 - acc: 0.971 - ETA: 3s - loss: 0.0906 - acc: 0.971 - ETA: 3s - loss: 0.0906 - acc: 0.971 - ETA: 3s - loss: 0.0895 - acc: 0.971 - ETA: 2s - loss: 0.0886 - acc: 0.972 - ETA: 2s - loss: 0.0886 - acc: 0.971 - ETA: 2s - loss: 0.0887 - acc: 0.971 - ETA: 2s - loss: 0.0882 - acc: 0.972 - ETA: 1s - loss: 0.0881 - acc: 0.972 - ETA: 1s - loss: 0.0880 - acc: 0.972 - ETA: 1s - loss: 0.0884 - acc: 0.972 - ETA: 0s - loss: 0.0882 - acc: 0.972 - ETA: 0s - loss: 0.0879 - acc: 0.972 - ETA: 0s - loss: 0.0876 - acc: 0.972 - 13s 304us/step - loss: 0.0873 - acc: 0.9725\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - ETA: 13s - loss: 0.0770 - acc: 0.97 - ETA: 10s - loss: 0.0750 - acc: 0.97 - ETA: 10s - loss: 0.0764 - acc: 0.97 - ETA: 9s - loss: 0.0822 - acc: 0.9728 - ETA: 8s - loss: 0.0819 - acc: 0.973 - ETA: 8s - loss: 0.0805 - acc: 0.974 - ETA: 8s - loss: 0.0796 - acc: 0.974 - ETA: 8s - loss: 0.0776 - acc: 0.975 - ETA: 7s - loss: 0.0790 - acc: 0.974 - ETA: 7s - loss: 0.0807 - acc: 0.974 - ETA: 7s - loss: 0.0804 - acc: 0.974 - ETA: 7s - loss: 0.0793 - acc: 0.975 - ETA: 6s - loss: 0.0794 - acc: 0.974 - ETA: 6s - loss: 0.0798 - acc: 0.974 - ETA: 6s - loss: 0.0814 - acc: 0.973 - ETA: 6s - loss: 0.0817 - acc: 0.974 - ETA: 6s - loss: 0.0841 - acc: 0.973 - ETA: 6s - loss: 0.0841 - acc: 0.973 - ETA: 6s - loss: 0.0842 - acc: 0.973 - ETA: 5s - loss: 0.0851 - acc: 0.972 - ETA: 5s - loss: 0.0842 - acc: 0.972 - ETA: 5s - loss: 0.0834 - acc: 0.973 - ETA: 5s - loss: 0.0837 - acc: 0.973 - ETA: 4s - loss: 0.0834 - acc: 0.973 - ETA: 4s - loss: 0.0834 - acc: 0.973 - ETA: 4s - loss: 0.0825 - acc: 0.973 - ETA: 4s - loss: 0.0820 - acc: 0.973 - ETA: 3s - loss: 0.0823 - acc: 0.973 - ETA: 3s - loss: 0.0827 - acc: 0.973 - ETA: 3s - loss: 0.0828 - acc: 0.973 - ETA: 3s - loss: 0.0829 - acc: 0.973 - ETA: 2s - loss: 0.0835 - acc: 0.973 - ETA: 2s - loss: 0.0843 - acc: 0.972 - ETA: 2s - loss: 0.0847 - acc: 0.972 - ETA: 2s - loss: 0.0846 - acc: 0.972 - ETA: 1s - loss: 0.0849 - acc: 0.972 - ETA: 1s - loss: 0.0848 - acc: 0.972 - ETA: 1s - loss: 0.0849 - acc: 0.972 - ETA: 0s - loss: 0.0846 - acc: 0.972 - ETA: 0s - loss: 0.0851 - acc: 0.972 - ETA: 0s - loss: 0.0851 - acc: 0.972 - 12s 277us/step - loss: 0.0855 - acc: 0.9722\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=1000, epochs=30)\n",
    "model.save('cnn.h5')\n",
    "# score = model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Label\n",
      "0   0      6\n",
      "1   1      7\n",
      "2   2      2\n",
      "3   3      9\n",
      "4   4      7\n",
      "(10000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_max_y.csv')\n",
    "train_images = torch.load('paddedDigitData.pkl')\n",
    "test_images = pd.read_pickle('test_max_x')\n",
    "\n",
    "print(train_labels.head())\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for img in train_images:\n",
    "    digit_pred = [];\n",
    "    if(len(img)==0):\n",
    "        y_pred.append(-1)\n",
    "    else: \n",
    "        for digit in img:\n",
    "            digit = digit/255;\n",
    "    #         print(np.argmax(model.predict(np.reshape(digit, (-1, 28, 28, 1)))))\n",
    "    #         plt.imshow(digit, cmap='gray')\n",
    "    #         plt.show()\n",
    "\n",
    "            digit_pred.append(np.argmax(model.predict(np.reshape(digit, (-1, 28, 28, 1)))))\n",
    "        y_pred.append(np.amax(digit_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_labels['Label'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
