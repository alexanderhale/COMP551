{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic\n",
    "import os\n",
    "\n",
    "# Data management\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "# Deep learning \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Math and plots\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL = True\n",
    "if NEW_MODEL:\n",
    "    model_type = 'inception'\n",
    "else:\n",
    "    model_type = 'vanilla'\n",
    "TRAIN = True\n",
    "FOLD_USE = 1\n",
    "LOAD_FEATS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_path = './Data/reddit_test.csv'\n",
    "\n",
    "# gloabal labels\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch', 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FEATS:\n",
    "    #load\n",
    "    comment_data = pd.read_csv(train_data)\n",
    "\n",
    "    #clean\n",
    "    comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "    comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "    #load\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    #clean\n",
    "    test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "    test_data['prep'] = test_data['prep'].str.lower()\n",
    "    test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "    test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "    test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "    test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_FEATS:\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tt = TweetTokenizer()\n",
    "    def lemmatize_col(row):\n",
    "        row = tt.tokenize(row)\n",
    "        return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "\n",
    "\n",
    "    comment_data['prep'] = comment_data.prep.apply(lemmatize_col)\n",
    "    test_data['prep'] = test_data.prep.apply(lemmatize_col)\n",
    "\n",
    "    full_set = pd.concat([comment_data['prep'],test_data['prep']])\n",
    "\n",
    "    # vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,2))\n",
    "    tfidf = tfidf_vectorizer.fit_transform(full_set)\n",
    "    print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# feature space reduction\n",
    "tsvd = TruncatedSVD(n_components=1000)\n",
    "reduced_features = tsvd.fit_transform(tfidf)\n",
    "print(reduced_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # feature space reduction\n",
    "    tsvd = TruncatedSVD(n_components=2000)\n",
    "    reduced_features = tsvd.fit_transform(tfidf)\n",
    "    print(reduced_features.shape)\n",
    "except:\n",
    "    print(failed)\n",
    "    \n",
    "try:\n",
    "    # feature space reduction\n",
    "    tsvd = TruncatedSVD(n_components=3000)\n",
    "    reduced_features = tsvd.fit_transform(tfidf)\n",
    "    print(reduced_features.shape)\n",
    "except:\n",
    "    print(failed)\n",
    "    \n",
    "try:\n",
    "    # feature space reduction\n",
    "    tsvd = TruncatedSVD(n_components=4000)\n",
    "    reduced_features = tsvd.fit_transform(tfidf)\n",
    "    print(reduced_features.shape)\n",
    "except:\n",
    "    print(failed)\n",
    "    \n",
    "try:\n",
    "    # feature space reduction\n",
    "    tsvd = TruncatedSVD(n_components=5000)\n",
    "    reduced_features = tsvd.fit_transform(tfidf)\n",
    "    print(reduced_features.shape)\n",
    "except:\n",
    "    print(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./truncated.npy', reduced_features) # save\n",
    "reduced_features = np.load('./truncated.npy') # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 850)\n",
      "(30000, 850)\n"
     ]
    }
   ],
   "source": [
    "# separation\n",
    "train_stack = reduced_features[:70000]\n",
    "test_stack = reduced_features[70000:]\n",
    "print(train_stack.shape)\n",
    "print(test_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning and adapting for pipeline\n",
    "train_comments = []\n",
    "for idx, vec in enumerate(train_stack):\n",
    "    item = np.array((vec, comment_data['subreddits'][idx]))\n",
    "    train_comments.append(item)\n",
    "train_comments = np.asarray(train_comments)\n",
    "test_comments = test_stack \n",
    "\n",
    "print(train_comments.shape)\n",
    "print(test_comments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader fir train and test\n",
    "class CommentData(Dataset):\n",
    "    \n",
    "    def __init__(self, comments, labels=labels):\n",
    "        self.frames = comments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        element, label = self.frames[idx]\n",
    "        enc_label = self.encode(label)\n",
    "        return (element, enc_label)\n",
    "    \n",
    "    # one-hot encoding on element fetch\n",
    "    def encode(self, label):\n",
    "        location = self.labels.index(label)\n",
    "        return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, test):\n",
    "        self.frames = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv1d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: RECOMPUTE SIZE\n",
    "\n",
    "class WhoReddit(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WhoReddit, self).__init__()\n",
    "        \n",
    "        self.convA1 = nn.Conv1d(1, 64, 3, padding = 1)\n",
    "        self.normA1 = nn.BatchNorm1d(64)\n",
    "        self.reluA1 = nn.ReLU(True)\n",
    "        self.poolA1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convB1 = nn.Conv1d(1, 64, 5, padding = 2)\n",
    "        self.normB1 = nn.BatchNorm1d(64)\n",
    "        self.reluB1 = nn.ReLU(True)\n",
    "        self.poolB1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC1 = nn.Conv1d(1, 64, 7, padding = 3)\n",
    "        self.normC1 = nn.BatchNorm1d(64)\n",
    "        self.reluC1 = nn.ReLU(True)\n",
    "        self.poolC1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.blend1 = nn.Sequential(\n",
    "            nn.Conv1d(3*64, 96, 3, padding = 1),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 2\n",
    "        self.convA2 = nn.Conv1d(96, 128, 3, padding = 1)\n",
    "        self.normA2 = nn.BatchNorm1d(128)\n",
    "        self.reluA2 = nn.ReLU(True)\n",
    "        self.poolA2 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convB2 = nn.Conv1d(96, 128, 5, padding = 2)\n",
    "        self.normB2 = nn.BatchNorm1d(128)\n",
    "        self.reluB2 = nn.ReLU(True)\n",
    "        self.poolB2 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC2 = nn.Conv1d(96, 128, 7, padding = 3)\n",
    "        self.normC2 = nn.BatchNorm1d(128)\n",
    "        self.reluC2 = nn.ReLU(True)\n",
    "        self.poolC2 = nn.MaxPool1d(3, 3)\n",
    "    \n",
    "        self.blend2 = nn.Sequential(\n",
    "            nn.Conv1d(3*128, 196, 3, padding = 1),\n",
    "            nn.BatchNorm1d(196),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 3\n",
    "        self.convA3 = nn.Conv1d(196, 256, 3, padding = 1)\n",
    "        self.normA3 = nn.BatchNorm1d(256)\n",
    "        self.reluA3 = nn.ReLU(True)\n",
    "        self.poolA3 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        # core modules\n",
    "        self.convB3 = nn.Conv1d(196, 256, 5, padding = 2)\n",
    "        self.normB3 = nn.BatchNorm1d(256)\n",
    "        self.reluB3 = nn.ReLU(True)\n",
    "        self.poolB3 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC3 = nn.Conv1d(196, 256, 7, padding = 3)\n",
    "        self.normC3 = nn.BatchNorm1d(256)\n",
    "        self.reluC3 = nn.ReLU(True)\n",
    "        self.poolC3 = nn.MaxPool1d(3, 3)\n",
    "    \n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv1d(3*256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(23808, 20)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        A = self.dropout(self.poolA1(self.reluA1(self.normA1(self.convA1(x)))))\n",
    "        B = self.dropout(self.poolB1(self.reluB1(self.normB1(self.convB1(x)))))\n",
    "        C = self.dropout(self.poolC1(self.reluC1(self.normC1(self.convC1(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend1(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA2(self.reluA2(self.normA2(self.convA2(x)))))\n",
    "        B = self.dropout(self.poolB2(self.reluB2(self.normB2(self.convB2(x)))))\n",
    "        C = self.dropout(self.poolC2(self.reluC2(self.normC2(self.convC2(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend2(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA3(self.reluA3(self.normA3(self.convA3(x)))))\n",
    "        B = self.dropout(self.poolB3(self.reluB3(self.normB3(self.convB3(x)))))\n",
    "        C = self.dropout(self.poolC3(self.reluC3(self.normC3(self.convC3(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.merge(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhoRedditTwo(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WhoRedditTwo, self).__init__()\n",
    "        \n",
    "        self.convA1 = nn.Conv1d(1, 32, 3, padding = 1)\n",
    "        self.normA1 = nn.BatchNorm1d(32)\n",
    "        self.reluA1 = nn.ReLU(True)\n",
    "        self.poolA1 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        self.convB1 = nn.Conv1d(1, 32, 5, padding = 2)\n",
    "        self.normB1 = nn.BatchNorm1d(32)\n",
    "        self.reluB1 = nn.ReLU(True)\n",
    "        self.poolB1 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        self.convC1 = nn.Conv1d(1, 32, 7, padding = 3)\n",
    "        self.normC1 = nn.BatchNorm1d(32)\n",
    "        self.reluC1 = nn.ReLU(True)\n",
    "        self.poolC1 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        self.blend1 = nn.Sequential(\n",
    "            nn.Conv1d(3*32, 64, 3, padding = 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(5,5),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 2\n",
    "        self.convA2 = nn.Conv1d(64, 64, 3, padding = 1)\n",
    "        self.normA2 = nn.BatchNorm1d(64)\n",
    "        self.reluA2 = nn.ReLU(True)\n",
    "        self.poolA2 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        self.convB2 = nn.Conv1d(64, 64, 5, padding = 2)\n",
    "        self.normB2 = nn.BatchNorm1d(64)\n",
    "        self.reluB2 = nn.ReLU(True)\n",
    "        self.poolB2 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        self.convC2 = nn.Conv1d(64, 64, 7, padding = 3)\n",
    "        self.normC2 = nn.BatchNorm1d(64)\n",
    "        self.reluC2 = nn.ReLU(True)\n",
    "        self.poolC2 = nn.MaxPool1d(5, 5)\n",
    "    \n",
    "        self.blend2 = nn.Sequential(\n",
    "            nn.Conv1d(3*64, 96, 3, padding = 1),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(8,8)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 3\n",
    "        self.convA3 = nn.Conv1d(96, 96, 3, padding = 1)\n",
    "        self.normA3 = nn.BatchNorm1d(96)\n",
    "        self.reluA3 = nn.ReLU(True)\n",
    "        self.poolA3 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        # core modules\n",
    "        self.convB3 = nn.Conv1d(96, 96, 5, padding = 2)\n",
    "        self.normB3 = nn.BatchNorm1d(96)\n",
    "        self.reluB3 = nn.ReLU(True)\n",
    "        self.poolB3 = nn.MaxPool1d(5, 5)\n",
    "        \n",
    "        self.convC3 = nn.Conv1d(96, 96, 7, padding = 3)\n",
    "        self.normC3 = nn.BatchNorm1d(96)\n",
    "        self.reluC3 = nn.ReLU(True)\n",
    "        self.poolC3 = nn.MaxPool1d(5, 5)\n",
    "    \n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv1d(3*96, 128, 3, padding = 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(79*128, 4096),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096,20)\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        A = self.dropout(self.poolA1(self.reluA1(self.normA1(self.convA1(x)))))\n",
    "        B = self.dropout(self.poolB1(self.reluB1(self.normB1(self.convB1(x)))))\n",
    "        C = self.dropout(self.poolC1(self.reluC1(self.normC1(self.convC1(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend1(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA2(self.reluA2(self.normA2(self.convA2(x)))))\n",
    "        B = self.dropout(self.poolB2(self.reluB2(self.normB2(self.convB2(x)))))\n",
    "        C = self.dropout(self.poolC2(self.reluC2(self.normC2(self.convC2(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend2(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA3(self.reluA3(self.normA3(self.convA3(x)))))\n",
    "        B = self.dropout(self.poolB3(self.reluB3(self.normB3(self.convB3(x)))))\n",
    "        C = self.dropout(self.poolC3(self.reluC3(self.normC3(self.convC3(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.merge(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: RECOMPUTE SIZES AND DOWNSAMPLING FOR NGRAM 1,2\n",
    "class VanillaWhoReddit(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VanillaWhoReddit, self).__init__()\n",
    "        \n",
    "        # mini inception net block 1\n",
    "        self.convA1 = nn.Conv1d(1, 64, 3, padding = 1)\n",
    "        self.normA1 = nn.BatchNorm1d(64)\n",
    "        self.reluA1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convA2 = nn.Conv1d(64, 64, 3, padding = 1)\n",
    "        self.normA2 = nn.BatchNorm1d(64)\n",
    "        self.reluA2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolA = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convB1 = nn.Conv1d(64, 128, 3, padding = 1)\n",
    "        self.normB1 = nn.BatchNorm1d(128)\n",
    "        self.reluB1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convB2 = nn.Conv1d(128, 128, 3, padding = 1)\n",
    "        self.normB2 = nn.BatchNorm1d(128)\n",
    "        self.reluB2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolB = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convC1 = nn.Conv1d(128, 256, 3, padding = 1)\n",
    "        self.normC1 = nn.BatchNorm1d(256)\n",
    "        self.reluC1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convC2 = nn.Conv1d(256, 256, 3, padding = 1)\n",
    "        self.normC2 = nn.BatchNorm1d(256)\n",
    "        self.reluC2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolC = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convD1 = nn.Conv1d(256, 512, 3, padding = 1)\n",
    "        self.normD1 = nn.BatchNorm1d(512)\n",
    "        self.reluD1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convD2 = nn.Conv1d(512, 512, 3, padding = 1)\n",
    "        self.normD2 = nn.BatchNorm1d(512)\n",
    "        self.reluD2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolD = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convE1 = nn.Conv1d(512, 256, 3, padding = 1)\n",
    "        self.normE1 = nn.BatchNorm1d(256)\n",
    "        self.reluE1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convE2 = nn.Conv1d(256, 256, 3, padding = 1)\n",
    "        self.normE2 = nn.BatchNorm1d(256)\n",
    "        self.reluE2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolE = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, 3, padding = 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(8448, 2048)\n",
    "        self.fc2 =  nn.Linear(2048, 20)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        A = self.dropout(self.reluA1(self.normA1(self.convA1(x))))\n",
    "        A = self.dropout(self.reluA2(self.normA2(self.convA2(A))))\n",
    "        x = self.poolA(A)\n",
    "        \n",
    "        B = self.dropout(self.reluB1(self.normB1(self.convB1(x))))\n",
    "        B = self.dropout(self.reluB2(self.normB2(self.convB2(B))))\n",
    "        x = self.poolB(B)\n",
    "        \n",
    "        C = self.dropout(self.reluC1(self.normC1(self.convC1(x))))\n",
    "        C = self.dropout(self.reluC2(self.normC2(self.convC2(C))))\n",
    "        x = self.poolC(C)\n",
    "        \n",
    "        D = self.dropout(self.reluD1(self.normD1(self.convD1(x))))\n",
    "        D = self.dropout(self.reluD2(self.normD2(self.convD2(D))))\n",
    "        x = self.poolD(D)\n",
    "        \n",
    "        E = self.dropout(self.reluE1(self.normE1(self.convE1(x))))\n",
    "        E = self.dropout(self.reluE2(self.normE2(self.convE2(E))))\n",
    "        x = self.poolE(E)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NEW_MODEL:\n",
    "    net = WhoRedditTwo().to(device)\n",
    "else:\n",
    "    net = VanillaWhoReddit().to(device)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss().type(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split init\n",
    "commentFolds = kFold(train_comments) \n",
    "commentFolds.generateSplits()\n",
    "splits = commentFolds.splits\n",
    "x,y,z = splits[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, split in enumerate(splits): # split\n",
    "    \n",
    "    train, val, test = split\n",
    "    \n",
    "    # Dataset obj\n",
    "    train_set = CommentData(train)\n",
    "    val_set = CommentData(val)\n",
    "    test_set = CommentData(test)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, num_workers=8, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=8)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=8)\n",
    "    \n",
    "    # train cycle here\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(train_loader):\n",
    "            \n",
    "            # tensor to device\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = net(comment)\n",
    "            error = loss(output, target)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += error.item()\n",
    "            if i % 500 == 499:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Training Acc:\",train_acc)\n",
    "\n",
    "        net.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(val_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        valid_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Validation Acc:\",valid_acc)\n",
    "        \n",
    "        net.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(test_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        test_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Test Acc:\",test_acc)\n",
    "        \n",
    "        torch.save(net.state_dict(), './model'+ str(epoch) + model_type)\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # terminate cycle\n",
    "    if idx-1 >= FOLD_USE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running through ex:\n",
    "tester = TestData(test_comments)\n",
    "loader = torch.utils.data.DataLoader(tester, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "\n",
    "net.eval()\n",
    "for i, comment in enumerate(loader):\n",
    "    \n",
    "    comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    output = net(comment)\n",
    "    pred = F.softmax(output, dim=1)\n",
    "    index = pred.argmax(dim=1)\n",
    "    \n",
    "    test_labels.append(labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = pd.read_csv(test_data)\n",
    "test_output['subreddits'] = test_labels\n",
    "test_output.to_csv('results.csv')\n",
    "\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
