{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Assumptions\n",
    "    - All preprocessing is completed and has been stored in a .csv file\n",
    "    - There exists no \"bad\" data such that an associated label is out of the labeled set\n",
    "    - The vectors placed into the training set are of the same form as the test set\n",
    "        - There exists no errors due to unseen words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic\n",
    "import os\n",
    "\n",
    "# Data management\n",
    "import csv\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "# Deep learning \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Math and plots\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL = True\n",
    "TRAIN = True\n",
    "SPLIT_USE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "    - Assumes data is preprossessed such that no transformation must be done on load\n",
    "    - Does not load element as a tensor\n",
    "    - Return a descriptor vector and an encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_data = './Data/name_of_test.csv'\n",
    "\n",
    "# gloabal labels\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch', 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#clean\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "comment_data['prep'] = comment_data.prep.apply(lemmatize_col)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,1))\n",
    "tfidf = tfidf_vectorizer.fit_transform(comment_data.prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning\n",
    "comments = []\n",
    "vector_idf = tfidf.toarray()\n",
    "for idx, vec in enumerate(vector_idf):\n",
    "    item = np.array((vec, comment_data['subreddits'][idx]))\n",
    "    comments.append(item)\n",
    "comments = np.asarray(comments)\n",
    "print(comments[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader fir train and test\n",
    "class CommentData(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels=labels):\n",
    "        self.frames = comments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        element, label = self.frames[idx]\n",
    "        enc_label = self.encode(label)\n",
    "        return (element, enc_label)\n",
    "    \n",
    "    # one-hot encoding on element fetch\n",
    "    def encode(self, label):\n",
    "        location = self.labels.index(label)\n",
    "        return location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhoReddit(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WhoReddit, self).__init__()\n",
    "        \n",
    "        # mini inception net block 1\n",
    "        self.convA1 = nn.Conv1d(1, 64, 3, padding = 1)\n",
    "        self.normA1 = nn.BatchNorm1d(64)\n",
    "        self.reluA1 = nn.ReLU(True)\n",
    "        self.poolA1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convB1 = nn.Conv1d(1, 64, 5, padding = 2)\n",
    "        self.normB1 = nn.BatchNorm1d(64)\n",
    "        self.reluB1 = nn.ReLU(True)\n",
    "        self.poolB1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC1 = nn.Conv1d(1, 64, 7, padding = 3)\n",
    "        self.normC1 = nn.BatchNorm1d(64)\n",
    "        self.reluC1 = nn.ReLU(True)\n",
    "        self.poolC1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.blend1 = nn.Sequential(\n",
    "            nn.Conv1d(3*64, 96, 3, padding = 1),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 2\n",
    "        self.convA2 = nn.Conv1d(96, 128, 3, padding = 1)\n",
    "        self.normA2 = nn.BatchNorm1d(128)\n",
    "        self.reluA2 = nn.ReLU(True)\n",
    "        self.poolA2 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convB2 = nn.Conv1d(96, 128, 5, padding = 2)\n",
    "        self.normB2 = nn.BatchNorm1d(128)\n",
    "        self.reluB2 = nn.ReLU(True)\n",
    "        self.poolB2 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC2 = nn.Conv1d(96, 128, 7, padding = 3)\n",
    "        self.normC2 = nn.BatchNorm1d(128)\n",
    "        self.reluC2 = nn.ReLU(True)\n",
    "        self.poolC2 = nn.MaxPool1d(3, 3)\n",
    "    \n",
    "        self.blend2 = nn.Sequential(\n",
    "            nn.Conv1d(3*128, 196, 3, padding = 1),\n",
    "            nn.BatchNorm1d(196),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 3\n",
    "        self.convA3 = nn.Conv1d(196, 256, 3, padding = 1)\n",
    "        self.normA3 = nn.BatchNorm1d(256)\n",
    "        self.reluA3 = nn.ReLU(True)\n",
    "        self.poolA3 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        # core modules\n",
    "        self.convB3 = nn.Conv1d(196, 256, 5, padding = 2)\n",
    "        self.normB3 = nn.BatchNorm1d(256)\n",
    "        self.reluB3 = nn.ReLU(True)\n",
    "        self.poolB3 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC3 = nn.Conv1d(196, 256, 7, padding = 3)\n",
    "        self.normC3 = nn.BatchNorm1d(256)\n",
    "        self.reluC3 = nn.ReLU(True)\n",
    "        self.poolC3 = nn.MaxPool1d(3, 3)\n",
    "    \n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv1d(3*256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(23808, 20)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        A = self.dropout(self.poolA1(self.reluA1(self.normA1(self.convA1(x)))))\n",
    "        B = self.dropout(self.poolB1(self.reluB1(self.normB1(self.convB1(x)))))\n",
    "        C = self.dropout(self.poolC1(self.reluC1(self.normC1(self.convC1(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend1(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA2(self.reluA2(self.normA2(self.convA2(x)))))\n",
    "        B = self.dropout(self.poolB2(self.reluB2(self.normB2(self.convB2(x)))))\n",
    "        C = self.dropout(self.poolC2(self.reluC2(self.normC2(self.convC2(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend2(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA3(self.reluA3(self.normA3(self.convA3(x)))))\n",
    "        B = self.dropout(self.poolB3(self.reluB3(self.normB3(self.convB3(x)))))\n",
    "        C = self.dropout(self.poolC3(self.reluC3(self.normC3(self.convC3(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.merge(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NEW_MODEL:\n",
    "    net = WhoReddit().to(device)\n",
    "else:\n",
    "    print('save a model') #todo: load network\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss().type(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split init\n",
    "commentFolds = kFold(comments) \n",
    "commentFolds.generateSplits()\n",
    "splits = commentFolds.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, split in enumerate(splits): # split\n",
    "    \n",
    "    train, val, test = split\n",
    "    \n",
    "    # Dataset obj\n",
    "    train_set = CommentData(train)\n",
    "    val_set = CommentData(val)\n",
    "    test_set = CommentData(test)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, num_workers=8)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, num_workers=8)\n",
    "    \n",
    "    # train cycle here\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(train_loader):\n",
    "            \n",
    "            # tensor to device\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = net(comment)\n",
    "            error = loss(output, target)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += error.item()\n",
    "            if i % 500 == 499:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Training Acc:\",train_acc)\n",
    "\n",
    "        net.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(val_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        valid_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Validation Acc:\",valid_acc)\n",
    "        \n",
    "        for i, (comment, target) in enumerate(test_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        test_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Test Acc:\",test_acc)\n",
    "        \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # terminate cycle\n",
    "    if idx-1 >= FOLD_USE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
