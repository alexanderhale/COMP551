{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic\n",
    "import os\n",
    "\n",
    "# Data management\n",
    "import csv\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymfe.mfe import MFE\n",
    "\n",
    "# Deep learning \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Math and plots\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL = True\n",
    "TRAIN = True\n",
    "SPLIT_USE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "    - Assumes data is preprossessed such that no transformation must be done on load\n",
    "    - Does not load element as a tensor\n",
    "    - Return a descriptor vector and an encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_data = './Data/reddit_test.csv'\n",
    "\n",
    "# gloabal labels\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch', 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#clean\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "def lemmatize_col(row):\n",
    "    row = tt.tokenize(row)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in row])\n",
    "comment_data['prep'] = comment_data.prep.apply(lemmatize_col)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, stop_words=\"english\", ngram_range=(1,1))\n",
    "tfidf = tfidf_vectorizer.fit_transform(comment_data.prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68455,)\n"
     ]
    }
   ],
   "source": [
    "#binning\n",
    "comments = []\n",
    "vector_idf = tfidf.toarray()\n",
    "for idx, vec in enumerate(vector_idf):\n",
    "    item = np.array((vec, comment_data['subreddits'][idx]))\n",
    "    comments.append(item)\n",
    "comments = np.asarray(comments)\n",
    "print(comments[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader fir train and test\n",
    "class CommentData(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels=labels):\n",
    "        self.frames = comments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        element, label = self.frames[idx]\n",
    "        enc_label = self.encode(label)\n",
    "        return (element, enc_label)\n",
    "    \n",
    "    # one-hot encoding on element fetch\n",
    "    def encode(self, label):\n",
    "        location = self.labels.index(label)\n",
    "        return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, test):\n",
    "        self.frames = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhoReddit(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WhoReddit, self).__init__()\n",
    "        \n",
    "        # mini inception net block 1\n",
    "        self.convA1 = nn.Conv1d(1, 64, 3, padding = 1)\n",
    "        self.normA1 = nn.BatchNorm1d(64)\n",
    "        self.reluA1 = nn.ReLU(True)\n",
    "        self.poolA1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convB1 = nn.Conv1d(1, 64, 5, padding = 2)\n",
    "        self.normB1 = nn.BatchNorm1d(64)\n",
    "        self.reluB1 = nn.ReLU(True)\n",
    "        self.poolB1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC1 = nn.Conv1d(1, 64, 7, padding = 3)\n",
    "        self.normC1 = nn.BatchNorm1d(64)\n",
    "        self.reluC1 = nn.ReLU(True)\n",
    "        self.poolC1 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.blend1 = nn.Sequential(\n",
    "            nn.Conv1d(3*64, 96, 3, padding = 1),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 2\n",
    "        self.convA2 = nn.Conv1d(96, 128, 3, padding = 1)\n",
    "        self.normA2 = nn.BatchNorm1d(128)\n",
    "        self.reluA2 = nn.ReLU(True)\n",
    "        self.poolA2 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convB2 = nn.Conv1d(96, 128, 5, padding = 2)\n",
    "        self.normB2 = nn.BatchNorm1d(128)\n",
    "        self.reluB2 = nn.ReLU(True)\n",
    "        self.poolB2 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC2 = nn.Conv1d(96, 128, 7, padding = 3)\n",
    "        self.normC2 = nn.BatchNorm1d(128)\n",
    "        self.reluC2 = nn.ReLU(True)\n",
    "        self.poolC2 = nn.MaxPool1d(3, 3)\n",
    "    \n",
    "        self.blend2 = nn.Sequential(\n",
    "            nn.Conv1d(3*128, 196, 3, padding = 1),\n",
    "            nn.BatchNorm1d(196),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3)\n",
    "        )\n",
    "        \n",
    "        # mini inception net block 3\n",
    "        self.convA3 = nn.Conv1d(196, 256, 3, padding = 1)\n",
    "        self.normA3 = nn.BatchNorm1d(256)\n",
    "        self.reluA3 = nn.ReLU(True)\n",
    "        self.poolA3 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        # core modules\n",
    "        self.convB3 = nn.Conv1d(196, 256, 5, padding = 2)\n",
    "        self.normB3 = nn.BatchNorm1d(256)\n",
    "        self.reluB3 = nn.ReLU(True)\n",
    "        self.poolB3 = nn.MaxPool1d(3, 3)\n",
    "        \n",
    "        self.convC3 = nn.Conv1d(196, 256, 7, padding = 3)\n",
    "        self.normC3 = nn.BatchNorm1d(256)\n",
    "        self.reluC3 = nn.ReLU(True)\n",
    "        self.poolC3 = nn.MaxPool1d(3, 3)\n",
    "    \n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv1d(3*256, 256, 3, padding = 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3,3),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(23808, 20)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        A = self.dropout(self.poolA1(self.reluA1(self.normA1(self.convA1(x)))))\n",
    "        B = self.dropout(self.poolB1(self.reluB1(self.normB1(self.convB1(x)))))\n",
    "        C = self.dropout(self.poolC1(self.reluC1(self.normC1(self.convC1(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend1(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA2(self.reluA2(self.normA2(self.convA2(x)))))\n",
    "        B = self.dropout(self.poolB2(self.reluB2(self.normB2(self.convB2(x)))))\n",
    "        C = self.dropout(self.poolC2(self.reluC2(self.normC2(self.convC2(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.blend2(x)\n",
    "        \n",
    "        A = self.dropout(self.poolA3(self.reluA3(self.normA3(self.convA3(x)))))\n",
    "        B = self.dropout(self.poolB3(self.reluB3(self.normB3(self.convB3(x)))))\n",
    "        C = self.dropout(self.poolC3(self.reluC3(self.normC3(self.convC3(x)))))\n",
    "        x = torch.cat((A,B,C), dim=1)\n",
    "        x = self.merge(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaWhoReddit(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VanillaWhoReddit, self).__init__()\n",
    "        \n",
    "        # mini inception net block 1\n",
    "        self.convA1 = nn.Conv1d(1, 64, 3, padding = 1)\n",
    "        self.normA1 = nn.BatchNorm1d(256)\n",
    "        self.reluA1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convA2 = nn.Conv1d(64, 64, 3, padding = 1)\n",
    "        self.normA2 = nn.BatchNorm1d(64)\n",
    "        self.reluA2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolA = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convB1 = nn.Conv1d(64, 128, 3, padding = 1)\n",
    "        self.normB1 = nn.BatchNorm1d(128)\n",
    "        self.reluB1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convB2 = nn.Conv1d(128, 128, 3, padding = 1)\n",
    "        self.normB2 = nn.BatchNorm1d(128)\n",
    "        self.reluB2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolB = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convC1 = nn.Conv1d(128, 256, 3, padding = 1)\n",
    "        self.normC1 = nn.BatchNorm1d(256)\n",
    "        self.reluC1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convC2 = nn.Conv1d(256, 256, 3, padding = 1)\n",
    "        self.normC2 = nn.BatchNorm1d(256)\n",
    "        self.reluC2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolC = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convD1 = nn.Conv1d(256, 512, 3, padding = 1)\n",
    "        self.normD1 = nn.BatchNorm1d(512)\n",
    "        self.reluD1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convD2 = nn.Conv1d(512, 512, 3, padding = 1)\n",
    "        self.normD2 = nn.BatchNorm1d(512)\n",
    "        self.reluD2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolD = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.convE1 = nn.Conv1d(512, 512, 3, padding = 1)\n",
    "        self.normE1 = nn.BatchNorm1d(512)\n",
    "        self.reluE1 = nn.ReLU(True)\n",
    "        \n",
    "        self.convE2 = nn.Conv1d(512, 512, 3, padding = 1)\n",
    "        self.normE2 = nn.BatchNorm1d(512)\n",
    "        self.reluE2 = nn.ReLU(True)\n",
    "        \n",
    "        self.poolE = nn.MaxPool1d(4, 4)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, 3, padding = 1)\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(33792, 4096)\n",
    "        self.fc2 =  nn.Linear(4096, 20)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        A = self.dropout(self.poolA1(self.reluA1(self.normA1(self.convA1(x)))))\n",
    "        A = self.dropout(self.poolA2(self.reluA2(self.normA2(self.convA2(A)))))\n",
    "        x = self.poolA(A)\n",
    "        \n",
    "        B = self.dropout(self.poolB1(self.reluB1(self.normB1(self.convB1(x)))))\n",
    "        B = self.dropout(self.poolA2(self.reluA2(self.normB2(self.convB2(B)))))\n",
    "        x = self.poolB(B)\n",
    "        \n",
    "        C = self.dropout(self.poolC1(self.reluC1(self.normC1(self.convC1(x)))))\n",
    "        C = self.dropout(self.poolC2(self.reluC2(self.normC2(self.convC2(C)))))\n",
    "        x = self.poolC(C)\n",
    "        \n",
    "        D = self.dropout(self.poolD1(self.reluD1(self.normD1(self.convD1(x)))))\n",
    "        D = self.dropout(self.poolD2(self.reluD2(self.normD2(self.convD2(D)))))\n",
    "        x = self.poolD(D)\n",
    "        \n",
    "        E = self.dropout(self.poolE1(self.reluE1(self.normE1(self.convE1(x)))))\n",
    "        E = self.dropout(self.poolE2(self.reluE2(self.normE2(self.convE2(E)))))\n",
    "        x = self.poolE(E)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc2(fc1(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhoReddit(\n",
      "  (convA1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (normA1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluA1): ReLU(inplace=True)\n",
      "  (poolA1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convB1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (normB1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluB1): ReLU(inplace=True)\n",
      "  (poolB1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convC1): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (normC1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluC1): ReLU(inplace=True)\n",
      "  (poolC1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (blend1): Sequential(\n",
      "    (0): Conv1d(192, 96, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (convA2): Conv1d(96, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (normA2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluA2): ReLU(inplace=True)\n",
      "  (poolA2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convB2): Conv1d(96, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (normB2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluB2): ReLU(inplace=True)\n",
      "  (poolB2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convC2): Conv1d(96, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (normC2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluC2): ReLU(inplace=True)\n",
      "  (poolC2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (blend2): Sequential(\n",
      "    (0): Conv1d(384, 196, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (convA3): Conv1d(196, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (normA3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluA3): ReLU(inplace=True)\n",
      "  (poolA3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convB3): Conv1d(196, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (normB3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluB3): ReLU(inplace=True)\n",
      "  (poolB3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convC3): Conv1d(196, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (normC3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reluC3): ReLU(inplace=True)\n",
      "  (poolC3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (merge): Sequential(\n",
      "    (0): Conv1d(768, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (linear): Linear(in_features=23808, out_features=20, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if NEW_MODEL:\n",
    "    net = WhoReddit().to(device)\n",
    "else:\n",
    "    print('save a model') #todo: load network\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss().type(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Models/name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 2) (7000, 2) (7000, 2)\n"
     ]
    }
   ],
   "source": [
    "# split init\n",
    "commentFolds = kFold(comments) \n",
    "commentFolds.generateSplits()\n",
    "splits = commentFolds.splits\n",
    "x,y,z = splits[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 3.24261\n",
      "[1,  1000] loss: 2.69859\n",
      "[1,  1500] loss: 2.53258\n",
      "[1,  2000] loss: 2.44120\n",
      "[1,  2500] loss: 2.35050\n",
      "Epoch: 1 Training Acc: 0.239\n",
      "Epoch: 1 Validation Acc: 0.3959714285714286\n",
      "Epoch: 1 Test Acc: 0.3959714285714286\n",
      "[2,   500] loss: 2.24223\n",
      "[2,  1000] loss: 2.15288\n",
      "[2,  1500] loss: 2.12084\n",
      "[2,  2000] loss: 2.08046\n",
      "[2,  2500] loss: 2.00634\n",
      "Epoch: 2 Training Acc: 0.38612857142857143\n",
      "Epoch: 2 Validation Acc: 0.48937142857142857\n",
      "Epoch: 2 Test Acc: 0.48937142857142857\n",
      "[3,   500] loss: 1.99049\n",
      "[3,  1000] loss: 1.88816\n",
      "[3,  1500] loss: 1.84728\n",
      "[3,  2000] loss: 1.88008\n",
      "[3,  2500] loss: 1.81016\n",
      "Epoch: 3 Training Acc: 0.4513\n"
     ]
    }
   ],
   "source": [
    "for idx, split in enumerate(splits): # split\n",
    "    \n",
    "    train, val, test = split\n",
    "    \n",
    "    # Dataset obj\n",
    "    train_set = CommentData(train)\n",
    "    val_set = CommentData(val)\n",
    "    test_set = CommentData(test)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=24, num_workers=8, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=8, num_workers=8)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=8, num_workers=8)\n",
    "    \n",
    "    # train cycle here\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(train_loader):\n",
    "            \n",
    "            # tensor to device\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = net(comment)\n",
    "            error = loss(output, target)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += error.item()\n",
    "            if i % 500 == 499:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Training Acc:\",train_acc)\n",
    "\n",
    "        net.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        \n",
    "        for i, (comment, target) in enumerate(val_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        valid_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Validation Acc:\",valid_acc)\n",
    "        \n",
    "        for i, (comment, target) in enumerate(test_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        test_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Test Acc:\",test_acc)\n",
    "        \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # terminate cycle\n",
    "    if idx-1 >= FOLD_USE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "test_data = pd.read_csv(test_data)\n",
    "\n",
    "#clean\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "# lemmatization\n",
    "test_data['prep'] = test_data.prep.apply(lemmatize_col)\n",
    "\n",
    "# vectorize\n",
    "test_tfidf = tfidf_vectorizer.fit_transform(test_data.prep)\n",
    "\n",
    "#binning\n",
    "tests = np.asarray(test_tfidf.toarray())\n",
    "print(tests.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running through ex:\n",
    "tester = TestData(tests)\n",
    "loader = torch.utils.data.DataLoader(tester, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "\n",
    "net.eval()\n",
    "for i, comment in enumerate(loader):\n",
    "    \n",
    "    comment = comment.to(device=device, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    output = net(comment)\n",
    "    pred = F.softmax(output, dim=1)\n",
    "    index = pred.argmax(dim=1)\n",
    "    \n",
    "    test_labels.append(labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
