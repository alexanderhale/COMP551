{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(file_path, delimiter, skiprows=0):\n",
    "    \"\"\"loads a data file and returns a numpy array\"\"\"\n",
    "    file = open(file_path, \"rb\")\n",
    "    arr = np.loadtxt(file, delimiter=delimiter, skiprows=skiprows)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"breast-cancer-wisconsin.csv\", \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    row[-1] = 0 if row[-1] == 2 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[:,1:-1]\n",
    "Y=np.ravel((data[:,-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = true labels\n",
    "# y_hat = training labels\n",
    "# return: accuracy of training labels (in percentage)\n",
    "# Ensure that y and y_hat contain the labels for the same training examples.\n",
    "def evaluate_acc(y, y_hat):\n",
    "    score = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == y_hat[i]:\n",
    "            score += 1\n",
    "    return (score / y.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = class labels of training examples\n",
    "# x = feature data of training examples\n",
    "# 2 < k = number of folds to use in validation\n",
    "# return: average of prediction error over the k rounds of execution\n",
    "def k_fold(x, y, k, model):\n",
    "    if k < 1:\n",
    "        return \"Must have at least 1 fold.\"\n",
    "    elif k > (x.shape[0]//2):\n",
    "        return \"Too many folds.\"\n",
    "    elif k == 1:\n",
    "        print(\"1 fold selected - model will be trained and validated on same data set\")\n",
    "        model.fit(x, y)\n",
    "        return evaluate_acc(y, model.predict(x))\n",
    "    else:\n",
    "        rows_per_fold = (x.shape[0] + 1)//k       # a few rows at the end of the training data will be unused\n",
    "        accuracy = 0\n",
    "\n",
    "        for exec_round in range(k):\n",
    "            # determine held-out range\n",
    "            lower_row = exec_round * rows_per_fold\n",
    "            upper_row = ((exec_round + 1) * rows_per_fold) - 1\n",
    "            \n",
    "            # create validation set\n",
    "            x_val = np.copy(x)[lower_row:upper_row]\n",
    "            y_val = np.copy(y)[lower_row:upper_row]\n",
    "\n",
    "            # create training set\n",
    "            x_trn = np.concatenate((x[0:lower_row], x[upper_row:]))\n",
    "            y_trn = np.concatenate((y[0:lower_row], y[upper_row:]))\n",
    "\n",
    "            # train model\n",
    "            model.fit(x_trn, y_trn)\n",
    "\n",
    "            # run validation set through model\n",
    "            y_hat = model.predict(x_val)\n",
    "            accuracy += evaluate_acc(y_val, y_hat)\n",
    "\n",
    "        return accuracy / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, alpha=0.001, threshold = 0.0005):\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.stop = False\n",
    "        self.weights = None\n",
    "        self.max_iter = 10000\n",
    "        self.change = []\n",
    "\n",
    "    def __intercept(self, X):\n",
    "        return np.c_[np.ones(len(X)), X]\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def __grad(self, X_i, y_i):\n",
    "        z = np.dot(self.weights.T, X_i)\n",
    "        return X_i*(y_i-self.__sigmoid(z))\n",
    "    \n",
    "    def __update(self, X, Y):\n",
    "        changeW = np.zeros(np.size(X, 1))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            grad = self.__grad(X[i], Y[i])\n",
    "            changeW = changeW + self.alpha*grad\n",
    "        self.change.append(np.linalg.norm(changeW))\n",
    "        self.weights = self.weights + changeW\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.change = [] # reset the gradients before running a new fit\n",
    "        padded_X = self.__intercept(X)\n",
    "        self.weights = np.zeros(np.size(padded_X,1))\n",
    "        \n",
    "        num_iter = 0\n",
    "        while self.change == [] or self.change[-1] > self.threshold and num_iter < self.max_iter:\n",
    "            self.__update(padded_X, Y)\n",
    "            num_iter+=1\n",
    "            \n",
    "            if (num_iter == self.max_iter):\n",
    "                print(f\"Warning, reached max iterations of {self.max_iter}, stopping because we haven't converged yet\")\n",
    "                break\n",
    "\n",
    "        print(f\"learning rate:{self.alpha} \\n stop threshold:{self.threshold} \\n number of iterations: {num_iter}\")\n",
    "        print(f\"weights:{self.weights}\")\n",
    "        \n",
    "        return self.weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        padded_X = self.__intercept(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(X)):\n",
    "            Z = np.dot(self.weights.T, padded_X[i])\n",
    "            pred = self.__sigmoid(Z).round()\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2478\n",
      "weights:[-8.73376447  0.42100942  0.2638203   0.15393149  0.2476816  -0.02602803\n",
      "  0.43092158  0.34816798  0.1423106   0.23651423]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 3817\n",
      "weights:[-11.51891952   0.65191633  -0.03875821   0.28358922   0.42365216\n",
      "   0.0261974    0.3445303    0.5355565    0.40110438   0.89270398]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2181\n",
      "weights:[-9.15461191  0.44457472 -0.05989988  0.3742316   0.24228946  0.11673248\n",
      "  0.36603158  0.4744639   0.20678322  0.35313014]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2243\n",
      "weights:[-9.24656417  0.46134985  0.02934435  0.34663377  0.39038616  0.08425962\n",
      "  0.38456313  0.33594039  0.16692075  0.4423335 ]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 1901\n",
      "weights:[-8.96909556  0.49106145 -0.05104293  0.33683637  0.28933323  0.1039448\n",
      "  0.35834415  0.35902555  0.18947939  0.43196568]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.44444444444444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "k_fold(X, Y, 5, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-238.6202961    13.30005716    2.10932043    4.45296995    5.93992835\n",
      "    4.65260375    8.60426831    9.72560884    1.60836691   16.04879417]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-2.93950976e+02  1.67587172e+01 -3.14806181e+00  5.66048679e+00\n",
      "  1.04643890e+01 -1.08123477e-01  6.44782010e+00  1.42651101e+01\n",
      "  8.42807625e+00  2.68256680e+01]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-256.52149304   20.64212512   -0.62939669   12.34498756   10.73544487\n",
      "    8.63026927   13.10171349   18.00154207    7.98362522    8.87807796]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-245.80275739   14.65693351    1.40950682    9.64672327   11.97198907\n",
      "    5.27673178   10.11265646   11.76964129    6.14933515   10.4774469 ]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-294.82464315   18.704577     -1.79197404    9.65449318   10.77273003\n",
      "    6.24930397   10.11715953   14.34293134    6.19838033   22.81382647]\n",
      "95.25925925925927\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.01 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-35.97382858   2.40511719   1.39958258   0.95554357   1.1741935\n",
      "   0.32130825   1.98444226   2.01702149   0.87840069   1.52383217]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.01 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-44.40431351   3.07140859  -0.05505491   1.71729283   1.931675\n",
      "   0.58737776   1.75029724   2.40130599   1.59731362   3.73510386]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.01 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-44.67741134   3.40233331  -0.11256022   2.41027457   1.64220287\n",
      "   1.2033508    2.2741297    3.05486693   1.42273463   1.72234414]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.01 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-5.15511410e+01  2.93150863e+00  2.82147633e-02  1.71400294e+00\n",
      "  2.32223190e+00  8.38133187e-01  1.93876214e+00  2.16247463e+00\n",
      "  8.53315041e-01  3.23997126e+00]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.01 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-54.72848294  -1.40625617   5.65872375   7.11715276   2.04900878\n",
      "  -4.5384342    6.92761297  -2.86431178   4.12294937   0.75242095]\n",
      "96.59259259259258\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-18.75216705   1.1813079    0.8511734    0.47933514   0.62082356\n",
      "   0.09085547   1.09341457   0.99324339   0.49561727   0.69316707]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-2.41653627e+01  1.61565720e+00 -1.24577263e-03  8.33264909e-01\n",
      "  9.99311195e-01  1.78466239e-01  9.48649025e-01  1.31996074e+00\n",
      "  8.76948944e-01  2.03626333e+00]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-24.61107068   1.58104324  -0.28505518   1.16476522   0.76438258\n",
      "   0.41469846   1.05928365   1.54630197   0.61742207   1.10784293]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-22.43676942   0.93687117   0.09726718   0.66796832   0.64599668\n",
      "   0.18535888   0.82124347   0.77704301   0.25209721   1.08563431]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-25.38300269   1.37629666  -0.31864564   0.7893239    0.61705601\n",
      "   0.26632374   0.84462045   0.98592416   0.35270464   1.20651481]\n",
      "97.18518518518519\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2478\n",
      "weights:[-8.73376447  0.42100942  0.2638203   0.15393149  0.2476816  -0.02602803\n",
      "  0.43092158  0.34816798  0.1423106   0.23651423]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 3817\n",
      "weights:[-11.51891952   0.65191633  -0.03875821   0.28358922   0.42365216\n",
      "   0.0261974    0.3445303    0.5355565    0.40110438   0.89270398]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2181\n",
      "weights:[-9.15461191  0.44457472 -0.05989988  0.3742316   0.24228946  0.11673248\n",
      "  0.36603158  0.4744639   0.20678322  0.35313014]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2243\n",
      "weights:[-9.24656417  0.46134985  0.02934435  0.34663377  0.39038616  0.08425962\n",
      "  0.38456313  0.33594039  0.16692075  0.4423335 ]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 1901\n",
      "weights:[-8.96909556  0.49106145 -0.05104293  0.33683637  0.28933323  0.1039448\n",
      "  0.35834415  0.35902555  0.18947939  0.43196568]\n",
      "96.44444444444444\n",
      "learning rate:0.0005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 3190\n",
      "weights:[-8.08811001  0.36984556  0.29164761  0.1492008   0.23245444 -0.06182856\n",
      "  0.429951    0.30353427  0.1537977   0.14875283]\n",
      "learning rate:0.0005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 4438\n",
      "weights:[-1.03625261e+01  5.64802028e-01 -6.30364830e-03  2.82861473e-01\n",
      "  3.73427104e-01  6.17667945e-03  3.40289758e-01  4.64789091e-01\n",
      "  3.68309607e-01  7.42190040e-01]\n",
      "learning rate:0.0005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 3003\n",
      "weights:[-8.62028551  0.40342255 -0.02867653  0.36246048  0.22029218  0.10096283\n",
      "  0.35810406  0.43308457  0.19953896  0.3257927 ]\n",
      "learning rate:0.0005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 3117\n",
      "weights:[-8.67617259  0.42203954  0.05328855  0.33643964  0.36060974  0.06666233\n",
      "  0.37594398  0.30198164  0.16311296  0.39327803]\n",
      "learning rate:0.0005 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 2858\n",
      "weights:[-8.47196958  0.45422216 -0.03351394  0.33299815  0.26926716  0.08909976\n",
      "  0.35384714  0.32360117  0.18476145  0.38718738]\n",
      "96.44444444444444\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(0.05)\n",
    "print(k_fold(X, Y, 5, lr))\n",
    "lr = LogisticRegression(0.01)\n",
    "print(k_fold(X, Y, 5, lr))\n",
    "lr = LogisticRegression(0.005)\n",
    "print(k_fold(X, Y, 5, lr))\n",
    "lr = LogisticRegression(0.001)\n",
    "print(k_fold(X, Y, 5, lr))\n",
    "lr = LogisticRegression(0.0005)\n",
    "print(k_fold(X, Y, 5, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
