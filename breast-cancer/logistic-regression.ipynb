{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(file_path, delimiter, skiprows=0):\n",
    "    \"\"\"loads a data file and returns a numpy array\"\"\"\n",
    "    file = open(file_path, \"rb\")\n",
    "    arr = np.loadtxt(file, delimiter=delimiter, skiprows=skiprows)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"breast-cancer-wisconsin.csv\", \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    row[-1] = 0 if row[-1] == 2 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[:,1:10]\n",
    "Y=np.ravel((data[:,-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = true labels\n",
    "# y_hat = training labels\n",
    "# return: accuracy of training labels (in percentage)\n",
    "# Ensure that y and y_hat contain the labels for the same training examples.\n",
    "def evaluate_acc(y, y_hat):\n",
    "    score = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == y_hat[i]:\n",
    "            score += 1\n",
    "    return (score / y.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = class labels of training examples\n",
    "# x = feature data of training examples\n",
    "# 2 < k = number of folds to use in validation\n",
    "# return: average of prediction error over the k rounds of execution\n",
    "def k_fold(x, y, k, model):\n",
    "    if k < 1:\n",
    "        return \"Must have at least 1 fold.\"\n",
    "    elif k > (x.shape[0]//2):\n",
    "        return \"Too many folds.\"\n",
    "    elif k == 1:\n",
    "        print(\"1 fold selected - model will be trained and validated on same data set\")\n",
    "        model.fit(x, y)\n",
    "        return evaluate_acc(y, model.predict(x))\n",
    "    else:\n",
    "        rows_per_fold = (x.shape[0] + 1)//k       # a few rows at the end of the training data will be unused\n",
    "        accuracy = 0\n",
    "\n",
    "        for exec_round in range(k):\n",
    "            # determine held-out range\n",
    "            lower_row = exec_round * rows_per_fold\n",
    "            upper_row = ((exec_round + 1) * rows_per_fold) - 1\n",
    "            \n",
    "            # create validation set\n",
    "            x_val = np.copy(x)[lower_row:upper_row]\n",
    "            y_val = np.copy(y)[lower_row:upper_row]\n",
    "\n",
    "            # create training set\n",
    "            x_trn = np.concatenate((x[0:lower_row], x[upper_row:]))\n",
    "            y_trn = np.concatenate((y[0:lower_row], y[upper_row:]))\n",
    "\n",
    "            # train model\n",
    "            model.fit(x_trn, y_trn)\n",
    "\n",
    "            # run validation set through model\n",
    "            y_hat = model.predict(x_val)\n",
    "            accuracy += evaluate_acc(y_val, y_hat)\n",
    "\n",
    "        return accuracy / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, alpha=0.001, threshold = 0.0005):\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.stop = False\n",
    "        self.weights = None\n",
    "        self.max_iter = 10000\n",
    "        self.change = []\n",
    "\n",
    "    def __intercept(self, X):\n",
    "        return np.c_[np.ones(len(X)), X]\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def __grad(self, X_i, y_i):\n",
    "        z = np.dot(self.weights.T, X_i)\n",
    "        return X_i*(y_i-self.__sigmoid(z))\n",
    "    \n",
    "    def __update(self, X, Y):\n",
    "        changeW = np.zeros(np.size(X, 1))\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            grad = self.__grad(X[i], Y[i])\n",
    "            changeW = changeW + self.alpha*grad\n",
    "        self.change.append(np.linalg.norm(changeW))\n",
    "        self.weights = self.weights + changeW\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.change = [] # reset the gradients before running a new fit\n",
    "        padded_X = self.__intercept(X)\n",
    "        self.weights = np.zeros(np.size(padded_X,1))\n",
    "        \n",
    "        num_iter = 0\n",
    "        while self.change == [] or self.change[-1] > self.threshold and num_iter < self.max_iter:\n",
    "            self.__update(padded_X, Y)\n",
    "            num_iter+=1\n",
    "            \n",
    "            if (num_iter == self.max_iter):\n",
    "                print(f\"Warning, reached max iterations of {self.max_iter}, stopping because we haven't converged yet\")\n",
    "                break\n",
    "\n",
    "        print(f\"learning rate:{self.alpha} \\n stop threshold:{self.threshold} \\n number of iterations: {num_iter}\")\n",
    "        print(f\"weights:{self.weights}\")\n",
    "        \n",
    "        return self.weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        padded_X = self.__intercept(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(X)):\n",
    "            Z = np.dot(self.weights.T, padded_X[i])\n",
    "            pred = self.__sigmoid(Z).round()\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:0.001 \n",
      " stop threshold:0.0002 \n",
      " number of iterations: 4009\n",
      "weights:[-9.21846794e+00  4.57654155e-01  2.44140004e-01  1.59499609e-01\n",
      "  2.60163201e-01 -1.16421552e-03  4.32458808e-01  3.78837694e-01\n",
      "  1.33913577e-01  3.11444890e-01]\n",
      "Warning, reached max iterations of 5000, stopping because we haven't converged yet\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0002 \n",
      " number of iterations: 5000\n",
      "weights:[-12.00112852   0.68824535  -0.05098261   0.28361941   0.44429868\n",
      "   0.03412249   0.34651163   0.56463723   0.41573122   0.95229659]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0002 \n",
      " number of iterations: 3366\n",
      "weights:[-9.5343657   0.473617   -0.08087272  0.3824713   0.25780921  0.1272465\n",
      "  0.37188773  0.50335314  0.21237498  0.37232926]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0002 \n",
      " number of iterations: 3525\n",
      "weights:[-9.6568054   0.48951105  0.01338228  0.35384836  0.4118834   0.09600074\n",
      "  0.39121251  0.35895796  0.17021498  0.47914982]\n",
      "learning rate:0.001 \n",
      " stop threshold:0.0002 \n",
      " number of iterations: 2994\n",
      "weights:[-9.31904696  0.51658909 -0.06245557  0.33931854  0.30334659  0.11382729\n",
      "  0.36166383  0.38340896  0.19315242  0.46596119]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.5925925925926"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "k_fold(X, Y, 5, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-238.6202961    13.30005716    2.10932043    4.45296995    5.93992835\n",
      "    4.65260375    8.60426831    9.72560884    1.60836691   16.04879417]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-2.93950976e+02  1.67587172e+01 -3.14806181e+00  5.66048679e+00\n",
      "  1.04643890e+01 -1.08123477e-01  6.44782010e+00  1.42651101e+01\n",
      "  8.42807625e+00  2.68256680e+01]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-256.52149304   20.64212512   -0.62939669   12.34498756   10.73544487\n",
      "    8.63026927   13.10171349   18.00154207    7.98362522    8.87807796]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-245.80275739   14.65693351    1.40950682    9.64672327   11.97198907\n",
      "    5.27673178   10.11265646   11.76964129    6.14933515   10.4774469 ]\n",
      "Warning, reached max iterations of 10000, stopping because we haven't converged yet\n",
      "learning rate:0.05 \n",
      " stop threshold:0.0005 \n",
      " number of iterations: 10000\n",
      "weights:[-294.82464315   18.704577     -1.79197404    9.65449318   10.77273003\n",
      "    6.24930397   10.11715953   14.34293134    6.19838033   22.81382647]\n",
      "95.25925925925927\n"
     ]
    }
   ],
   "source": [
    "# lr = LogisticRegression(0.05)\n",
    "# print(k_fold(X, Y, 5, lr))\n",
    "# lr = LogisticRegression(0.01)\n",
    "# print(k_fold(X, Y, 5, lr))\n",
    "# lr = LogisticRegression(0.005)\n",
    "# print(k_fold(X, Y, 5, lr))\n",
    "# lr = LogisticRegression(0.001)\n",
    "# print(k_fold(X, Y, 5, lr))\n",
    "# lr = LogisticRegression(0.0005)\n",
    "# print(k_fold(X, Y, 5, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
