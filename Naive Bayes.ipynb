{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = np.load('./Data/cleaned_data_train.npy', allow_pickle=True)\n",
    "feature_matrix = sp.load_npz('./Data/feature_matrix_train.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes():\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.alpha = alpha # used for Laplace smoothing\n",
    "        self.classes = None\n",
    "        self.classes_log_priors = None\n",
    "    \n",
    "    def __group_samples(self, X,Y):\n",
    "        # append X|Y arrays\n",
    "        XY = np.column_stack((X,Y))\n",
    "        \n",
    "        # initialize array of empty arrays with length of number of classes\n",
    "        group_by_class = [[] for _ in range(len(self.classes))]\n",
    "        \n",
    "        # for each class, append an X|y sample into array index i if y == classes[i]\n",
    "        for class_index in range(len(self.classes)):\n",
    "            for sample in XY:\n",
    "                if sample[-1] == self.classes[class_index]:\n",
    "                    group_by_class[class_index].append(sample)\n",
    "        return group_by_class\n",
    "    \n",
    "    def __predict(self, X):\n",
    "#         return [(np.log(self.features_probs[i])) for i in range(len(self.classes))]\n",
    "        scores = np.zeros(len(self.classes))\n",
    "        for i in range(len(self.classes)):\n",
    "            sum = 0\n",
    "            for j in range(len(X)):\n",
    "                if (X[j] == 0):\n",
    "                    sum += np.log(1 - self.features_probs[i][j])\n",
    "                else:\n",
    "                    sum += np.log(self.features_probs[i][j])\n",
    "            sum += self.classes_log_priors[i]\n",
    "            scores[i] = sum\n",
    "            \n",
    "        \n",
    "        return np.argmax(scores)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "        num_samples = len(X)\n",
    "        groups = self.__group_samples(X,Y)\n",
    "\n",
    "        self.classes_log_priors = np.array(list(map(lambda g: np.log(len(g)/num_samples), groups)))\n",
    "\n",
    "        # get count per group, number of samples per group, and divide.\n",
    "        word_count = np.array(list(map(lambda g: np.array(g).sum(axis=0)[:-1] + self.alpha, groups)))\n",
    "        group_count = np.array(list(map(lambda g: len(g), groups)))\n",
    "        self.features_probs = word_count/((group_count + 2*self.alpha)[:,None])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.__predict(X)\n",
    "#         return np.argmax(self.__predict(X), axis=1);\n",
    "#         return np.argmax(self.__predict(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,1,0],[1,0,1],[0,0,1],[1,1,1]])\n",
    "Y = np.array([6,7,6,6])\n",
    "b = BernoulliNaiveBayes()\n",
    "\n",
    "b.fit(X,Y)\n",
    "print(b.predict([1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([11, 13, 15]), array([4, 5, 6])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_samples = len(X)\n",
    "XY = np.column_stack((X,Y))\n",
    "group_by_class = [[] for _ in range(len(classes))]\n",
    "for class_index in range(len(classes)):\n",
    "    for sample in XY:\n",
    "        if sample[-1] == classes[class_index]:\n",
    "            group_by_class[class_index].append(sample)\n",
    "\n",
    "\n",
    "# group_by_class[0][0][:-1]\n",
    "\n",
    "list(map(lambda c: np.array(c).sum(axis=0)[:-1] + 1, group_by_class))\n",
    "\n",
    "            \n",
    "# z = zip(X,Y)\n",
    "# print(list(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_path = './Data/reddit_test.csv'\n",
    "\n",
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#clean\n",
    "comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "\n",
    "#load\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "#clean\n",
    "test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', '')\n",
    "test_data['prep'] = test_data['prep'].str.lower()\n",
    "test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
