{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to run nltk.download() if you're getting errors\n",
    "import string\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this code just takes the data set and creates an embedding layer with randomly initialized word vectors for the positive examples.\n",
    "\n",
    "# TODO\n",
    "\n",
    "* Include negative examples as well\n",
    "* The rest of the CNN-rand model\n",
    "* Encapsulate the model code into a Python class\n",
    "* K-fold cross-validation pipeline\n",
    "* Tune hyperparameters to achieve better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13639\n",
      "Max sentence length: 51\n"
     ]
    }
   ],
   "source": [
    "pos_examples = open('data/rt-polarity.pos', encoding='ISO-8859-1').readlines()\n",
    "# neg_examples = open('data/rt-polarity.neg', encoding='ISO-8859-1').readlines()\n",
    "\n",
    "vocab = collections.defaultdict(int)\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# 1. Tokenize all strings\n",
    "token_pos = list(map(lambda ex: word_tokenize(ex.translate(translator)), pos_examples))\n",
    "\n",
    "# 2. Get vocabulary size and max sentence length\n",
    "max_sentence_length = 0\n",
    "for ex in token_pos:\n",
    "    max_sentence_length = max(max_sentence_length, len(ex))\n",
    "    for word in ex:\n",
    "        vocab[word]+=1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size: \" + str(len(vocab)))\n",
    "print(\"Max sentence length: \" + str(max_sentence_length))\n",
    "\n",
    "# 3. One-Hot encode and pad.\n",
    "encoded_pos = [preprocessing.text.one_hot(ex, vocab_size) for ex in pos_examples]\n",
    "padded_pos = preprocessing.sequence.pad_sequences(encoded_pos, maxlen=max_sentence_length, padding='post')\n",
    "\n",
    "\n",
    "# 4. Create the CNN.\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
