{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# loading the original MNIST hand-written digits\n",
    "mndata = MNIST('')\n",
    "mndata.gz = True\n",
    "\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALUklEQVR4nO3dQaxc5XnG8f9TkmwIUk0RlktISSt2WZAKsSmq6CIRZQNZpAorR6nkLEqV7oLSRZCiSFHVpstKjoLiVilRJKAgVDVBKApZRRhEwcRKoBFJHCxbyK1KVmng7eIeo2tz753rOTNzzvX7/0mjmTl37jmvj/34+873zcyXqkLS1e93pi5A0mYYdqkJwy41YdilJgy71MT7NnmwJA79S2tWVdlp+6iWPcndSX6S5LUkD47Zl6T1yrLz7EmuAX4KfBw4AzwH3F9VP97jd2zZpTVbR8t+B/BaVf2sqn4DfBu4d8T+JK3RmLDfBPxy2/Mzw7ZLJDmW5GSSkyOOJWmkMQN0O3UV3tNNr6rjwHGwGy9NaUzLfga4edvzDwFvjCtH0rqMCftzwK1JPpLkA8CngSdXU5akVVu6G19Vv03yAPBd4Brg4ap6ZWWVSVqppafeljqY1+zS2q3lTTWSDg7DLjVh2KUmDLvUhGGXmjDsUhOGXWrCsEtNGHapCcMuNWHYpSYMu9SEYZeaMOxSE4ZdasKwS00YdqkJwy41YdilJgy71IRhl5rY6JLN6med316c7PglqtqFLbvUhGGXmjDsUhOGXWrCsEtNGHapCcMuNeE8u0bZ5CrAV3ps5+EvNSrsSV4H3gLeBn5bVbevoihJq7eKlv3PqurNFexH0hp5zS41MTbsBXwvyfNJju30giTHkpxMcnLksSSNkDEDLEl+v6reSHIj8DTw11X17B6vn240R2sx5QDdIl0H6Kpqxz/4qJa9qt4Y7s8DjwN3jNmfpPVZOuxJrk1y3cXHwCeAU6sqTNJqjRmNPww8PnSV3gf8a1X9x0qq0mzMuZuuKzPqmv2KD+Y1+4FzkMPuNfulnHqTmjDsUhOGXWrCsEtNGHapCT/i2tycR9sXjaYvqn3ku0OX/t25smWXmjDsUhOGXWrCsEtNGHapCcMuNWHYpSacZ78KzHmuXPNhyy41YdilJgy71IRhl5ow7FIThl1qwrBLTTjPrgNr7Ofdu7Fll5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmnGc/AKacLx77/el71X41fjf7nC1s2ZM8nOR8klPbtl2f5Okkrw73h9ZbpqSx9tON/yZw92XbHgSeqapbgWeG55JmbGHYq+pZ4MJlm+8FTgyPTwD3rbguSSu27DX74ao6C1BVZ5PcuNsLkxwDji15HEkrsvYBuqo6DhwHSOInE6SJLDv1di7JEYDh/vzqSpK0DsuG/Ung6PD4KPDEasqRtC5ZNIeb5BHgLuAG4BzwJeDfgO8AHwZ+AXyqqi4fxNtpX3bjd3CQ59HnbJ3ndc7nrap2LG5h2FfJsO/MsK+HYb+Ub5eVmjDsUhOGXWrCsEtNGHapCT/iugHrHm2f88iw5sOWXWrCsEtNGHapCcMuNWHYpSYMu9SEYZeacJ79AHAeXatgyy41YdilJgy71IRhl5ow7FIThl1qwrBLTTjPvgJTfjustF+27FIThl1qwrBLTRh2qQnDLjVh2KUmDLvUhGGXmlgY9iQPJzmf5NS2bQ8l+VWSF4fbPestU9JY+2nZvwncvcP2f6yq24bbv6+2LEmrtjDsVfUscGEDtUhaozHX7A8keWno5h/a7UVJjiU5meTkiGNJGin7+RBHkluAp6rqo8Pzw8CbQAFfBo5U1Wf3sZ+r8hMjLtw4T+v8e5nz30lV7VjcUi17VZ2rqrer6h3g68AdY4qTtH5LhT3JkW1PPwmc2u21kuZh4efZkzwC3AXckOQM8CXgriS3sdWNfx343BprvOrNuUs4Z1276cva1zX7yg7mNfuOrsZ/WJtg2He20mt2SQePYZeaMOxSE4ZdasKwS034VdKaLUfbV8uWXWrCsEtNGHapCcMuNWHYpSYMu9SEYZeacJ5dk/EbfjbLll1qwrBLTRh2qQnDLjVh2KUmDLvUhGGXmnCeXaNs8tuJL+c8+pWxZZeaMOxSE4ZdasKwS00YdqkJwy41YdilJpxnn4FFc9XrnE+ecp58EefRV2thy57k5iTfT3I6yStJPj9svz7J00leHe4Prb9cSctauD57kiPAkap6Icl1wPPAfcBngAtV9dUkDwKHquoLC/Y132ZkhIP8jSu27Fefpddnr6qzVfXC8Pgt4DRwE3AvcGJ42Qm2/gOQNFNXdM2e5BbgY8CPgMNVdRa2/kNIcuMuv3MMODauTEljLezGv/vC5IPAD4CvVNVjSf6nqn5328//u6r2vG63G78cu/G6Ekt34wGSvB94FPhWVT02bD43XM9fvK4/v4pCJa3HfkbjA3wDOF1VX9v2oyeBo8Pjo8ATqy9PsNX6rus2pSR73rRa+xmNvxP4IfAy8M6w+YtsXbd/B/gw8AvgU1V1YcG+5ttnHGHq0BxUBno9duvG7/uafRUMu7Yz7Osx6ppd0sFn2KUmDLvUhGGXmjDsUhN+xFWjOKJ+cNiyS00YdqkJwy41YdilJgy71IRhl5ow7FITzrOvwKK55jl/Ks558j5s2aUmDLvUhGGXmjDsUhOGXWrCsEtNGHapCefZN8C5bM2BLbvUhGGXmjDsUhOGXWrCsEtNGHapCcMuNbGf9dlvTvL9JKeTvJLk88P2h5L8KsmLw+2e9ZcraVn7WZ/9CHCkql5Ich3wPHAf8BfAr6vq7/d9sKt0yWZpTnZbsnnhO+iq6ixwdnj8VpLTwE2rLU/Sul3RNXuSW4CPAT8aNj2Q5KUkDyc5tMvvHEtyMsnJUZVKGmVhN/7dFyYfBH4AfKWqHktyGHgTKODLbHX1P7tgH3bjpTXbrRu/r7AneT/wFPDdqvraDj+/BXiqqj66YD+GXVqz3cK+n9H4AN8ATm8P+jBwd9EngVNji5S0PvsZjb8T+CHwMvDOsPmLwP3AbWx1418HPjcM5u21L1t2ac1GdeNXxbBL67d0N17S1cGwS00YdqkJwy41YdilJgy71IRhl5ow7FIThl1qwrBLTRh2qQnDLjVh2KUmDLvUxKaXbH4T+Pm25zcM2+ZorrXNtS6wtmWtsrY/2O0HG/08+3sOnpysqtsnK2APc61trnWBtS1rU7XZjZeaMOxSE1OH/fjEx9/LXGuba11gbcvaSG2TXrNL2pypW3ZJG2LYpSYmCXuSu5P8JMlrSR6coobdJHk9ycvDMtSTrk83rKF3PsmpbduuT/J0kleH+x3X2Juotlks473HMuOTnruplz/f+DV7kmuAnwIfB84AzwH3V9WPN1rILpK8DtxeVZO/ASPJnwK/Bv754tJaSf4OuFBVXx3+ozxUVV+YSW0PcYXLeK+ptt2WGf8ME567VS5/vowpWvY7gNeq6mdV9Rvg28C9E9Qxe1X1LHDhss33AieGxyfY+seycbvUNgtVdbaqXhgevwVcXGZ80nO3R10bMUXYbwJ+ue35Gea13nsB30vyfJJjUxezg8MXl9ka7m+cuJ7LLVzGe5MuW2Z8NudumeXPx5oi7DstTTOn+b8/qao/Bv4c+Kuhu6r9+Sfgj9haA/As8A9TFjMsM/4o8DdV9b9T1rLdDnVt5LxNEfYzwM3bnn8IeGOCOnZUVW8M9+eBx9m67JiTcxdX0B3uz09cz7uq6lxVvV1V7wBfZ8JzNywz/ijwrap6bNg8+bnbqa5Nnbcpwv4ccGuSjyT5APBp4MkJ6niPJNcOAyckuRb4BPNbivpJ4Ojw+CjwxIS1XGIuy3jvtsw4E5+7yZc/r6qN34B72BqR/y/gb6eoYZe6/hD4z+H2ytS1AY+w1a37P7Z6RH8J/B7wDPDqcH/9jGr7F7aW9n6JrWAdmai2O9m6NHwJeHG43TP1udujro2cN98uKzXhO+ikJgy71IRhl5ow7FIThl1qwrBLTRh2qYn/B/lvIMPXJ3jeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = np.asarray(images)\n",
    "imgs = np.reshape(imgs, (-1, 28, 28))\n",
    "# thresholds imgs\n",
    "imgs = np.where(imgs < 100, 0, 255)\n",
    "print(imgs.shape)\n",
    "\n",
    "plt.imshow(imgs[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3013: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1259: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2880: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 14)        140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 14)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 28)          9828      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 28)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 28)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 784)               352016    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 369,834\n",
      "Trainable params: 369,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(14, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(28, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(784, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(imgs).reshape(-1,28,28,1)\n",
    "\n",
    "# onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "# y_train = onehot_encoder.fit_transform(np.reshape(labels, (-1, 1)))\n",
    "\n",
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.30, random_state=0, stratify=labels)\n",
    "X_train = np.array(X_train)/255\n",
    "X_test = np.array(X_test)/255\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1,28,28,1)\n",
    "X_test = np.array(X_test).reshape(-1,28,28,1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(np.reshape(y_train, (-1,1)))\n",
    "y_test = onehot_encoder.transform(np.reshape(y_test, (-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:953: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\ip_pr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - ETA: 25s - loss: 2.3392 - acc: 0.11 - ETA: 17s - loss: 2.3073 - acc: 0.12 - ETA: 14s - loss: 2.2850 - acc: 0.13 - ETA: 12s - loss: 2.2667 - acc: 0.15 - ETA: 11s - loss: 2.2423 - acc: 0.17 - ETA: 10s - loss: 2.2186 - acc: 0.19 - ETA: 9s - loss: 2.1932 - acc: 0.2191 - ETA: 9s - loss: 2.1643 - acc: 0.242 - ETA: 9s - loss: 2.1337 - acc: 0.262 - ETA: 8s - loss: 2.1022 - acc: 0.281 - ETA: 8s - loss: 2.0664 - acc: 0.300 - ETA: 7s - loss: 2.0309 - acc: 0.315 - ETA: 7s - loss: 1.9900 - acc: 0.333 - ETA: 7s - loss: 1.9477 - acc: 0.349 - ETA: 6s - loss: 1.9092 - acc: 0.362 - ETA: 6s - loss: 1.8681 - acc: 0.377 - ETA: 6s - loss: 1.8308 - acc: 0.390 - ETA: 6s - loss: 1.7938 - acc: 0.402 - ETA: 5s - loss: 1.7567 - acc: 0.415 - ETA: 5s - loss: 1.7237 - acc: 0.426 - ETA: 5s - loss: 1.6887 - acc: 0.438 - ETA: 5s - loss: 1.6551 - acc: 0.449 - ETA: 4s - loss: 1.6259 - acc: 0.459 - ETA: 4s - loss: 1.5953 - acc: 0.469 - ETA: 4s - loss: 1.5682 - acc: 0.478 - ETA: 4s - loss: 1.5409 - acc: 0.487 - ETA: 3s - loss: 1.5148 - acc: 0.496 - ETA: 3s - loss: 1.4900 - acc: 0.504 - ETA: 3s - loss: 1.4673 - acc: 0.511 - ETA: 3s - loss: 1.4446 - acc: 0.520 - ETA: 2s - loss: 1.4252 - acc: 0.527 - ETA: 2s - loss: 1.4032 - acc: 0.535 - ETA: 2s - loss: 1.3815 - acc: 0.542 - ETA: 2s - loss: 1.3591 - acc: 0.549 - ETA: 1s - loss: 1.3411 - acc: 0.555 - ETA: 1s - loss: 1.3247 - acc: 0.560 - ETA: 1s - loss: 1.3080 - acc: 0.566 - ETA: 1s - loss: 1.2910 - acc: 0.572 - ETA: 0s - loss: 1.2752 - acc: 0.577 - ETA: 0s - loss: 1.2603 - acc: 0.582 - ETA: 0s - loss: 1.2461 - acc: 0.587 - 11s 253us/step - loss: 1.2304 - acc: 0.5924\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.5989 - acc: 0.79 - ETA: 10s - loss: 0.6303 - acc: 0.79 - ETA: 10s - loss: 0.6047 - acc: 0.80 - ETA: 9s - loss: 0.6058 - acc: 0.8067 - ETA: 9s - loss: 0.5981 - acc: 0.809 - ETA: 9s - loss: 0.5966 - acc: 0.810 - ETA: 8s - loss: 0.6000 - acc: 0.809 - ETA: 8s - loss: 0.5946 - acc: 0.811 - ETA: 8s - loss: 0.5839 - acc: 0.814 - ETA: 8s - loss: 0.5780 - acc: 0.816 - ETA: 7s - loss: 0.5738 - acc: 0.817 - ETA: 7s - loss: 0.5722 - acc: 0.817 - ETA: 7s - loss: 0.5722 - acc: 0.815 - ETA: 7s - loss: 0.5715 - acc: 0.815 - ETA: 6s - loss: 0.5677 - acc: 0.816 - ETA: 6s - loss: 0.5642 - acc: 0.818 - ETA: 6s - loss: 0.5618 - acc: 0.819 - ETA: 6s - loss: 0.5580 - acc: 0.819 - ETA: 5s - loss: 0.5537 - acc: 0.821 - ETA: 5s - loss: 0.5504 - acc: 0.822 - ETA: 5s - loss: 0.5481 - acc: 0.822 - ETA: 5s - loss: 0.5436 - acc: 0.824 - ETA: 4s - loss: 0.5430 - acc: 0.824 - ETA: 4s - loss: 0.5408 - acc: 0.826 - ETA: 4s - loss: 0.5394 - acc: 0.826 - ETA: 4s - loss: 0.5367 - acc: 0.827 - ETA: 3s - loss: 0.5325 - acc: 0.829 - ETA: 3s - loss: 0.5291 - acc: 0.830 - ETA: 3s - loss: 0.5269 - acc: 0.831 - ETA: 3s - loss: 0.5239 - acc: 0.832 - ETA: 2s - loss: 0.5214 - acc: 0.833 - ETA: 2s - loss: 0.5185 - acc: 0.834 - ETA: 2s - loss: 0.5163 - acc: 0.835 - ETA: 2s - loss: 0.5147 - acc: 0.835 - ETA: 1s - loss: 0.5125 - acc: 0.836 - ETA: 1s - loss: 0.5098 - acc: 0.837 - ETA: 1s - loss: 0.5062 - acc: 0.838 - ETA: 1s - loss: 0.5028 - acc: 0.840 - ETA: 0s - loss: 0.5010 - acc: 0.840 - ETA: 0s - loss: 0.4971 - acc: 0.841 - ETA: 0s - loss: 0.4944 - acc: 0.842 - 11s 251us/step - loss: 0.4924 - acc: 0.8434\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.3888 - acc: 0.88 - ETA: 10s - loss: 0.3678 - acc: 0.88 - ETA: 10s - loss: 0.3818 - acc: 0.87 - ETA: 10s - loss: 0.3808 - acc: 0.87 - ETA: 9s - loss: 0.3871 - acc: 0.8762 - ETA: 9s - loss: 0.3845 - acc: 0.877 - ETA: 8s - loss: 0.3836 - acc: 0.877 - ETA: 8s - loss: 0.3829 - acc: 0.878 - ETA: 8s - loss: 0.3816 - acc: 0.879 - ETA: 8s - loss: 0.3823 - acc: 0.879 - ETA: 7s - loss: 0.3826 - acc: 0.879 - ETA: 7s - loss: 0.3812 - acc: 0.879 - ETA: 7s - loss: 0.3803 - acc: 0.880 - ETA: 6s - loss: 0.3835 - acc: 0.878 - ETA: 6s - loss: 0.3827 - acc: 0.879 - ETA: 6s - loss: 0.3800 - acc: 0.880 - ETA: 6s - loss: 0.3802 - acc: 0.880 - ETA: 5s - loss: 0.3808 - acc: 0.880 - ETA: 5s - loss: 0.3805 - acc: 0.880 - ETA: 5s - loss: 0.3767 - acc: 0.881 - ETA: 5s - loss: 0.3756 - acc: 0.881 - ETA: 4s - loss: 0.3721 - acc: 0.883 - ETA: 4s - loss: 0.3711 - acc: 0.883 - ETA: 4s - loss: 0.3691 - acc: 0.884 - ETA: 4s - loss: 0.3660 - acc: 0.885 - ETA: 3s - loss: 0.3658 - acc: 0.885 - ETA: 3s - loss: 0.3643 - acc: 0.885 - ETA: 3s - loss: 0.3623 - acc: 0.886 - ETA: 3s - loss: 0.3616 - acc: 0.886 - ETA: 2s - loss: 0.3604 - acc: 0.886 - ETA: 2s - loss: 0.3594 - acc: 0.887 - ETA: 2s - loss: 0.3584 - acc: 0.887 - ETA: 2s - loss: 0.3582 - acc: 0.887 - ETA: 1s - loss: 0.3566 - acc: 0.888 - ETA: 1s - loss: 0.3561 - acc: 0.888 - ETA: 1s - loss: 0.3556 - acc: 0.889 - ETA: 1s - loss: 0.3547 - acc: 0.889 - ETA: 0s - loss: 0.3544 - acc: 0.889 - ETA: 0s - loss: 0.3535 - acc: 0.889 - ETA: 0s - loss: 0.3523 - acc: 0.889 - ETA: 0s - loss: 0.3515 - acc: 0.889 - 10s 240us/step - loss: 0.3511 - acc: 0.8901\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.3546 - acc: 0.892 - ETA: 9s - loss: 0.3384 - acc: 0.893 - ETA: 8s - loss: 0.3140 - acc: 0.901 - ETA: 8s - loss: 0.3087 - acc: 0.903 - ETA: 8s - loss: 0.3016 - acc: 0.905 - ETA: 8s - loss: 0.3018 - acc: 0.903 - ETA: 8s - loss: 0.3021 - acc: 0.904 - ETA: 8s - loss: 0.2986 - acc: 0.904 - ETA: 7s - loss: 0.3004 - acc: 0.904 - ETA: 7s - loss: 0.3049 - acc: 0.904 - ETA: 7s - loss: 0.3000 - acc: 0.905 - ETA: 7s - loss: 0.2996 - acc: 0.905 - ETA: 6s - loss: 0.2961 - acc: 0.905 - ETA: 6s - loss: 0.2971 - acc: 0.906 - ETA: 6s - loss: 0.2972 - acc: 0.906 - ETA: 6s - loss: 0.2972 - acc: 0.906 - ETA: 5s - loss: 0.2956 - acc: 0.906 - ETA: 5s - loss: 0.2960 - acc: 0.906 - ETA: 5s - loss: 0.2937 - acc: 0.906 - ETA: 5s - loss: 0.2915 - acc: 0.907 - ETA: 5s - loss: 0.2881 - acc: 0.908 - ETA: 4s - loss: 0.2895 - acc: 0.908 - ETA: 4s - loss: 0.2889 - acc: 0.908 - ETA: 4s - loss: 0.2904 - acc: 0.907 - ETA: 4s - loss: 0.2910 - acc: 0.907 - ETA: 3s - loss: 0.2882 - acc: 0.908 - ETA: 3s - loss: 0.2874 - acc: 0.908 - ETA: 3s - loss: 0.2874 - acc: 0.908 - ETA: 3s - loss: 0.2868 - acc: 0.909 - ETA: 3s - loss: 0.2859 - acc: 0.909 - ETA: 2s - loss: 0.2851 - acc: 0.909 - ETA: 2s - loss: 0.2855 - acc: 0.909 - ETA: 2s - loss: 0.2861 - acc: 0.909 - ETA: 2s - loss: 0.2855 - acc: 0.909 - ETA: 1s - loss: 0.2852 - acc: 0.909 - ETA: 1s - loss: 0.2855 - acc: 0.909 - ETA: 1s - loss: 0.2852 - acc: 0.910 - ETA: 1s - loss: 0.2857 - acc: 0.909 - ETA: 0s - loss: 0.2848 - acc: 0.910 - ETA: 0s - loss: 0.2832 - acc: 0.910 - ETA: 0s - loss: 0.2828 - acc: 0.911 - 11s 254us/step - loss: 0.2820 - acc: 0.9112\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 9s - loss: 0.2483 - acc: 0.921 - ETA: 9s - loss: 0.2498 - acc: 0.921 - ETA: 9s - loss: 0.2535 - acc: 0.920 - ETA: 9s - loss: 0.2549 - acc: 0.919 - ETA: 9s - loss: 0.2545 - acc: 0.919 - ETA: 9s - loss: 0.2480 - acc: 0.921 - ETA: 9s - loss: 0.2491 - acc: 0.921 - ETA: 8s - loss: 0.2488 - acc: 0.922 - ETA: 8s - loss: 0.2466 - acc: 0.923 - ETA: 8s - loss: 0.2483 - acc: 0.922 - ETA: 8s - loss: 0.2500 - acc: 0.921 - ETA: 7s - loss: 0.2516 - acc: 0.921 - ETA: 7s - loss: 0.2529 - acc: 0.920 - ETA: 7s - loss: 0.2541 - acc: 0.920 - ETA: 7s - loss: 0.2534 - acc: 0.920 - ETA: 6s - loss: 0.2567 - acc: 0.919 - ETA: 6s - loss: 0.2558 - acc: 0.919 - ETA: 6s - loss: 0.2541 - acc: 0.920 - ETA: 6s - loss: 0.2532 - acc: 0.920 - ETA: 5s - loss: 0.2512 - acc: 0.920 - ETA: 5s - loss: 0.2501 - acc: 0.920 - ETA: 5s - loss: 0.2513 - acc: 0.920 - ETA: 5s - loss: 0.2511 - acc: 0.920 - ETA: 4s - loss: 0.2500 - acc: 0.920 - ETA: 4s - loss: 0.2498 - acc: 0.920 - ETA: 4s - loss: 0.2505 - acc: 0.920 - ETA: 4s - loss: 0.2502 - acc: 0.920 - ETA: 3s - loss: 0.2498 - acc: 0.920 - ETA: 3s - loss: 0.2499 - acc: 0.920 - ETA: 3s - loss: 0.2500 - acc: 0.920 - ETA: 2s - loss: 0.2481 - acc: 0.921 - ETA: 2s - loss: 0.2482 - acc: 0.921 - ETA: 2s - loss: 0.2499 - acc: 0.921 - ETA: 2s - loss: 0.2486 - acc: 0.921 - ETA: 1s - loss: 0.2479 - acc: 0.921 - ETA: 1s - loss: 0.2477 - acc: 0.921 - ETA: 1s - loss: 0.2474 - acc: 0.922 - ETA: 1s - loss: 0.2460 - acc: 0.922 - ETA: 0s - loss: 0.2461 - acc: 0.922 - ETA: 0s - loss: 0.2453 - acc: 0.922 - ETA: 0s - loss: 0.2461 - acc: 0.922 - 11s 271us/step - loss: 0.2463 - acc: 0.9221\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.2219 - acc: 0.93 - ETA: 10s - loss: 0.2186 - acc: 0.93 - ETA: 11s - loss: 0.2285 - acc: 0.93 - ETA: 10s - loss: 0.2190 - acc: 0.93 - ETA: 10s - loss: 0.2171 - acc: 0.93 - ETA: 9s - loss: 0.2142 - acc: 0.9337 - ETA: 9s - loss: 0.2172 - acc: 0.933 - ETA: 9s - loss: 0.2176 - acc: 0.933 - ETA: 9s - loss: 0.2196 - acc: 0.933 - ETA: 9s - loss: 0.2168 - acc: 0.933 - ETA: 8s - loss: 0.2189 - acc: 0.932 - ETA: 8s - loss: 0.2189 - acc: 0.932 - ETA: 8s - loss: 0.2172 - acc: 0.932 - ETA: 7s - loss: 0.2188 - acc: 0.931 - ETA: 7s - loss: 0.2181 - acc: 0.931 - ETA: 7s - loss: 0.2192 - acc: 0.931 - ETA: 6s - loss: 0.2187 - acc: 0.931 - ETA: 6s - loss: 0.2170 - acc: 0.932 - ETA: 6s - loss: 0.2201 - acc: 0.931 - ETA: 5s - loss: 0.2226 - acc: 0.930 - ETA: 5s - loss: 0.2221 - acc: 0.930 - ETA: 5s - loss: 0.2228 - acc: 0.930 - ETA: 5s - loss: 0.2225 - acc: 0.930 - ETA: 4s - loss: 0.2224 - acc: 0.931 - ETA: 4s - loss: 0.2227 - acc: 0.930 - ETA: 4s - loss: 0.2234 - acc: 0.930 - ETA: 3s - loss: 0.2225 - acc: 0.930 - ETA: 3s - loss: 0.2216 - acc: 0.930 - ETA: 3s - loss: 0.2213 - acc: 0.930 - ETA: 3s - loss: 0.2217 - acc: 0.930 - ETA: 2s - loss: 0.2236 - acc: 0.929 - ETA: 2s - loss: 0.2229 - acc: 0.930 - ETA: 2s - loss: 0.2217 - acc: 0.930 - ETA: 2s - loss: 0.2219 - acc: 0.930 - ETA: 1s - loss: 0.2213 - acc: 0.930 - ETA: 1s - loss: 0.2208 - acc: 0.930 - ETA: 1s - loss: 0.2202 - acc: 0.931 - ETA: 1s - loss: 0.2191 - acc: 0.931 - ETA: 0s - loss: 0.2203 - acc: 0.931 - ETA: 0s - loss: 0.2203 - acc: 0.931 - ETA: 0s - loss: 0.2199 - acc: 0.931 - 11s 269us/step - loss: 0.2201 - acc: 0.9312\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.2453 - acc: 0.91 - ETA: 9s - loss: 0.2324 - acc: 0.9215 - ETA: 9s - loss: 0.2245 - acc: 0.926 - ETA: 10s - loss: 0.2215 - acc: 0.92 - ETA: 9s - loss: 0.2098 - acc: 0.9302 - ETA: 9s - loss: 0.2091 - acc: 0.930 - ETA: 9s - loss: 0.2081 - acc: 0.931 - ETA: 9s - loss: 0.2073 - acc: 0.932 - ETA: 9s - loss: 0.2058 - acc: 0.932 - ETA: 9s - loss: 0.2032 - acc: 0.933 - ETA: 8s - loss: 0.2025 - acc: 0.934 - ETA: 8s - loss: 0.2055 - acc: 0.933 - ETA: 8s - loss: 0.2042 - acc: 0.933 - ETA: 7s - loss: 0.2061 - acc: 0.932 - ETA: 7s - loss: 0.2069 - acc: 0.932 - ETA: 7s - loss: 0.2060 - acc: 0.933 - ETA: 6s - loss: 0.2046 - acc: 0.933 - ETA: 6s - loss: 0.2033 - acc: 0.934 - ETA: 6s - loss: 0.2036 - acc: 0.934 - ETA: 6s - loss: 0.2056 - acc: 0.933 - ETA: 5s - loss: 0.2055 - acc: 0.934 - ETA: 5s - loss: 0.2069 - acc: 0.934 - ETA: 5s - loss: 0.2073 - acc: 0.933 - ETA: 5s - loss: 0.2065 - acc: 0.933 - ETA: 4s - loss: 0.2071 - acc: 0.933 - ETA: 4s - loss: 0.2071 - acc: 0.934 - ETA: 4s - loss: 0.2064 - acc: 0.934 - ETA: 3s - loss: 0.2058 - acc: 0.934 - ETA: 3s - loss: 0.2054 - acc: 0.934 - ETA: 3s - loss: 0.2059 - acc: 0.934 - ETA: 3s - loss: 0.2048 - acc: 0.935 - ETA: 2s - loss: 0.2043 - acc: 0.935 - ETA: 2s - loss: 0.2041 - acc: 0.935 - ETA: 2s - loss: 0.2039 - acc: 0.935 - ETA: 1s - loss: 0.2028 - acc: 0.935 - ETA: 1s - loss: 0.2022 - acc: 0.936 - ETA: 1s - loss: 0.2015 - acc: 0.936 - ETA: 1s - loss: 0.2006 - acc: 0.936 - ETA: 0s - loss: 0.1997 - acc: 0.936 - ETA: 0s - loss: 0.1997 - acc: 0.936 - ETA: 0s - loss: 0.2000 - acc: 0.936 - 12s 291us/step - loss: 0.2000 - acc: 0.9368\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1792 - acc: 0.94 - ETA: 9s - loss: 0.1952 - acc: 0.9375 - ETA: 9s - loss: 0.1950 - acc: 0.940 - ETA: 9s - loss: 0.1965 - acc: 0.941 - ETA: 9s - loss: 0.1923 - acc: 0.941 - ETA: 8s - loss: 0.1893 - acc: 0.942 - ETA: 8s - loss: 0.1909 - acc: 0.941 - ETA: 8s - loss: 0.1929 - acc: 0.940 - ETA: 8s - loss: 0.1904 - acc: 0.941 - ETA: 7s - loss: 0.1898 - acc: 0.941 - ETA: 7s - loss: 0.1886 - acc: 0.941 - ETA: 7s - loss: 0.1873 - acc: 0.941 - ETA: 7s - loss: 0.1868 - acc: 0.942 - ETA: 6s - loss: 0.1870 - acc: 0.942 - ETA: 6s - loss: 0.1867 - acc: 0.942 - ETA: 6s - loss: 0.1848 - acc: 0.943 - ETA: 6s - loss: 0.1861 - acc: 0.942 - ETA: 6s - loss: 0.1862 - acc: 0.942 - ETA: 5s - loss: 0.1859 - acc: 0.942 - ETA: 5s - loss: 0.1858 - acc: 0.942 - ETA: 5s - loss: 0.1854 - acc: 0.942 - ETA: 5s - loss: 0.1844 - acc: 0.942 - ETA: 4s - loss: 0.1850 - acc: 0.942 - ETA: 4s - loss: 0.1849 - acc: 0.942 - ETA: 4s - loss: 0.1854 - acc: 0.942 - ETA: 4s - loss: 0.1856 - acc: 0.942 - ETA: 3s - loss: 0.1859 - acc: 0.941 - ETA: 3s - loss: 0.1870 - acc: 0.941 - ETA: 3s - loss: 0.1872 - acc: 0.941 - ETA: 3s - loss: 0.1867 - acc: 0.940 - ETA: 2s - loss: 0.1853 - acc: 0.941 - ETA: 2s - loss: 0.1849 - acc: 0.941 - ETA: 2s - loss: 0.1852 - acc: 0.941 - ETA: 2s - loss: 0.1852 - acc: 0.941 - ETA: 1s - loss: 0.1853 - acc: 0.941 - ETA: 1s - loss: 0.1857 - acc: 0.941 - ETA: 1s - loss: 0.1853 - acc: 0.941 - ETA: 1s - loss: 0.1854 - acc: 0.941 - ETA: 0s - loss: 0.1854 - acc: 0.941 - ETA: 0s - loss: 0.1855 - acc: 0.941 - ETA: 0s - loss: 0.1855 - acc: 0.940 - 11s 266us/step - loss: 0.1856 - acc: 0.9410\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1944 - acc: 0.93 - ETA: 10s - loss: 0.1844 - acc: 0.93 - ETA: 10s - loss: 0.1883 - acc: 0.93 - ETA: 10s - loss: 0.1826 - acc: 0.93 - ETA: 10s - loss: 0.1844 - acc: 0.93 - ETA: 10s - loss: 0.1834 - acc: 0.93 - ETA: 10s - loss: 0.1828 - acc: 0.93 - ETA: 10s - loss: 0.1849 - acc: 0.93 - ETA: 10s - loss: 0.1835 - acc: 0.93 - ETA: 9s - loss: 0.1833 - acc: 0.9395 - ETA: 9s - loss: 0.1821 - acc: 0.940 - ETA: 8s - loss: 0.1807 - acc: 0.940 - ETA: 8s - loss: 0.1840 - acc: 0.939 - ETA: 8s - loss: 0.1824 - acc: 0.940 - ETA: 7s - loss: 0.1832 - acc: 0.940 - ETA: 7s - loss: 0.1828 - acc: 0.940 - ETA: 7s - loss: 0.1810 - acc: 0.940 - ETA: 7s - loss: 0.1794 - acc: 0.941 - ETA: 6s - loss: 0.1792 - acc: 0.941 - ETA: 6s - loss: 0.1792 - acc: 0.941 - ETA: 6s - loss: 0.1791 - acc: 0.941 - ETA: 5s - loss: 0.1775 - acc: 0.942 - ETA: 5s - loss: 0.1777 - acc: 0.942 - ETA: 5s - loss: 0.1787 - acc: 0.942 - ETA: 4s - loss: 0.1767 - acc: 0.942 - ETA: 4s - loss: 0.1766 - acc: 0.942 - ETA: 4s - loss: 0.1782 - acc: 0.942 - ETA: 4s - loss: 0.1779 - acc: 0.942 - ETA: 3s - loss: 0.1780 - acc: 0.942 - ETA: 3s - loss: 0.1780 - acc: 0.941 - ETA: 3s - loss: 0.1783 - acc: 0.941 - ETA: 2s - loss: 0.1789 - acc: 0.941 - ETA: 2s - loss: 0.1791 - acc: 0.942 - ETA: 2s - loss: 0.1790 - acc: 0.941 - ETA: 1s - loss: 0.1788 - acc: 0.942 - ETA: 1s - loss: 0.1788 - acc: 0.942 - ETA: 1s - loss: 0.1791 - acc: 0.941 - ETA: 1s - loss: 0.1785 - acc: 0.942 - ETA: 0s - loss: 0.1787 - acc: 0.942 - ETA: 0s - loss: 0.1787 - acc: 0.942 - ETA: 0s - loss: 0.1786 - acc: 0.942 - 12s 278us/step - loss: 0.1786 - acc: 0.9425\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1953 - acc: 0.935 - ETA: 9s - loss: 0.1838 - acc: 0.941 - ETA: 9s - loss: 0.1705 - acc: 0.946 - ETA: 9s - loss: 0.1753 - acc: 0.943 - ETA: 9s - loss: 0.1770 - acc: 0.941 - ETA: 9s - loss: 0.1786 - acc: 0.941 - ETA: 9s - loss: 0.1774 - acc: 0.942 - ETA: 8s - loss: 0.1713 - acc: 0.944 - ETA: 8s - loss: 0.1700 - acc: 0.945 - ETA: 8s - loss: 0.1691 - acc: 0.945 - ETA: 8s - loss: 0.1687 - acc: 0.945 - ETA: 7s - loss: 0.1664 - acc: 0.946 - ETA: 7s - loss: 0.1649 - acc: 0.947 - ETA: 7s - loss: 0.1651 - acc: 0.946 - ETA: 7s - loss: 0.1668 - acc: 0.946 - ETA: 6s - loss: 0.1666 - acc: 0.946 - ETA: 6s - loss: 0.1659 - acc: 0.946 - ETA: 6s - loss: 0.1655 - acc: 0.946 - ETA: 5s - loss: 0.1656 - acc: 0.946 - ETA: 5s - loss: 0.1639 - acc: 0.947 - ETA: 5s - loss: 0.1643 - acc: 0.946 - ETA: 5s - loss: 0.1632 - acc: 0.947 - ETA: 4s - loss: 0.1629 - acc: 0.947 - ETA: 4s - loss: 0.1620 - acc: 0.947 - ETA: 4s - loss: 0.1618 - acc: 0.948 - ETA: 4s - loss: 0.1626 - acc: 0.947 - ETA: 3s - loss: 0.1632 - acc: 0.947 - ETA: 3s - loss: 0.1632 - acc: 0.947 - ETA: 3s - loss: 0.1628 - acc: 0.948 - ETA: 3s - loss: 0.1622 - acc: 0.948 - ETA: 2s - loss: 0.1621 - acc: 0.948 - ETA: 2s - loss: 0.1625 - acc: 0.948 - ETA: 2s - loss: 0.1629 - acc: 0.948 - ETA: 2s - loss: 0.1624 - acc: 0.948 - ETA: 1s - loss: 0.1624 - acc: 0.948 - ETA: 1s - loss: 0.1616 - acc: 0.948 - ETA: 1s - loss: 0.1620 - acc: 0.948 - ETA: 1s - loss: 0.1618 - acc: 0.948 - ETA: 0s - loss: 0.1615 - acc: 0.948 - ETA: 0s - loss: 0.1617 - acc: 0.948 - ETA: 0s - loss: 0.1623 - acc: 0.948 - 11s 254us/step - loss: 0.1619 - acc: 0.9486\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1388 - acc: 0.96 - ETA: 10s - loss: 0.1521 - acc: 0.95 - ETA: 9s - loss: 0.1478 - acc: 0.9530 - ETA: 9s - loss: 0.1585 - acc: 0.951 - ETA: 9s - loss: 0.1535 - acc: 0.953 - ETA: 9s - loss: 0.1560 - acc: 0.951 - ETA: 8s - loss: 0.1539 - acc: 0.952 - ETA: 8s - loss: 0.1553 - acc: 0.951 - ETA: 8s - loss: 0.1541 - acc: 0.951 - ETA: 8s - loss: 0.1543 - acc: 0.951 - ETA: 8s - loss: 0.1545 - acc: 0.951 - ETA: 7s - loss: 0.1555 - acc: 0.951 - ETA: 7s - loss: 0.1558 - acc: 0.952 - ETA: 7s - loss: 0.1548 - acc: 0.952 - ETA: 7s - loss: 0.1538 - acc: 0.952 - ETA: 6s - loss: 0.1551 - acc: 0.951 - ETA: 6s - loss: 0.1540 - acc: 0.951 - ETA: 6s - loss: 0.1532 - acc: 0.951 - ETA: 6s - loss: 0.1542 - acc: 0.951 - ETA: 5s - loss: 0.1537 - acc: 0.951 - ETA: 5s - loss: 0.1530 - acc: 0.952 - ETA: 5s - loss: 0.1549 - acc: 0.951 - ETA: 4s - loss: 0.1542 - acc: 0.951 - ETA: 4s - loss: 0.1523 - acc: 0.952 - ETA: 4s - loss: 0.1528 - acc: 0.952 - ETA: 4s - loss: 0.1522 - acc: 0.952 - ETA: 4s - loss: 0.1513 - acc: 0.952 - ETA: 3s - loss: 0.1513 - acc: 0.952 - ETA: 3s - loss: 0.1526 - acc: 0.951 - ETA: 3s - loss: 0.1539 - acc: 0.950 - ETA: 2s - loss: 0.1535 - acc: 0.950 - ETA: 2s - loss: 0.1542 - acc: 0.950 - ETA: 2s - loss: 0.1543 - acc: 0.950 - ETA: 2s - loss: 0.1550 - acc: 0.950 - ETA: 1s - loss: 0.1540 - acc: 0.950 - ETA: 1s - loss: 0.1539 - acc: 0.950 - ETA: 1s - loss: 0.1548 - acc: 0.950 - ETA: 1s - loss: 0.1552 - acc: 0.950 - ETA: 0s - loss: 0.1555 - acc: 0.949 - ETA: 0s - loss: 0.1552 - acc: 0.950 - ETA: 0s - loss: 0.1545 - acc: 0.950 - 11s 268us/step - loss: 0.1543 - acc: 0.9504\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1174 - acc: 0.96 - ETA: 9s - loss: 0.1192 - acc: 0.9635 - ETA: 9s - loss: 0.1257 - acc: 0.962 - ETA: 9s - loss: 0.1277 - acc: 0.961 - ETA: 9s - loss: 0.1324 - acc: 0.959 - ETA: 8s - loss: 0.1397 - acc: 0.956 - ETA: 8s - loss: 0.1415 - acc: 0.956 - ETA: 8s - loss: 0.1465 - acc: 0.953 - ETA: 8s - loss: 0.1478 - acc: 0.952 - ETA: 8s - loss: 0.1487 - acc: 0.952 - ETA: 7s - loss: 0.1490 - acc: 0.952 - ETA: 7s - loss: 0.1477 - acc: 0.953 - ETA: 7s - loss: 0.1449 - acc: 0.953 - ETA: 7s - loss: 0.1481 - acc: 0.952 - ETA: 7s - loss: 0.1464 - acc: 0.953 - ETA: 6s - loss: 0.1461 - acc: 0.953 - ETA: 6s - loss: 0.1470 - acc: 0.952 - ETA: 6s - loss: 0.1485 - acc: 0.952 - ETA: 6s - loss: 0.1484 - acc: 0.952 - ETA: 5s - loss: 0.1487 - acc: 0.951 - ETA: 5s - loss: 0.1472 - acc: 0.952 - ETA: 5s - loss: 0.1478 - acc: 0.952 - ETA: 4s - loss: 0.1465 - acc: 0.953 - ETA: 4s - loss: 0.1463 - acc: 0.953 - ETA: 4s - loss: 0.1464 - acc: 0.953 - ETA: 4s - loss: 0.1470 - acc: 0.953 - ETA: 3s - loss: 0.1474 - acc: 0.952 - ETA: 3s - loss: 0.1463 - acc: 0.953 - ETA: 3s - loss: 0.1455 - acc: 0.953 - ETA: 3s - loss: 0.1461 - acc: 0.953 - ETA: 2s - loss: 0.1471 - acc: 0.953 - ETA: 2s - loss: 0.1469 - acc: 0.953 - ETA: 2s - loss: 0.1473 - acc: 0.953 - ETA: 2s - loss: 0.1466 - acc: 0.953 - ETA: 1s - loss: 0.1461 - acc: 0.953 - ETA: 1s - loss: 0.1458 - acc: 0.953 - ETA: 1s - loss: 0.1455 - acc: 0.953 - ETA: 1s - loss: 0.1455 - acc: 0.953 - ETA: 0s - loss: 0.1451 - acc: 0.954 - ETA: 0s - loss: 0.1450 - acc: 0.954 - ETA: 0s - loss: 0.1454 - acc: 0.954 - 11s 256us/step - loss: 0.1454 - acc: 0.9538\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1349 - acc: 0.963 - ETA: 10s - loss: 0.1413 - acc: 0.95 - ETA: 9s - loss: 0.1540 - acc: 0.9510 - ETA: 9s - loss: 0.1432 - acc: 0.954 - ETA: 9s - loss: 0.1402 - acc: 0.955 - ETA: 9s - loss: 0.1448 - acc: 0.954 - ETA: 8s - loss: 0.1482 - acc: 0.953 - ETA: 8s - loss: 0.1521 - acc: 0.952 - ETA: 8s - loss: 0.1511 - acc: 0.952 - ETA: 8s - loss: 0.1473 - acc: 0.953 - ETA: 7s - loss: 0.1468 - acc: 0.952 - ETA: 7s - loss: 0.1469 - acc: 0.952 - ETA: 7s - loss: 0.1447 - acc: 0.953 - ETA: 7s - loss: 0.1462 - acc: 0.953 - ETA: 6s - loss: 0.1462 - acc: 0.953 - ETA: 6s - loss: 0.1460 - acc: 0.953 - ETA: 6s - loss: 0.1461 - acc: 0.953 - ETA: 6s - loss: 0.1460 - acc: 0.953 - ETA: 5s - loss: 0.1445 - acc: 0.954 - ETA: 5s - loss: 0.1453 - acc: 0.953 - ETA: 5s - loss: 0.1459 - acc: 0.953 - ETA: 5s - loss: 0.1450 - acc: 0.954 - ETA: 4s - loss: 0.1459 - acc: 0.953 - ETA: 4s - loss: 0.1454 - acc: 0.954 - ETA: 4s - loss: 0.1434 - acc: 0.954 - ETA: 4s - loss: 0.1445 - acc: 0.954 - ETA: 3s - loss: 0.1458 - acc: 0.954 - ETA: 3s - loss: 0.1456 - acc: 0.954 - ETA: 3s - loss: 0.1446 - acc: 0.954 - ETA: 3s - loss: 0.1449 - acc: 0.954 - ETA: 2s - loss: 0.1441 - acc: 0.954 - ETA: 2s - loss: 0.1439 - acc: 0.954 - ETA: 2s - loss: 0.1442 - acc: 0.954 - ETA: 2s - loss: 0.1445 - acc: 0.954 - ETA: 1s - loss: 0.1437 - acc: 0.954 - ETA: 1s - loss: 0.1436 - acc: 0.953 - ETA: 1s - loss: 0.1432 - acc: 0.954 - ETA: 1s - loss: 0.1434 - acc: 0.954 - ETA: 0s - loss: 0.1427 - acc: 0.954 - ETA: 0s - loss: 0.1430 - acc: 0.954 - ETA: 0s - loss: 0.1433 - acc: 0.954 - 11s 253us/step - loss: 0.1427 - acc: 0.9544\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1381 - acc: 0.956 - ETA: 9s - loss: 0.1282 - acc: 0.958 - ETA: 9s - loss: 0.1347 - acc: 0.955 - ETA: 9s - loss: 0.1340 - acc: 0.955 - ETA: 9s - loss: 0.1329 - acc: 0.956 - ETA: 8s - loss: 0.1344 - acc: 0.956 - ETA: 8s - loss: 0.1361 - acc: 0.956 - ETA: 8s - loss: 0.1370 - acc: 0.956 - ETA: 8s - loss: 0.1363 - acc: 0.957 - ETA: 7s - loss: 0.1350 - acc: 0.958 - ETA: 7s - loss: 0.1367 - acc: 0.957 - ETA: 7s - loss: 0.1374 - acc: 0.957 - ETA: 7s - loss: 0.1388 - acc: 0.956 - ETA: 6s - loss: 0.1395 - acc: 0.956 - ETA: 6s - loss: 0.1385 - acc: 0.956 - ETA: 6s - loss: 0.1362 - acc: 0.956 - ETA: 6s - loss: 0.1350 - acc: 0.957 - ETA: 5s - loss: 0.1334 - acc: 0.957 - ETA: 5s - loss: 0.1345 - acc: 0.957 - ETA: 5s - loss: 0.1343 - acc: 0.956 - ETA: 5s - loss: 0.1347 - acc: 0.956 - ETA: 4s - loss: 0.1348 - acc: 0.956 - ETA: 4s - loss: 0.1349 - acc: 0.956 - ETA: 4s - loss: 0.1357 - acc: 0.956 - ETA: 4s - loss: 0.1354 - acc: 0.956 - ETA: 4s - loss: 0.1358 - acc: 0.956 - ETA: 3s - loss: 0.1351 - acc: 0.956 - ETA: 3s - loss: 0.1353 - acc: 0.956 - ETA: 3s - loss: 0.1351 - acc: 0.956 - ETA: 3s - loss: 0.1350 - acc: 0.956 - ETA: 2s - loss: 0.1344 - acc: 0.956 - ETA: 2s - loss: 0.1337 - acc: 0.956 - ETA: 2s - loss: 0.1336 - acc: 0.956 - ETA: 2s - loss: 0.1326 - acc: 0.957 - ETA: 1s - loss: 0.1333 - acc: 0.956 - ETA: 1s - loss: 0.1333 - acc: 0.956 - ETA: 1s - loss: 0.1336 - acc: 0.956 - ETA: 1s - loss: 0.1332 - acc: 0.956 - ETA: 0s - loss: 0.1332 - acc: 0.956 - ETA: 0s - loss: 0.1325 - acc: 0.956 - ETA: 0s - loss: 0.1320 - acc: 0.957 - 11s 250us/step - loss: 0.1325 - acc: 0.9569\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1638 - acc: 0.94 - ETA: 10s - loss: 0.1367 - acc: 0.95 - ETA: 9s - loss: 0.1367 - acc: 0.9587 - ETA: 9s - loss: 0.1278 - acc: 0.960 - ETA: 9s - loss: 0.1293 - acc: 0.959 - ETA: 9s - loss: 0.1278 - acc: 0.960 - ETA: 8s - loss: 0.1238 - acc: 0.961 - ETA: 8s - loss: 0.1284 - acc: 0.961 - ETA: 8s - loss: 0.1275 - acc: 0.961 - ETA: 8s - loss: 0.1280 - acc: 0.961 - ETA: 7s - loss: 0.1295 - acc: 0.959 - ETA: 7s - loss: 0.1296 - acc: 0.960 - ETA: 7s - loss: 0.1278 - acc: 0.960 - ETA: 7s - loss: 0.1281 - acc: 0.960 - ETA: 7s - loss: 0.1281 - acc: 0.960 - ETA: 6s - loss: 0.1284 - acc: 0.960 - ETA: 6s - loss: 0.1284 - acc: 0.960 - ETA: 6s - loss: 0.1285 - acc: 0.960 - ETA: 5s - loss: 0.1272 - acc: 0.960 - ETA: 5s - loss: 0.1284 - acc: 0.959 - ETA: 5s - loss: 0.1283 - acc: 0.959 - ETA: 5s - loss: 0.1287 - acc: 0.959 - ETA: 4s - loss: 0.1297 - acc: 0.959 - ETA: 4s - loss: 0.1290 - acc: 0.959 - ETA: 4s - loss: 0.1298 - acc: 0.959 - ETA: 4s - loss: 0.1296 - acc: 0.959 - ETA: 3s - loss: 0.1299 - acc: 0.959 - ETA: 3s - loss: 0.1296 - acc: 0.959 - ETA: 3s - loss: 0.1289 - acc: 0.959 - ETA: 3s - loss: 0.1284 - acc: 0.959 - ETA: 2s - loss: 0.1285 - acc: 0.959 - ETA: 2s - loss: 0.1280 - acc: 0.959 - ETA: 2s - loss: 0.1293 - acc: 0.959 - ETA: 2s - loss: 0.1294 - acc: 0.959 - ETA: 1s - loss: 0.1295 - acc: 0.959 - ETA: 1s - loss: 0.1298 - acc: 0.959 - ETA: 1s - loss: 0.1292 - acc: 0.959 - ETA: 1s - loss: 0.1298 - acc: 0.959 - ETA: 0s - loss: 0.1295 - acc: 0.959 - ETA: 0s - loss: 0.1300 - acc: 0.958 - ETA: 0s - loss: 0.1301 - acc: 0.958 - 11s 255us/step - loss: 0.1300 - acc: 0.9588\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0990 - acc: 0.969 - ETA: 9s - loss: 0.1038 - acc: 0.964 - ETA: 9s - loss: 0.1071 - acc: 0.963 - ETA: 9s - loss: 0.1025 - acc: 0.966 - ETA: 9s - loss: 0.1018 - acc: 0.968 - ETA: 8s - loss: 0.1092 - acc: 0.966 - ETA: 8s - loss: 0.1124 - acc: 0.964 - ETA: 8s - loss: 0.1114 - acc: 0.964 - ETA: 8s - loss: 0.1114 - acc: 0.964 - ETA: 7s - loss: 0.1112 - acc: 0.964 - ETA: 7s - loss: 0.1105 - acc: 0.964 - ETA: 7s - loss: 0.1136 - acc: 0.963 - ETA: 7s - loss: 0.1136 - acc: 0.963 - ETA: 7s - loss: 0.1153 - acc: 0.963 - ETA: 6s - loss: 0.1171 - acc: 0.962 - ETA: 6s - loss: 0.1188 - acc: 0.961 - ETA: 6s - loss: 0.1213 - acc: 0.961 - ETA: 6s - loss: 0.1217 - acc: 0.961 - ETA: 5s - loss: 0.1218 - acc: 0.961 - ETA: 5s - loss: 0.1213 - acc: 0.961 - ETA: 5s - loss: 0.1221 - acc: 0.960 - ETA: 5s - loss: 0.1229 - acc: 0.961 - ETA: 4s - loss: 0.1226 - acc: 0.961 - ETA: 4s - loss: 0.1226 - acc: 0.961 - ETA: 4s - loss: 0.1225 - acc: 0.961 - ETA: 4s - loss: 0.1230 - acc: 0.960 - ETA: 3s - loss: 0.1231 - acc: 0.960 - ETA: 3s - loss: 0.1222 - acc: 0.961 - ETA: 3s - loss: 0.1222 - acc: 0.961 - ETA: 3s - loss: 0.1227 - acc: 0.960 - ETA: 2s - loss: 0.1233 - acc: 0.960 - ETA: 2s - loss: 0.1231 - acc: 0.960 - ETA: 2s - loss: 0.1234 - acc: 0.960 - ETA: 2s - loss: 0.1237 - acc: 0.960 - ETA: 1s - loss: 0.1232 - acc: 0.960 - ETA: 1s - loss: 0.1231 - acc: 0.960 - ETA: 1s - loss: 0.1228 - acc: 0.960 - ETA: 1s - loss: 0.1235 - acc: 0.960 - ETA: 0s - loss: 0.1235 - acc: 0.960 - ETA: 0s - loss: 0.1240 - acc: 0.960 - ETA: 0s - loss: 0.1244 - acc: 0.960 - 11s 257us/step - loss: 0.1240 - acc: 0.9604\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1326 - acc: 0.95 - ETA: 11s - loss: 0.1212 - acc: 0.96 - ETA: 11s - loss: 0.1245 - acc: 0.96 - ETA: 10s - loss: 0.1246 - acc: 0.96 - ETA: 10s - loss: 0.1212 - acc: 0.96 - ETA: 9s - loss: 0.1233 - acc: 0.9605 - ETA: 9s - loss: 0.1210 - acc: 0.960 - ETA: 9s - loss: 0.1191 - acc: 0.961 - ETA: 8s - loss: 0.1223 - acc: 0.961 - ETA: 8s - loss: 0.1221 - acc: 0.961 - ETA: 8s - loss: 0.1211 - acc: 0.961 - ETA: 7s - loss: 0.1197 - acc: 0.961 - ETA: 7s - loss: 0.1191 - acc: 0.961 - ETA: 7s - loss: 0.1176 - acc: 0.961 - ETA: 6s - loss: 0.1182 - acc: 0.961 - ETA: 6s - loss: 0.1185 - acc: 0.961 - ETA: 6s - loss: 0.1196 - acc: 0.960 - ETA: 6s - loss: 0.1196 - acc: 0.960 - ETA: 5s - loss: 0.1195 - acc: 0.960 - ETA: 5s - loss: 0.1194 - acc: 0.960 - ETA: 5s - loss: 0.1200 - acc: 0.960 - ETA: 5s - loss: 0.1207 - acc: 0.960 - ETA: 4s - loss: 0.1210 - acc: 0.960 - ETA: 4s - loss: 0.1210 - acc: 0.960 - ETA: 4s - loss: 0.1206 - acc: 0.960 - ETA: 4s - loss: 0.1209 - acc: 0.960 - ETA: 3s - loss: 0.1212 - acc: 0.960 - ETA: 3s - loss: 0.1213 - acc: 0.960 - ETA: 3s - loss: 0.1210 - acc: 0.960 - ETA: 3s - loss: 0.1212 - acc: 0.960 - ETA: 3s - loss: 0.1213 - acc: 0.960 - ETA: 2s - loss: 0.1209 - acc: 0.960 - ETA: 2s - loss: 0.1207 - acc: 0.960 - ETA: 2s - loss: 0.1210 - acc: 0.960 - ETA: 1s - loss: 0.1205 - acc: 0.960 - ETA: 1s - loss: 0.1206 - acc: 0.960 - ETA: 1s - loss: 0.1205 - acc: 0.960 - ETA: 1s - loss: 0.1204 - acc: 0.960 - ETA: 0s - loss: 0.1201 - acc: 0.960 - ETA: 0s - loss: 0.1201 - acc: 0.960 - ETA: 0s - loss: 0.1200 - acc: 0.960 - 12s 278us/step - loss: 0.1204 - acc: 0.9610\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1280 - acc: 0.95 - ETA: 9s - loss: 0.1170 - acc: 0.9615 - ETA: 9s - loss: 0.1124 - acc: 0.964 - ETA: 9s - loss: 0.1145 - acc: 0.963 - ETA: 9s - loss: 0.1140 - acc: 0.963 - ETA: 9s - loss: 0.1160 - acc: 0.963 - ETA: 9s - loss: 0.1192 - acc: 0.962 - ETA: 8s - loss: 0.1188 - acc: 0.961 - ETA: 8s - loss: 0.1186 - acc: 0.960 - ETA: 8s - loss: 0.1178 - acc: 0.960 - ETA: 8s - loss: 0.1172 - acc: 0.961 - ETA: 7s - loss: 0.1186 - acc: 0.960 - ETA: 7s - loss: 0.1187 - acc: 0.960 - ETA: 7s - loss: 0.1162 - acc: 0.961 - ETA: 6s - loss: 0.1155 - acc: 0.962 - ETA: 6s - loss: 0.1163 - acc: 0.962 - ETA: 6s - loss: 0.1171 - acc: 0.961 - ETA: 6s - loss: 0.1173 - acc: 0.962 - ETA: 5s - loss: 0.1174 - acc: 0.961 - ETA: 5s - loss: 0.1169 - acc: 0.961 - ETA: 5s - loss: 0.1175 - acc: 0.961 - ETA: 5s - loss: 0.1186 - acc: 0.961 - ETA: 4s - loss: 0.1192 - acc: 0.961 - ETA: 4s - loss: 0.1194 - acc: 0.961 - ETA: 4s - loss: 0.1180 - acc: 0.962 - ETA: 4s - loss: 0.1179 - acc: 0.961 - ETA: 3s - loss: 0.1171 - acc: 0.962 - ETA: 3s - loss: 0.1168 - acc: 0.962 - ETA: 3s - loss: 0.1159 - acc: 0.962 - ETA: 3s - loss: 0.1154 - acc: 0.962 - ETA: 2s - loss: 0.1158 - acc: 0.962 - ETA: 2s - loss: 0.1153 - acc: 0.963 - ETA: 2s - loss: 0.1146 - acc: 0.963 - ETA: 2s - loss: 0.1143 - acc: 0.963 - ETA: 1s - loss: 0.1142 - acc: 0.963 - ETA: 1s - loss: 0.1142 - acc: 0.963 - ETA: 1s - loss: 0.1149 - acc: 0.962 - ETA: 1s - loss: 0.1144 - acc: 0.963 - ETA: 0s - loss: 0.1143 - acc: 0.963 - ETA: 0s - loss: 0.1141 - acc: 0.963 - ETA: 0s - loss: 0.1146 - acc: 0.962 - 11s 260us/step - loss: 0.1144 - acc: 0.9629\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - ETA: 13s - loss: 0.1282 - acc: 0.95 - ETA: 12s - loss: 0.1127 - acc: 0.96 - ETA: 12s - loss: 0.1103 - acc: 0.96 - ETA: 13s - loss: 0.1099 - acc: 0.96 - ETA: 12s - loss: 0.1069 - acc: 0.96 - ETA: 12s - loss: 0.1060 - acc: 0.96 - ETA: 12s - loss: 0.1033 - acc: 0.96 - ETA: 11s - loss: 0.1035 - acc: 0.96 - ETA: 11s - loss: 0.1079 - acc: 0.96 - ETA: 10s - loss: 0.1076 - acc: 0.96 - ETA: 10s - loss: 0.1072 - acc: 0.96 - ETA: 10s - loss: 0.1082 - acc: 0.96 - ETA: 9s - loss: 0.1088 - acc: 0.9662 - ETA: 9s - loss: 0.1102 - acc: 0.965 - ETA: 9s - loss: 0.1113 - acc: 0.965 - ETA: 8s - loss: 0.1124 - acc: 0.965 - ETA: 8s - loss: 0.1129 - acc: 0.964 - ETA: 8s - loss: 0.1126 - acc: 0.964 - ETA: 7s - loss: 0.1129 - acc: 0.964 - ETA: 7s - loss: 0.1115 - acc: 0.965 - ETA: 6s - loss: 0.1119 - acc: 0.965 - ETA: 6s - loss: 0.1112 - acc: 0.965 - ETA: 6s - loss: 0.1109 - acc: 0.965 - ETA: 5s - loss: 0.1099 - acc: 0.965 - ETA: 5s - loss: 0.1097 - acc: 0.965 - ETA: 5s - loss: 0.1102 - acc: 0.965 - ETA: 4s - loss: 0.1110 - acc: 0.964 - ETA: 4s - loss: 0.1109 - acc: 0.964 - ETA: 4s - loss: 0.1107 - acc: 0.964 - ETA: 3s - loss: 0.1108 - acc: 0.964 - ETA: 3s - loss: 0.1110 - acc: 0.964 - ETA: 3s - loss: 0.1112 - acc: 0.964 - ETA: 2s - loss: 0.1116 - acc: 0.964 - ETA: 2s - loss: 0.1129 - acc: 0.964 - ETA: 2s - loss: 0.1128 - acc: 0.964 - ETA: 1s - loss: 0.1133 - acc: 0.964 - ETA: 1s - loss: 0.1128 - acc: 0.964 - ETA: 1s - loss: 0.1137 - acc: 0.964 - ETA: 0s - loss: 0.1131 - acc: 0.964 - ETA: 0s - loss: 0.1128 - acc: 0.964 - ETA: 0s - loss: 0.1124 - acc: 0.964 - 13s 316us/step - loss: 0.1121 - acc: 0.9643\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1481 - acc: 0.95 - ETA: 9s - loss: 0.1238 - acc: 0.9595 - ETA: 9s - loss: 0.1150 - acc: 0.962 - ETA: 9s - loss: 0.1149 - acc: 0.962 - ETA: 9s - loss: 0.1150 - acc: 0.962 - ETA: 9s - loss: 0.1122 - acc: 0.963 - ETA: 8s - loss: 0.1122 - acc: 0.963 - ETA: 8s - loss: 0.1107 - acc: 0.964 - ETA: 8s - loss: 0.1090 - acc: 0.964 - ETA: 8s - loss: 0.1112 - acc: 0.964 - ETA: 7s - loss: 0.1114 - acc: 0.964 - ETA: 7s - loss: 0.1116 - acc: 0.964 - ETA: 7s - loss: 0.1116 - acc: 0.963 - ETA: 7s - loss: 0.1143 - acc: 0.963 - ETA: 6s - loss: 0.1141 - acc: 0.963 - ETA: 6s - loss: 0.1132 - acc: 0.964 - ETA: 6s - loss: 0.1129 - acc: 0.964 - ETA: 6s - loss: 0.1120 - acc: 0.964 - ETA: 5s - loss: 0.1113 - acc: 0.964 - ETA: 5s - loss: 0.1124 - acc: 0.964 - ETA: 5s - loss: 0.1117 - acc: 0.965 - ETA: 5s - loss: 0.1120 - acc: 0.965 - ETA: 4s - loss: 0.1123 - acc: 0.965 - ETA: 4s - loss: 0.1108 - acc: 0.965 - ETA: 4s - loss: 0.1095 - acc: 0.966 - ETA: 4s - loss: 0.1092 - acc: 0.966 - ETA: 3s - loss: 0.1092 - acc: 0.966 - ETA: 3s - loss: 0.1101 - acc: 0.965 - ETA: 3s - loss: 0.1102 - acc: 0.965 - ETA: 3s - loss: 0.1107 - acc: 0.965 - ETA: 2s - loss: 0.1107 - acc: 0.965 - ETA: 2s - loss: 0.1115 - acc: 0.965 - ETA: 2s - loss: 0.1117 - acc: 0.965 - ETA: 2s - loss: 0.1117 - acc: 0.965 - ETA: 1s - loss: 0.1119 - acc: 0.964 - ETA: 1s - loss: 0.1121 - acc: 0.964 - ETA: 1s - loss: 0.1121 - acc: 0.964 - ETA: 1s - loss: 0.1121 - acc: 0.964 - ETA: 0s - loss: 0.1121 - acc: 0.964 - ETA: 0s - loss: 0.1120 - acc: 0.964 - ETA: 0s - loss: 0.1120 - acc: 0.964 - 11s 254us/step - loss: 0.1128 - acc: 0.9641\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1115 - acc: 0.96 - ETA: 10s - loss: 0.1071 - acc: 0.96 - ETA: 9s - loss: 0.1099 - acc: 0.9653 - ETA: 9s - loss: 0.1042 - acc: 0.966 - ETA: 9s - loss: 0.1118 - acc: 0.963 - ETA: 9s - loss: 0.1105 - acc: 0.963 - ETA: 9s - loss: 0.1095 - acc: 0.963 - ETA: 9s - loss: 0.1072 - acc: 0.964 - ETA: 8s - loss: 0.1057 - acc: 0.964 - ETA: 8s - loss: 0.1081 - acc: 0.964 - ETA: 8s - loss: 0.1110 - acc: 0.963 - ETA: 7s - loss: 0.1127 - acc: 0.963 - ETA: 7s - loss: 0.1117 - acc: 0.963 - ETA: 7s - loss: 0.1107 - acc: 0.964 - ETA: 7s - loss: 0.1104 - acc: 0.964 - ETA: 6s - loss: 0.1098 - acc: 0.965 - ETA: 6s - loss: 0.1110 - acc: 0.964 - ETA: 6s - loss: 0.1107 - acc: 0.964 - ETA: 6s - loss: 0.1094 - acc: 0.964 - ETA: 5s - loss: 0.1091 - acc: 0.964 - ETA: 5s - loss: 0.1094 - acc: 0.964 - ETA: 5s - loss: 0.1093 - acc: 0.964 - ETA: 5s - loss: 0.1093 - acc: 0.964 - ETA: 4s - loss: 0.1093 - acc: 0.964 - ETA: 4s - loss: 0.1090 - acc: 0.964 - ETA: 4s - loss: 0.1088 - acc: 0.964 - ETA: 4s - loss: 0.1091 - acc: 0.964 - ETA: 3s - loss: 0.1081 - acc: 0.965 - ETA: 3s - loss: 0.1083 - acc: 0.964 - ETA: 3s - loss: 0.1080 - acc: 0.964 - ETA: 2s - loss: 0.1077 - acc: 0.964 - ETA: 2s - loss: 0.1079 - acc: 0.964 - ETA: 2s - loss: 0.1074 - acc: 0.964 - ETA: 2s - loss: 0.1074 - acc: 0.964 - ETA: 1s - loss: 0.1073 - acc: 0.964 - ETA: 1s - loss: 0.1071 - acc: 0.964 - ETA: 1s - loss: 0.1071 - acc: 0.964 - ETA: 1s - loss: 0.1072 - acc: 0.964 - ETA: 0s - loss: 0.1075 - acc: 0.964 - ETA: 0s - loss: 0.1076 - acc: 0.964 - ETA: 0s - loss: 0.1080 - acc: 0.964 - 11s 273us/step - loss: 0.1081 - acc: 0.9645\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.1122 - acc: 0.96 - ETA: 10s - loss: 0.1078 - acc: 0.96 - ETA: 10s - loss: 0.1137 - acc: 0.96 - ETA: 9s - loss: 0.1161 - acc: 0.9628 - ETA: 9s - loss: 0.1152 - acc: 0.963 - ETA: 9s - loss: 0.1096 - acc: 0.965 - ETA: 8s - loss: 0.1078 - acc: 0.965 - ETA: 8s - loss: 0.1077 - acc: 0.964 - ETA: 8s - loss: 0.1100 - acc: 0.964 - ETA: 8s - loss: 0.1096 - acc: 0.964 - ETA: 7s - loss: 0.1068 - acc: 0.965 - ETA: 7s - loss: 0.1092 - acc: 0.965 - ETA: 7s - loss: 0.1107 - acc: 0.964 - ETA: 7s - loss: 0.1119 - acc: 0.964 - ETA: 6s - loss: 0.1117 - acc: 0.964 - ETA: 6s - loss: 0.1099 - acc: 0.965 - ETA: 6s - loss: 0.1089 - acc: 0.965 - ETA: 6s - loss: 0.1078 - acc: 0.965 - ETA: 5s - loss: 0.1064 - acc: 0.965 - ETA: 5s - loss: 0.1058 - acc: 0.966 - ETA: 5s - loss: 0.1057 - acc: 0.966 - ETA: 5s - loss: 0.1061 - acc: 0.966 - ETA: 5s - loss: 0.1052 - acc: 0.966 - ETA: 4s - loss: 0.1051 - acc: 0.966 - ETA: 4s - loss: 0.1049 - acc: 0.966 - ETA: 4s - loss: 0.1048 - acc: 0.967 - ETA: 3s - loss: 0.1045 - acc: 0.967 - ETA: 3s - loss: 0.1039 - acc: 0.967 - ETA: 3s - loss: 0.1040 - acc: 0.967 - ETA: 3s - loss: 0.1045 - acc: 0.967 - ETA: 2s - loss: 0.1051 - acc: 0.967 - ETA: 2s - loss: 0.1051 - acc: 0.967 - ETA: 2s - loss: 0.1048 - acc: 0.967 - ETA: 2s - loss: 0.1043 - acc: 0.967 - ETA: 1s - loss: 0.1040 - acc: 0.967 - ETA: 1s - loss: 0.1035 - acc: 0.967 - ETA: 1s - loss: 0.1033 - acc: 0.967 - ETA: 1s - loss: 0.1031 - acc: 0.967 - ETA: 0s - loss: 0.1033 - acc: 0.967 - ETA: 0s - loss: 0.1031 - acc: 0.967 - ETA: 0s - loss: 0.1024 - acc: 0.967 - 11s 256us/step - loss: 0.1025 - acc: 0.9678\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1260 - acc: 0.96 - ETA: 9s - loss: 0.1183 - acc: 0.9690 - ETA: 9s - loss: 0.1228 - acc: 0.964 - ETA: 9s - loss: 0.1131 - acc: 0.967 - ETA: 9s - loss: 0.1070 - acc: 0.967 - ETA: 8s - loss: 0.1053 - acc: 0.968 - ETA: 8s - loss: 0.1059 - acc: 0.968 - ETA: 8s - loss: 0.1077 - acc: 0.968 - ETA: 8s - loss: 0.1052 - acc: 0.969 - ETA: 7s - loss: 0.1055 - acc: 0.969 - ETA: 7s - loss: 0.1064 - acc: 0.969 - ETA: 7s - loss: 0.1070 - acc: 0.969 - ETA: 7s - loss: 0.1060 - acc: 0.968 - ETA: 6s - loss: 0.1057 - acc: 0.968 - ETA: 6s - loss: 0.1054 - acc: 0.968 - ETA: 6s - loss: 0.1061 - acc: 0.968 - ETA: 6s - loss: 0.1053 - acc: 0.968 - ETA: 5s - loss: 0.1048 - acc: 0.968 - ETA: 5s - loss: 0.1048 - acc: 0.968 - ETA: 5s - loss: 0.1034 - acc: 0.968 - ETA: 5s - loss: 0.1025 - acc: 0.968 - ETA: 4s - loss: 0.1023 - acc: 0.968 - ETA: 4s - loss: 0.1026 - acc: 0.968 - ETA: 4s - loss: 0.1024 - acc: 0.968 - ETA: 4s - loss: 0.1026 - acc: 0.968 - ETA: 3s - loss: 0.1023 - acc: 0.968 - ETA: 3s - loss: 0.1013 - acc: 0.968 - ETA: 3s - loss: 0.1026 - acc: 0.968 - ETA: 3s - loss: 0.1025 - acc: 0.968 - ETA: 2s - loss: 0.1028 - acc: 0.968 - ETA: 2s - loss: 0.1016 - acc: 0.968 - ETA: 2s - loss: 0.1014 - acc: 0.968 - ETA: 2s - loss: 0.1012 - acc: 0.968 - ETA: 1s - loss: 0.1017 - acc: 0.968 - ETA: 1s - loss: 0.1027 - acc: 0.968 - ETA: 1s - loss: 0.1023 - acc: 0.968 - ETA: 1s - loss: 0.1020 - acc: 0.968 - ETA: 0s - loss: 0.1014 - acc: 0.968 - ETA: 0s - loss: 0.1016 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.968 - ETA: 0s - loss: 0.1009 - acc: 0.968 - 10s 247us/step - loss: 0.1012 - acc: 0.9680\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0974 - acc: 0.968 - ETA: 9s - loss: 0.1007 - acc: 0.966 - ETA: 9s - loss: 0.1030 - acc: 0.965 - ETA: 9s - loss: 0.1021 - acc: 0.965 - ETA: 9s - loss: 0.1039 - acc: 0.965 - ETA: 8s - loss: 0.1027 - acc: 0.965 - ETA: 8s - loss: 0.1034 - acc: 0.965 - ETA: 8s - loss: 0.1039 - acc: 0.965 - ETA: 8s - loss: 0.1030 - acc: 0.964 - ETA: 7s - loss: 0.1019 - acc: 0.965 - ETA: 7s - loss: 0.1019 - acc: 0.965 - ETA: 7s - loss: 0.1029 - acc: 0.965 - ETA: 7s - loss: 0.1024 - acc: 0.966 - ETA: 6s - loss: 0.1024 - acc: 0.966 - ETA: 6s - loss: 0.1017 - acc: 0.966 - ETA: 6s - loss: 0.1026 - acc: 0.966 - ETA: 6s - loss: 0.1029 - acc: 0.966 - ETA: 5s - loss: 0.1025 - acc: 0.966 - ETA: 5s - loss: 0.1020 - acc: 0.967 - ETA: 5s - loss: 0.1023 - acc: 0.967 - ETA: 5s - loss: 0.1017 - acc: 0.967 - ETA: 4s - loss: 0.1002 - acc: 0.968 - ETA: 4s - loss: 0.1008 - acc: 0.967 - ETA: 4s - loss: 0.1015 - acc: 0.967 - ETA: 4s - loss: 0.1011 - acc: 0.967 - ETA: 3s - loss: 0.1015 - acc: 0.967 - ETA: 3s - loss: 0.1028 - acc: 0.966 - ETA: 3s - loss: 0.1031 - acc: 0.966 - ETA: 3s - loss: 0.1041 - acc: 0.966 - ETA: 2s - loss: 0.1033 - acc: 0.966 - ETA: 2s - loss: 0.1033 - acc: 0.966 - ETA: 2s - loss: 0.1033 - acc: 0.966 - ETA: 2s - loss: 0.1029 - acc: 0.966 - ETA: 1s - loss: 0.1015 - acc: 0.967 - ETA: 1s - loss: 0.1014 - acc: 0.967 - ETA: 1s - loss: 0.1011 - acc: 0.967 - ETA: 1s - loss: 0.1018 - acc: 0.967 - ETA: 0s - loss: 0.1024 - acc: 0.967 - ETA: 0s - loss: 0.1023 - acc: 0.967 - ETA: 0s - loss: 0.1025 - acc: 0.967 - ETA: 0s - loss: 0.1022 - acc: 0.967 - 10s 249us/step - loss: 0.1022 - acc: 0.9675\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0813 - acc: 0.974 - ETA: 9s - loss: 0.0937 - acc: 0.970 - ETA: 9s - loss: 0.1051 - acc: 0.968 - ETA: 9s - loss: 0.1054 - acc: 0.967 - ETA: 9s - loss: 0.1012 - acc: 0.968 - ETA: 8s - loss: 0.1020 - acc: 0.969 - ETA: 8s - loss: 0.1000 - acc: 0.969 - ETA: 8s - loss: 0.1017 - acc: 0.969 - ETA: 8s - loss: 0.1028 - acc: 0.968 - ETA: 7s - loss: 0.1010 - acc: 0.969 - ETA: 7s - loss: 0.1014 - acc: 0.969 - ETA: 7s - loss: 0.1011 - acc: 0.969 - ETA: 7s - loss: 0.1003 - acc: 0.969 - ETA: 6s - loss: 0.1005 - acc: 0.969 - ETA: 6s - loss: 0.0999 - acc: 0.969 - ETA: 6s - loss: 0.0990 - acc: 0.969 - ETA: 6s - loss: 0.0987 - acc: 0.969 - ETA: 5s - loss: 0.0981 - acc: 0.969 - ETA: 5s - loss: 0.0981 - acc: 0.969 - ETA: 5s - loss: 0.0981 - acc: 0.969 - ETA: 5s - loss: 0.0990 - acc: 0.968 - ETA: 4s - loss: 0.0995 - acc: 0.968 - ETA: 4s - loss: 0.0988 - acc: 0.968 - ETA: 4s - loss: 0.0996 - acc: 0.968 - ETA: 4s - loss: 0.0997 - acc: 0.968 - ETA: 3s - loss: 0.0993 - acc: 0.968 - ETA: 3s - loss: 0.0995 - acc: 0.968 - ETA: 3s - loss: 0.0983 - acc: 0.969 - ETA: 3s - loss: 0.0989 - acc: 0.968 - ETA: 2s - loss: 0.0998 - acc: 0.968 - ETA: 2s - loss: 0.0996 - acc: 0.968 - ETA: 2s - loss: 0.0990 - acc: 0.968 - ETA: 2s - loss: 0.0989 - acc: 0.968 - ETA: 1s - loss: 0.0995 - acc: 0.968 - ETA: 1s - loss: 0.0989 - acc: 0.968 - ETA: 1s - loss: 0.0990 - acc: 0.968 - ETA: 1s - loss: 0.0995 - acc: 0.968 - ETA: 0s - loss: 0.0986 - acc: 0.968 - ETA: 0s - loss: 0.0984 - acc: 0.968 - ETA: 0s - loss: 0.0984 - acc: 0.968 - ETA: 0s - loss: 0.0986 - acc: 0.968 - 10s 245us/step - loss: 0.0985 - acc: 0.9683\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.1040 - acc: 0.964 - ETA: 9s - loss: 0.0943 - acc: 0.969 - ETA: 9s - loss: 0.0903 - acc: 0.969 - ETA: 9s - loss: 0.0901 - acc: 0.969 - ETA: 8s - loss: 0.0895 - acc: 0.968 - ETA: 8s - loss: 0.0936 - acc: 0.967 - ETA: 8s - loss: 0.0916 - acc: 0.968 - ETA: 8s - loss: 0.0927 - acc: 0.968 - ETA: 8s - loss: 0.0910 - acc: 0.968 - ETA: 7s - loss: 0.0918 - acc: 0.968 - ETA: 7s - loss: 0.0911 - acc: 0.969 - ETA: 7s - loss: 0.0925 - acc: 0.969 - ETA: 7s - loss: 0.0928 - acc: 0.969 - ETA: 6s - loss: 0.0945 - acc: 0.968 - ETA: 6s - loss: 0.0945 - acc: 0.968 - ETA: 6s - loss: 0.0965 - acc: 0.968 - ETA: 6s - loss: 0.0955 - acc: 0.968 - ETA: 5s - loss: 0.0948 - acc: 0.969 - ETA: 5s - loss: 0.0943 - acc: 0.969 - ETA: 5s - loss: 0.0948 - acc: 0.969 - ETA: 5s - loss: 0.0944 - acc: 0.969 - ETA: 4s - loss: 0.0949 - acc: 0.969 - ETA: 4s - loss: 0.0953 - acc: 0.968 - ETA: 4s - loss: 0.0950 - acc: 0.969 - ETA: 4s - loss: 0.0959 - acc: 0.969 - ETA: 4s - loss: 0.0956 - acc: 0.969 - ETA: 3s - loss: 0.0947 - acc: 0.969 - ETA: 3s - loss: 0.0947 - acc: 0.969 - ETA: 3s - loss: 0.0937 - acc: 0.969 - ETA: 3s - loss: 0.0932 - acc: 0.969 - ETA: 2s - loss: 0.0927 - acc: 0.970 - ETA: 2s - loss: 0.0932 - acc: 0.970 - ETA: 2s - loss: 0.0937 - acc: 0.970 - ETA: 2s - loss: 0.0937 - acc: 0.969 - ETA: 1s - loss: 0.0936 - acc: 0.969 - ETA: 1s - loss: 0.0936 - acc: 0.969 - ETA: 1s - loss: 0.0937 - acc: 0.969 - ETA: 1s - loss: 0.0941 - acc: 0.969 - ETA: 0s - loss: 0.0944 - acc: 0.969 - ETA: 0s - loss: 0.0942 - acc: 0.969 - ETA: 0s - loss: 0.0947 - acc: 0.969 - 11s 257us/step - loss: 0.0947 - acc: 0.9690\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - ETA: 10s - loss: 0.0758 - acc: 0.97 - ETA: 10s - loss: 0.0738 - acc: 0.97 - ETA: 9s - loss: 0.0876 - acc: 0.9703 - ETA: 9s - loss: 0.0854 - acc: 0.972 - ETA: 9s - loss: 0.0824 - acc: 0.973 - ETA: 9s - loss: 0.0830 - acc: 0.973 - ETA: 8s - loss: 0.0841 - acc: 0.972 - ETA: 8s - loss: 0.0861 - acc: 0.971 - ETA: 8s - loss: 0.0875 - acc: 0.971 - ETA: 8s - loss: 0.0884 - acc: 0.971 - ETA: 8s - loss: 0.0878 - acc: 0.971 - ETA: 7s - loss: 0.0882 - acc: 0.971 - ETA: 7s - loss: 0.0867 - acc: 0.971 - ETA: 7s - loss: 0.0855 - acc: 0.972 - ETA: 7s - loss: 0.0867 - acc: 0.971 - ETA: 6s - loss: 0.0874 - acc: 0.972 - ETA: 6s - loss: 0.0893 - acc: 0.971 - ETA: 6s - loss: 0.0884 - acc: 0.971 - ETA: 5s - loss: 0.0894 - acc: 0.971 - ETA: 5s - loss: 0.0895 - acc: 0.971 - ETA: 5s - loss: 0.0904 - acc: 0.971 - ETA: 5s - loss: 0.0903 - acc: 0.971 - ETA: 4s - loss: 0.0913 - acc: 0.970 - ETA: 4s - loss: 0.0925 - acc: 0.970 - ETA: 4s - loss: 0.0924 - acc: 0.970 - ETA: 4s - loss: 0.0922 - acc: 0.970 - ETA: 3s - loss: 0.0920 - acc: 0.970 - ETA: 3s - loss: 0.0923 - acc: 0.970 - ETA: 3s - loss: 0.0932 - acc: 0.970 - ETA: 3s - loss: 0.0937 - acc: 0.970 - ETA: 2s - loss: 0.0938 - acc: 0.970 - ETA: 2s - loss: 0.0940 - acc: 0.970 - ETA: 2s - loss: 0.0939 - acc: 0.970 - ETA: 2s - loss: 0.0942 - acc: 0.970 - ETA: 1s - loss: 0.0941 - acc: 0.970 - ETA: 1s - loss: 0.0948 - acc: 0.970 - ETA: 1s - loss: 0.0950 - acc: 0.970 - ETA: 1s - loss: 0.0954 - acc: 0.969 - ETA: 0s - loss: 0.0957 - acc: 0.969 - ETA: 0s - loss: 0.0968 - acc: 0.969 - ETA: 0s - loss: 0.0977 - acc: 0.969 - 11s 262us/step - loss: 0.0974 - acc: 0.9693\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - ETA: 11s - loss: 0.0827 - acc: 0.96 - ETA: 10s - loss: 0.0985 - acc: 0.96 - ETA: 10s - loss: 0.0913 - acc: 0.96 - ETA: 9s - loss: 0.0861 - acc: 0.9707 - ETA: 9s - loss: 0.0857 - acc: 0.971 - ETA: 9s - loss: 0.0892 - acc: 0.970 - ETA: 9s - loss: 0.0915 - acc: 0.970 - ETA: 9s - loss: 0.0893 - acc: 0.971 - ETA: 8s - loss: 0.0905 - acc: 0.970 - ETA: 8s - loss: 0.0917 - acc: 0.970 - ETA: 8s - loss: 0.0922 - acc: 0.970 - ETA: 7s - loss: 0.0915 - acc: 0.970 - ETA: 7s - loss: 0.0950 - acc: 0.969 - ETA: 7s - loss: 0.0954 - acc: 0.969 - ETA: 7s - loss: 0.0952 - acc: 0.969 - ETA: 6s - loss: 0.0946 - acc: 0.969 - ETA: 6s - loss: 0.0948 - acc: 0.969 - ETA: 6s - loss: 0.0951 - acc: 0.969 - ETA: 6s - loss: 0.0949 - acc: 0.969 - ETA: 5s - loss: 0.0945 - acc: 0.969 - ETA: 5s - loss: 0.0945 - acc: 0.969 - ETA: 5s - loss: 0.0942 - acc: 0.970 - ETA: 5s - loss: 0.0931 - acc: 0.970 - ETA: 4s - loss: 0.0929 - acc: 0.970 - ETA: 4s - loss: 0.0934 - acc: 0.970 - ETA: 4s - loss: 0.0947 - acc: 0.970 - ETA: 3s - loss: 0.0944 - acc: 0.970 - ETA: 3s - loss: 0.0953 - acc: 0.970 - ETA: 3s - loss: 0.0949 - acc: 0.969 - ETA: 3s - loss: 0.0947 - acc: 0.969 - ETA: 2s - loss: 0.0943 - acc: 0.970 - ETA: 2s - loss: 0.0941 - acc: 0.970 - ETA: 2s - loss: 0.0936 - acc: 0.970 - ETA: 2s - loss: 0.0933 - acc: 0.970 - ETA: 1s - loss: 0.0925 - acc: 0.970 - ETA: 1s - loss: 0.0926 - acc: 0.970 - ETA: 1s - loss: 0.0935 - acc: 0.970 - ETA: 1s - loss: 0.0933 - acc: 0.969 - ETA: 0s - loss: 0.0931 - acc: 0.970 - ETA: 0s - loss: 0.0930 - acc: 0.969 - ETA: 0s - loss: 0.0933 - acc: 0.969 - 11s 263us/step - loss: 0.0930 - acc: 0.9697\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - ETA: 10s - loss: 0.1123 - acc: 0.96 - ETA: 10s - loss: 0.1076 - acc: 0.96 - ETA: 9s - loss: 0.0970 - acc: 0.9687 - ETA: 9s - loss: 0.0923 - acc: 0.970 - ETA: 9s - loss: 0.0977 - acc: 0.971 - ETA: 9s - loss: 0.0959 - acc: 0.971 - ETA: 8s - loss: 0.0960 - acc: 0.971 - ETA: 8s - loss: 0.0952 - acc: 0.970 - ETA: 8s - loss: 0.0931 - acc: 0.970 - ETA: 8s - loss: 0.0913 - acc: 0.972 - ETA: 7s - loss: 0.0910 - acc: 0.971 - ETA: 7s - loss: 0.0903 - acc: 0.971 - ETA: 7s - loss: 0.0883 - acc: 0.972 - ETA: 7s - loss: 0.0889 - acc: 0.972 - ETA: 6s - loss: 0.0893 - acc: 0.972 - ETA: 6s - loss: 0.0883 - acc: 0.972 - ETA: 6s - loss: 0.0889 - acc: 0.972 - ETA: 6s - loss: 0.0889 - acc: 0.972 - ETA: 5s - loss: 0.0889 - acc: 0.972 - ETA: 5s - loss: 0.0892 - acc: 0.972 - ETA: 5s - loss: 0.0896 - acc: 0.972 - ETA: 5s - loss: 0.0897 - acc: 0.972 - ETA: 4s - loss: 0.0896 - acc: 0.972 - ETA: 4s - loss: 0.0908 - acc: 0.972 - ETA: 4s - loss: 0.0902 - acc: 0.972 - ETA: 4s - loss: 0.0904 - acc: 0.972 - ETA: 3s - loss: 0.0897 - acc: 0.972 - ETA: 3s - loss: 0.0891 - acc: 0.972 - ETA: 3s - loss: 0.0896 - acc: 0.972 - ETA: 3s - loss: 0.0902 - acc: 0.971 - ETA: 2s - loss: 0.0897 - acc: 0.971 - ETA: 2s - loss: 0.0896 - acc: 0.971 - ETA: 2s - loss: 0.0892 - acc: 0.971 - ETA: 1s - loss: 0.0896 - acc: 0.971 - ETA: 1s - loss: 0.0900 - acc: 0.971 - ETA: 1s - loss: 0.0894 - acc: 0.971 - ETA: 1s - loss: 0.0896 - acc: 0.971 - ETA: 0s - loss: 0.0897 - acc: 0.971 - ETA: 0s - loss: 0.0891 - acc: 0.971 - ETA: 0s - loss: 0.0888 - acc: 0.971 - ETA: 0s - loss: 0.0885 - acc: 0.971 - 10s 249us/step - loss: 0.0892 - acc: 0.9717\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - ETA: 9s - loss: 0.0837 - acc: 0.973 - ETA: 9s - loss: 0.0821 - acc: 0.976 - ETA: 9s - loss: 0.0860 - acc: 0.974 - ETA: 9s - loss: 0.0933 - acc: 0.972 - ETA: 8s - loss: 0.0982 - acc: 0.971 - ETA: 8s - loss: 0.0947 - acc: 0.973 - ETA: 8s - loss: 0.0922 - acc: 0.973 - ETA: 8s - loss: 0.0921 - acc: 0.973 - ETA: 7s - loss: 0.0924 - acc: 0.972 - ETA: 7s - loss: 0.0933 - acc: 0.971 - ETA: 7s - loss: 0.0937 - acc: 0.971 - ETA: 7s - loss: 0.0917 - acc: 0.972 - ETA: 6s - loss: 0.0924 - acc: 0.972 - ETA: 6s - loss: 0.0924 - acc: 0.972 - ETA: 6s - loss: 0.0917 - acc: 0.972 - ETA: 6s - loss: 0.0917 - acc: 0.971 - ETA: 6s - loss: 0.0918 - acc: 0.971 - ETA: 5s - loss: 0.0916 - acc: 0.971 - ETA: 5s - loss: 0.0912 - acc: 0.971 - ETA: 5s - loss: 0.0921 - acc: 0.972 - ETA: 5s - loss: 0.0919 - acc: 0.972 - ETA: 4s - loss: 0.0909 - acc: 0.972 - ETA: 4s - loss: 0.0904 - acc: 0.972 - ETA: 4s - loss: 0.0908 - acc: 0.972 - ETA: 4s - loss: 0.0903 - acc: 0.972 - ETA: 3s - loss: 0.0901 - acc: 0.972 - ETA: 3s - loss: 0.0897 - acc: 0.972 - ETA: 3s - loss: 0.0900 - acc: 0.972 - ETA: 3s - loss: 0.0898 - acc: 0.972 - ETA: 2s - loss: 0.0901 - acc: 0.972 - ETA: 2s - loss: 0.0896 - acc: 0.972 - ETA: 2s - loss: 0.0890 - acc: 0.972 - ETA: 2s - loss: 0.0887 - acc: 0.972 - ETA: 1s - loss: 0.0885 - acc: 0.972 - ETA: 1s - loss: 0.0884 - acc: 0.972 - ETA: 1s - loss: 0.0887 - acc: 0.972 - ETA: 1s - loss: 0.0882 - acc: 0.972 - ETA: 0s - loss: 0.0885 - acc: 0.972 - ETA: 0s - loss: 0.0887 - acc: 0.972 - ETA: 0s - loss: 0.0888 - acc: 0.972 - ETA: 0s - loss: 0.0891 - acc: 0.972 - 10s 246us/step - loss: 0.0893 - acc: 0.9720\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=1000, epochs=30)\n",
    "model.save('cnn.h5')\n",
    "# score = model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Label\n",
      "0   0      6\n",
      "1   1      7\n",
      "2   2      2\n",
      "3   3      9\n",
      "4   4      7\n",
      "(10000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_max_y.csv')\n",
    "train_images = torch.load('superPaddedDigitData.pkl')\n",
    "test_images = pd.read_pickle('test_max_x')\n",
    "\n",
    "print(train_labels.head())\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for img in train_images:\n",
    "    digit_pred = [];\n",
    "    if(len(img)==0):\n",
    "        y_pred.append(-1)\n",
    "    else: \n",
    "        for digit in img:\n",
    "            digit = digit/255;\n",
    "    #         print(np.argmax(model.predict(np.reshape(digit, (-1, 28, 28, 1)))))\n",
    "    #         plt.imshow(digit, cmap='gray')\n",
    "    #         plt.show()\n",
    "\n",
    "            digit_pred.append(np.argmax(model.predict(np.reshape(digit, (-1, 28, 28, 1)))))\n",
    "        y_pred.append(np.amax(digit_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_labels['Label'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
