{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SztnT7YsjLie",
        "outputId": "47dc52dd-ecf4-405f-f850-4de4b6fd0f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "import pickle\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "baseFilepath = '/content/drive/My Drive/School/COMP551/Assignment4/'\n",
        "epochNum = 3\n",
        "batchSize = 64"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-spxCOLxWvm",
        "colab_type": "text"
      },
      "source": [
        "# Sample LSTM on Keras' IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-ohdto6i6O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM and CNN with Dropout for sequence classification in the IMDB dataset\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model_imdb = Sequential()\n",
        "model_imdb.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model_imdb.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model_imdb.add(MaxPooling1D(pool_size=2))\n",
        "model_imdb.add(Dropout(0.2))\n",
        "model_imdb.add(LSTM(100))\n",
        "model_imdb.add(Dropout(0.2))\n",
        "model_imdb.add(Dense(1, activation='sigmoid'))\n",
        "model_imdb.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_imdb.summary())\n",
        "\n",
        "# train the model\n",
        "model_imdb.fit(X_train, y_train, epochs=epochNum, batch_size=batchSize)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_imdb.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7rqK3fnxd6f",
        "colab_type": "text"
      },
      "source": [
        "#  Data Preparation (Non-Word2Vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMf7wKYMx3QU",
        "colab_type": "code",
        "outputId": "292e9d99-3c5b-42ac-e96a-0d0ba90d7233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# file load\n",
        "pos_examples = open(baseFilepath + 'rt-polarity.pos', encoding='ISO-8859-1').readlines()\n",
        "neg_examples = open(baseFilepath + 'rt-polarity.neg', encoding='ISO-8859-1').readlines()\n",
        "\n",
        "vocab = collections.defaultdict(int)\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Tokenize all strings\n",
        "token_pos = list(map(lambda ex: word_tokenize(ex.translate(translator)), pos_examples))\n",
        "token_neg = list(map(lambda ex: word_tokenize(ex.translate(translator)), neg_examples))\n",
        "\n",
        "# Get vocabulary size and max sentence length\n",
        "max_sentence_length = 0\n",
        "for ex_p, ex_n in zip(token_pos, token_neg):\n",
        "    max_sentence_length = max(max_sentence_length, len(ex_p), len(ex_n))\n",
        "    for word in ex_p:\n",
        "        vocab[word] += 1\n",
        "    for word in ex_n:\n",
        "        vocab[word] += 1\n",
        "        \n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary size: \" + str(vocab_size))\n",
        "print(\"Max sentence length: \" + str(max_sentence_length))\n",
        "\n",
        "# One-Hot encode and pad.\n",
        "encoded_pos = [preprocessing.text.one_hot(ex, vocab_size) for ex in pos_examples]\n",
        "padded_pos = preprocessing.sequence.pad_sequences(encoded_pos, maxlen=max_sentence_length, padding='post')\n",
        "encoded_neg = [preprocessing.text.one_hot(ex, vocab_size) for ex in neg_examples]\n",
        "padded_neg = preprocessing.sequence.pad_sequences(encoded_neg, maxlen=max_sentence_length, padding='post')\n",
        "\n",
        "# Create train-test split\n",
        "X = np.concatenate((padded_pos, padded_neg))\n",
        "y = np.concatenate((np.ones(padded_pos.shape[0]), np.zeros(padded_neg.shape[0])))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 20490\n",
            "Max sentence length: 51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKiXkpe2xkun",
        "colab_type": "text"
      },
      "source": [
        "# Barebones LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4iH36xhyBob",
        "colab_type": "code",
        "outputId": "2f45a360-dcd0-4e49-dfe7-a9885767fa18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 128\n",
        "model_reproduce_base = Sequential()\n",
        "model_reproduce_base.add(Embedding(vocab_size, embedding_vector_length, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_base.add(LSTM(100))\n",
        "model_reproduce_base.add(Dense(1, activation='relu'))\n",
        "model_reproduce_base.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_base.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_base.fit(X_train, y_train, epochs=epochNum, batch_size=batchSize)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_base.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# 75.34"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 51, 128)           2622720   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100)               91600     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 2,714,421\n",
            "Trainable params: 2,714,421\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "8529/8529 [==============================] - 15s 2ms/step - loss: 0.7126 - acc: 0.5003\n",
            "Epoch 2/3\n",
            "8529/8529 [==============================] - 12s 1ms/step - loss: 0.6949 - acc: 0.5056\n",
            "Epoch 3/3\n",
            "8529/8529 [==============================] - 12s 1ms/step - loss: 0.6071 - acc: 0.6691\n",
            "Accuracy: 74.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGhK7EORxuLl",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjUNUq2_yOQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 128\n",
        "model_reproduce_mid = Sequential()\n",
        "model_reproduce_mid.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_mid.add(Dropout(0.2))\n",
        "model_reproduce_mid.add(LSTM(100))\n",
        "model_reproduce_mid.add(Dropout(0.2))\n",
        "model_reproduce_mid.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_mid.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_mid.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_mid.fit(X_train, y_train, epochs=epochNum, batch_size=batchSize)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_mid.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i041PCYSxorp",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with CNN and Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-VxMd6xkJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 128\n",
        "model_reproduce_complex = Sequential()\n",
        "model_reproduce_complex.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_complex.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model_reproduce_complex.add(MaxPooling1D(pool_size=2))\n",
        "model_reproduce_complex.add(Dropout(0.2))\n",
        "model_reproduce_complex.add(LSTM(100))\n",
        "model_reproduce_complex.add(Dropout(0.2))\n",
        "model_reproduce_complex.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_complex.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_complex.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_complex.fit(X_train, y_train, epochs=epochNum, batch_size=batchSize)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_complex.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cDMhzK-Yp9o",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation (Word2Vec Embeddings)\n",
        "Note that *mr.p* pickle file was generated in OriginalCode.ipynb from the same data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUAM0Em3L7vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
        "    \"\"\"\n",
        "    Transforms sentence into a list of indices. Pad with zeroes.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    pad = filter_h - 1\n",
        "    for i in range(pad):\n",
        "        x.append(0)\n",
        "    words = sent.split()\n",
        "    for word in words:\n",
        "        if word in word_idx_map:\n",
        "            x.append(word_idx_map[word])\n",
        "    while len(x) < max_l+2*pad:\n",
        "        x.append(0)\n",
        "    return x\n",
        "\n",
        "def make_idx_data_cv(revs, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
        "    \"\"\"\n",
        "    Transforms sentences into a 2-d matrix.\n",
        "    \"\"\"\n",
        "    train, test = [], []\n",
        "    for rev in revs:\n",
        "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h)   \n",
        "        sent.append(rev[\"y\"])\n",
        "        if rev[\"split\"]==cv:            \n",
        "            test.append(sent)        \n",
        "        else:\n",
        "            train.append(sent)\n",
        "    train = np.array(train,dtype=\"int\")\n",
        "    test = np.array(test,dtype=\"int\")\n",
        "    return [train, test]\n",
        "\n",
        "x = pickle.load(open(baseFilepath + \"OriginalCode/mr.p\",\"rb\"), encoding='latin1')\n",
        "revs, W, W2, word_idx_map, vocabNew = x[0], x[1], x[2], x[3], x[4]\n",
        "kFoldData = []\n",
        "for i in range(10):\n",
        "  kFoldData.append(make_idx_data_cv(revs, word_idx_map, i, max_l=56,k=300, filter_h=5))\n",
        "\n",
        "# for simplification, only use the first fold\n",
        "X_train, y_train, X_test, y_test = [], [], [], []\n",
        "for i in range(kFoldData[0][0].shape[0]):\n",
        "  X_train.append(kFoldData[0][0][i][:-1])\n",
        "  y_train.append(kFoldData[0][0][i][-1])\n",
        "for i in range(kFoldData[0][1].shape[0]):\n",
        "  X_test.append(kFoldData[0][1][i][:-1])\n",
        "  y_test.append(kFoldData[0][1][i][-1])\n",
        "X_train = np.asarray(X_train)\n",
        "y_train = np.asarray(y_train)\n",
        "X_test = np.asarray(X_test)\n",
        "y_test = np.asarray(y_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH3ro9DHZS6o",
        "colab_type": "text"
      },
      "source": [
        "# Barebones LSTM (on Word2Vec-encoded data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MstMgS9RTt9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9e8060f-d120-403b-9cc7-45d35b5f8c2d"
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = X_train.shape[1]\n",
        "model_reproduce_base = Sequential()\n",
        "model_reproduce_base.add(Embedding(len(vocabNew), embedding_vector_length, input_length=64, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_base.add(LSTM(100))\n",
        "model_reproduce_base.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_base.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_base.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_base.fit(X_train, y_train, epochs=25, batch_size=50)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_base.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9591, 64) (9591,) (1071, 64) (1071,)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 64, 64)            1200896   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               66000     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,266,997\n",
            "Trainable params: 1,266,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n",
            "9591/9591 [==============================] - 17s 2ms/step - loss: 0.6938 - acc: 0.5008\n",
            "Epoch 2/25\n",
            "9591/9591 [==============================] - 17s 2ms/step - loss: 0.6932 - acc: 0.5006\n",
            "Epoch 3/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6932 - acc: 0.5050\n",
            "Epoch 4/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6933 - acc: 0.4974\n",
            "Epoch 5/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6934 - acc: 0.5086\n",
            "Epoch 6/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6932 - acc: 0.5001\n",
            "Epoch 7/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6933 - acc: 0.4991\n",
            "Epoch 8/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6932 - acc: 0.5058\n",
            "Epoch 9/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6928 - acc: 0.5037\n",
            "Epoch 10/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6940 - acc: 0.5008\n",
            "Epoch 11/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6944 - acc: 0.4977\n",
            "Epoch 12/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6939 - acc: 0.4979\n",
            "Epoch 13/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6934 - acc: 0.5039\n",
            "Epoch 14/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6934 - acc: 0.5095\n",
            "Epoch 15/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6939 - acc: 0.4923\n",
            "Epoch 16/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6936 - acc: 0.4995\n",
            "Epoch 17/25\n",
            "9591/9591 [==============================] - 17s 2ms/step - loss: 0.6937 - acc: 0.4911\n",
            "Epoch 18/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6933 - acc: 0.5108\n",
            "Epoch 19/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6937 - acc: 0.4933\n",
            "Epoch 20/25\n",
            "9591/9591 [==============================] - 17s 2ms/step - loss: 0.6935 - acc: 0.4975\n",
            "Epoch 21/25\n",
            "9591/9591 [==============================] - 17s 2ms/step - loss: 0.6933 - acc: 0.4983\n",
            "Epoch 22/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6935 - acc: 0.4999\n",
            "Epoch 23/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6936 - acc: 0.4896\n",
            "Epoch 24/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6934 - acc: 0.4972\n",
            "Epoch 25/25\n",
            "9591/9591 [==============================] - 16s 2ms/step - loss: 0.6935 - acc: 0.4939\n",
            "Accuracy: 48.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1GbscCBZr5H",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with CNN and Dropout (on Word2Vec data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTzrbjiaZrHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "b7e767bc-1f02-4ca4-8ed5-a3757a31c426"
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = X_train.shape[1]\n",
        "model_reproduce_complex = Sequential()\n",
        "model_reproduce_complex.add(Embedding(len(vocabNew), embedding_vector_length, input_length=64, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_complex.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model_reproduce_complex.add(MaxPooling1D(pool_size=2))\n",
        "model_reproduce_complex.add(Dropout(0.2))\n",
        "model_reproduce_complex.add(LSTM(100))\n",
        "model_reproduce_complex.add(Dropout(0.2))\n",
        "model_reproduce_complex.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_complex.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_complex.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_complex.fit(X_train, y_train, epochs=3, batch_size=batchSize)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_complex.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# 75.91%"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 64, 64)            1200896   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 64, 32)            6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 32, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,260,373\n",
            "Trainable params: 1,260,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "9591/9591 [==============================] - 8s 832us/step - loss: 0.6934 - acc: 0.5019\n",
            "Epoch 2/5\n",
            "9591/9591 [==============================] - 7s 694us/step - loss: 0.6026 - acc: 0.6399\n",
            "Epoch 3/5\n",
            "9591/9591 [==============================] - 7s 691us/step - loss: 0.2962 - acc: 0.8810\n",
            "Epoch 4/5\n",
            "9591/9591 [==============================] - 7s 698us/step - loss: 0.1220 - acc: 0.9613\n",
            "Epoch 5/5\n",
            "9591/9591 [==============================] - 7s 681us/step - loss: 0.0534 - acc: 0.9851\n",
            "Accuracy: 75.54%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}