{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SztnT7YsjLie",
        "outputId": "186ab0bc-155c-4399-eacd-f5e47697d061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "baseFilepath = '/content/drive/My Drive/School/COMP551/Assignment4/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-spxCOLxWvm",
        "colab_type": "text"
      },
      "source": [
        "# Sample LSTM on Keras' IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-ohdto6i6O1",
        "colab_type": "code",
        "outputId": "7117a63e-afe7-477e-d3f6-6cd0dcf4e128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "# LSTM and CNN with Dropout for sequence classification in the IMDB dataset\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model_imdb = Sequential()\n",
        "model_imdb.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model_imdb.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model_imdb.add(MaxPooling1D(pool_size=2))\n",
        "model_imdb.add(Dropout(0.2))\n",
        "model_imdb.add(LSTM(100))\n",
        "model_imdb.add(Dropout(0.2))\n",
        "model_imdb.add(Dense(1, activation='sigmoid'))\n",
        "model_imdb.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_imdb.summary())\n",
        "\n",
        "# train the model\n",
        "model_imdb.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_imdb.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 216,405\n",
            "Trainable params: 216,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 141s 6ms/step - loss: 0.4405 - acc: 0.7710\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 139s 6ms/step - loss: 0.2457 - acc: 0.9017\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 139s 6ms/step - loss: 0.2024 - acc: 0.9220\n",
            "Accuracy: 88.13%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7rqK3fnxd6f",
        "colab_type": "text"
      },
      "source": [
        "#  Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMf7wKYMx3QU",
        "colab_type": "code",
        "outputId": "cbd167d1-98e0-48a3-dfb1-6c587e11d348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# LSTM and CNN with Dropout for sequence classification of the project dataset\n",
        "pos_examples = open(baseFilepath + 'rt-polarity.pos', encoding='ISO-8859-1').readlines()\n",
        "neg_examples = open(baseFilepath + 'rt-polarity.neg', encoding='ISO-8859-1').readlines()\n",
        "\n",
        "vocab = collections.defaultdict(int)\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Tokenize all strings\n",
        "token_pos = list(map(lambda ex: word_tokenize(ex.translate(translator)), pos_examples))\n",
        "token_neg = list(map(lambda ex: word_tokenize(ex.translate(translator)), neg_examples))\n",
        "\n",
        "# Get vocabulary size and max sentence length\n",
        "max_sentence_length = 0\n",
        "for ex_p, ex_n in zip(token_pos, token_neg):\n",
        "    max_sentence_length = max(max_sentence_length, len(ex_p), len(ex_n))\n",
        "    for word in ex_p:\n",
        "        vocab[word] += 1\n",
        "    for word in ex_n:\n",
        "        vocab[word] += 1\n",
        "        \n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary size: \" + str(vocab_size))\n",
        "print(\"Max sentence length: \" + str(max_sentence_length))\n",
        "\n",
        "# One-Hot encode and pad.\n",
        "encoded_pos = [preprocessing.text.one_hot(ex, vocab_size) for ex in pos_examples]\n",
        "padded_pos = preprocessing.sequence.pad_sequences(encoded_pos, maxlen=max_sentence_length, padding='post')\n",
        "encoded_neg = [preprocessing.text.one_hot(ex, vocab_size) for ex in neg_examples]\n",
        "padded_neg = preprocessing.sequence.pad_sequences(encoded_neg, maxlen=max_sentence_length, padding='post')\n",
        "\n",
        "# Create train-test split\n",
        "X = np.concatenate((padded_pos, padded_neg))\n",
        "y = np.concatenate((np.ones(padded_pos.shape[0]), np.zeros(padded_neg.shape[0])))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 20490\n",
            "Max sentence length: 51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKiXkpe2xkun",
        "colab_type": "text"
      },
      "source": [
        "# Barebones LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4iH36xhyBob",
        "colab_type": "code",
        "outputId": "9a417c2b-f0c1-471c-934d-d8baec5cbdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 128\n",
        "model_reproduce_base = Sequential()\n",
        "model_reproduce_base.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_base.add(LSTM(100))\n",
        "model_reproduce_base.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_base.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_base.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_base.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_base.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 51, 32)            655680    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 708,981\n",
            "Trainable params: 708,981\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "8529/8529 [==============================] - 11s 1ms/step - loss: 0.6934 - acc: 0.5037\n",
            "Epoch 2/3\n",
            "8529/8529 [==============================] - 10s 1ms/step - loss: 0.6933 - acc: 0.4998\n",
            "Epoch 3/3\n",
            "8529/8529 [==============================] - 10s 1ms/step - loss: 0.6604 - acc: 0.5996\n",
            "Accuracy: 69.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGhK7EORxuLl",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjUNUq2_yOQL",
        "colab_type": "code",
        "outputId": "22a7ad66-d8e4-4ef5-f9f0-c0d34453a585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 128\n",
        "model_reproduce_mid = Sequential()\n",
        "model_reproduce_mid.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_mid.add(Dropout(0.2))\n",
        "model_reproduce_mid.add(LSTM(100))\n",
        "model_reproduce_mid.add(Dropout(0.2))\n",
        "model_reproduce_mid.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_mid.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_mid.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_mid.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_mid.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 51, 32)            655680    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 51, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 708,981\n",
            "Trainable params: 708,981\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "8529/8529 [==============================] - 11s 1ms/step - loss: 0.6935 - acc: 0.4957\n",
            "Epoch 2/3\n",
            "8529/8529 [==============================] - 10s 1ms/step - loss: 0.6933 - acc: 0.5094\n",
            "Epoch 3/3\n",
            "8529/8529 [==============================] - 10s 1ms/step - loss: 0.6934 - acc: 0.4992\n",
            "Accuracy: 49.84%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i041PCYSxorp",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with CNN and Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-VxMd6xkJv",
        "colab_type": "code",
        "outputId": "f4be0c21-582b-4443-d606-74f43c53d13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 128\n",
        "model_reproduce_complex = Sequential()\n",
        "model_reproduce_complex.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_sentence_length, embeddings_initializer='random_uniform'))\n",
        "model_reproduce_complex.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model_reproduce_complex.add(MaxPooling1D(pool_size=2))\n",
        "model_reproduce_complex.add(Dropout(0.2))\n",
        "model_reproduce_complex.add(LSTM(100))\n",
        "model_reproduce_complex.add(Dropout(0.2))\n",
        "model_reproduce_complex.add(Dense(1, activation='sigmoid'))\n",
        "model_reproduce_complex.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_reproduce_complex.summary())\n",
        "\n",
        "# train the model\n",
        "model_reproduce_complex.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model_reproduce_complex.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 51, 32)            655680    \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 51, 32)            3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 25, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 25, 32)            0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 712,085\n",
            "Trainable params: 712,085\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "8529/8529 [==============================] - 7s 836us/step - loss: 0.6934 - acc: 0.4954\n",
            "Epoch 2/3\n",
            "8529/8529 [==============================] - 5s 595us/step - loss: 0.5770 - acc: 0.6918\n",
            "Epoch 3/3\n",
            "8529/8529 [==============================] - 5s 602us/step - loss: 0.3119 - acc: 0.8808\n",
            "Accuracy: 74.54%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}