{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Units and Embedding  ðŸš®\n",
    "    - Word2Vec & GloVe embeddings ðŸ”¥\n",
    "    - LSTM and GRU units for learning ðŸ§ \n",
    "    - Attention modules provided ðŸ‘€ \n",
    "    - proprocess (doesn't help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Data management\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#generic nlp\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "\n",
    "# custom embeddings\n",
    "import torchtext.vocab as vocab\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "# Deep learning models\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Math and plots\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proprocessing\n",
    "REGEX = True\n",
    "SPELL_CHECK = False\n",
    "STOP_WORDS = False\n",
    "LEMMATIZE = False\n",
    "STEM = False\n",
    "\n",
    "# embedding type\n",
    "WORD2VEC = False\n",
    "GLOVE = True\n",
    "TOKENIZED = False   # ngram space and back -> unorder and remove words out of set\n",
    "WORD_CUTOFF = 0\n",
    "MAX_CUTOFF = 1.0\n",
    "\n",
    "# model params\n",
    "HIDDEN_SIZE = 128\n",
    "SENTENCE_SIZE = 200\n",
    "CELL_TYPE ='LSTM'\n",
    "NUM_LAYERS = 1\n",
    "WEIGHT_FREEZE = False\n",
    "\n",
    "# EMB hyper params\n",
    "GLOVE_SIZE = 300\n",
    "WINDOW_SIZE = 3\n",
    "VEC_SIZE = 300\n",
    "MIN_COUNT = WORD_CUTOFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data = './Data/reddit_train.csv'\n",
    "test_path = './Data/reddit_test.csv'\n",
    "\n",
    "# gloabal labels\n",
    "labels = ['hockey', 'nba', 'leagueoflegends', 'soccer', \\\n",
    "          'funny', 'movies', 'anime', 'Overwatch', 'trees', \\\n",
    "          'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', \\\n",
    "          'conspiracy', 'worldnews', 'wow', 'europe', 'canada', \\\n",
    "          'Music', 'baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "comment_data = pd.read_csv(train_data)\n",
    "\n",
    "#load\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "\n",
    "if REGEX:\n",
    "    #clean\n",
    "    comment_data['prep'] = comment_data['comments'].str.replace(r'[^\\w\\s]+', ' ')\n",
    "    comment_data['prep'] = comment_data['prep'].str.lower()\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace(r'\\s+', \" \")\n",
    "    comment_data['prep'] = comment_data['prep'].str.replace(\" +\", \" \")\n",
    "    \n",
    "    #clean   \n",
    "    test_data['prep'] = test_data['comments'].str.replace(r'[^\\w\\s]+', ' ')\n",
    "    test_data['prep'] = test_data['prep'].str.lower()\n",
    "    test_data['prep'] = test_data['prep'].str.replace('(\\d+)', ' num ')\n",
    "    test_data['prep'] = test_data['prep'].str.replace(r'http(?<=http).*', ' wasurl ')\n",
    "    test_data['prep'] = test_data['prep'].str.replace(r'\\s+', \" \")\n",
    "    test_data['prep'] = test_data['prep'].str.replace(\" +\", \" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honestly buffalo is the correct answer i remember people somewhat joking that buffalo s mantra for starting goalies was win a game get traded i think edmonton s front office was a travesty for the better part of num years but buffalo s systematic destruction of the term competitive was much more responsible for the change to the draft lottery \n"
     ]
    }
   ],
   "source": [
    "print(comment_data['prep'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPELL_CHECK:\n",
    "    #spellcheck\n",
    "    spell = SpellChecker(distance=1)\n",
    "    def spellcheck_col(row):\n",
    "        row = tt.tokenize(row)\n",
    "        return \" \".join([spell.correction(word) for word in row])\n",
    "\n",
    "    comment_data['prep'] = comment_data.prep.apply(spellcheck_col)\n",
    "    test_data['prep'] = test_data.prep.apply(spellcheck_col)\n",
    "\n",
    "if STOP_WORDS:\n",
    "    # stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    comment_data['prep'] = comment_data.prep.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    test_data['prep'] = test_data.prep.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "if LEMMATIZE:\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_col(row):\n",
    "        row = tt.tokenize(row)\n",
    "        return \" \".join([lemmatizer.lemmatize(w) for w in row])\n",
    "\n",
    "    comment_data['prep'] = comment_data.prep.apply(lemmatize_col)\n",
    "    test_data['prep'] = test_data.prep.apply(lemmatize_col)\n",
    "\n",
    "if STEM:\n",
    "    #stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_col(row):\n",
    "        row = tt.tokenize(row)\n",
    "        return \" \".join([stemmer.stem(word) for word in row])\n",
    "\n",
    "    comment_data['prep'] = comment_data.prep.apply(stem_col)\n",
    "    test_data['prep'] = test_data.prep.apply(stem_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honestly buffalo is the correct answer i remember people somewhat joking that buffalo s mantra for starting goalies was win a game get traded i think edmonton s front office was a travesty for the better part of num years but buffalo s systematic destruction of the term competitive was much more responsible for the change to the draft lottery \n"
     ]
    }
   ],
   "source": [
    "print(comment_data['prep'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65000,)\n",
      "(5000,)\n",
      "(65000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "#split data\n",
    "clean_data = comment_data['prep'].to_numpy()\n",
    "clean_labels = comment_data['subreddits'].to_numpy()\n",
    "\n",
    "training_data = clean_data[:65000]\n",
    "testing_data = clean_data[65000:]\n",
    "training_labels = clean_labels[:65000]\n",
    "testing_labels = clean_labels[65000:]\n",
    "\n",
    "print(training_data.shape)\n",
    "print(testing_data.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=0, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fdca05a7ba8>>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and remove min words on \"training set\"\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, ngram_range=(1,1), min_df=WORD_CUTOFF, max_df=MAX_CUTOFF)\n",
    "tfidf_vectorizer.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65000, 61247)\n",
      "(5000, 61247)\n"
     ]
    }
   ],
   "source": [
    "# filter out bad words for both sets\n",
    "training_vec = tfidf_vectorizer.transform(training_data)\n",
    "testing_vec = tfidf_vectorizer.transform(testing_data)\n",
    "\n",
    "print(training_vec.shape)\n",
    "print(testing_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 61247\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# revert back to word space - unordered tokens\n",
    "training_set = tfidf_vectorizer.inverse_transform(training_vec) \n",
    "testing_set = tfidf_vectorizer.inverse_transform(testing_vec)\n",
    "\n",
    "#generate vocab space\n",
    "items = []\n",
    "for comment in training_set:\n",
    "    items.extend(comment)\n",
    "    \n",
    "set_vocab = set(items)\n",
    "word_to_ix = {word: i+1 for i, word in enumerate(set_vocab)} # leave zero index for null\n",
    "\n",
    "print(\"vocab size:\", len(set_vocab))\n",
    "\n",
    "matrix_len = len(set_vocab)+1 # allow set buffer\n",
    "weights_matrix = np.zeros((matrix_len, VEC_SIZE), dtype = np.float32) #\n",
    "\n",
    "\n",
    "# w2v auto encoding\n",
    "if WORD2VEC:\n",
    "    \n",
    "    sentences = []\n",
    "    for comment in training_data:\n",
    "        sentences.append(comment.split())\n",
    "    \n",
    "    word2Vec = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        size=VEC_SIZE, \n",
    "        window=WINDOW_SIZE, \n",
    "        min_count=MIN_COUNT, \n",
    "        iter=3\n",
    "    )\n",
    "    \n",
    "    badwords = []    \n",
    "    for i, word in enumerate(set_vocab):\n",
    "        try: \n",
    "            weights_matrix[i+1] = word2Vec.wv[word]\n",
    "        except KeyError:\n",
    "            badwords.append(word)\n",
    "            weights_matrix[i+1] = np.random.normal(scale=0.6, size=(VEC_SIZE, ))\n",
    "    \n",
    "    print(len(badwords))\n",
    "\n",
    "# glove preembeddings\n",
    "if GLOVE:\n",
    "    \n",
    "    glove = vocab.GloVe(name='6B', dim=GLOVE_SIZE)\n",
    "    \n",
    "    badwords = []\n",
    "    weights_matrix = np.zeros((matrix_len, GLOVE_SIZE), dtype=np.float32)    \n",
    "    for i, word in enumerate(set_vocab):\n",
    "        try: \n",
    "            weights_matrix[i+1] = glove[word]\n",
    "        except KeyError:\n",
    "            badwords.append(word)\n",
    "            weights_matrix[i+1] = np.random.normal(scale=0.6, size=(GLOVE_SIZE, ))\n",
    "    \n",
    "    print(len(badwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65000, 2)\n",
      "(5000, 2)\n",
      "['honestly buffalo is the correct answer i remember people somewhat joking that buffalo s mantra for starting goalies was win a game get traded i think edmonton s front office was a travesty for the better part of num years but buffalo s systematic destruction of the term competitive was much more responsible for the change to the draft lottery '\n",
      " 'hockey']\n",
      "['lol hot or cold your choice isnt that what coffee drinkers do now run filter hot water over ground up coffee beans to get the flavor and caffine out then drink it my way just gets rid of the flavor and most of the water and still gives you the caffine fix buzz '\n",
      " 'canada']\n"
     ]
    }
   ],
   "source": [
    "# create objects for loaders:\n",
    "training_obj = []\n",
    "testing_obj = []\n",
    "\n",
    "if TOKENIZED:\n",
    "    \n",
    "    ## Untokenize to proper format\n",
    "    \n",
    "    print(\"tokenizing space!\")\n",
    "    for i in range(len(training_set)):\n",
    "        elem = TreebankWordDetokenizer().detokenize(training_set[i])\n",
    "        training_obj.append((elem, training_labels[i]))\n",
    "\n",
    "    \n",
    "    for i in range(len(testing_set)):\n",
    "        elem = TreebankWordDetokenizer().detokenize(testing_set[i])\n",
    "        testing_obj.append((elem, testing_labels[i]))\n",
    "\n",
    "else:\n",
    "    for i in range(training_data.shape[0]):\n",
    "        training_obj.append((training_data[i], training_labels[i]))\n",
    "\n",
    "    \n",
    "    for i in range(testing_data.shape[0]):\n",
    "        testing_obj.append((testing_data[i], testing_labels[i]))\n",
    "\n",
    "training_obj = np.asarray(training_obj)\n",
    "testing_obj = np.asarray(testing_obj)\n",
    "\n",
    "print(training_obj.shape)\n",
    "print(testing_obj.shape)\n",
    "\n",
    "print(training_obj[0])\n",
    "print(testing_obj[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disregard words not in corpus\n",
    "def sentenceToVec(sentence, lut, mLength=SENTENCE_SIZE):\n",
    "    vec = np.zeros((mLength), dtype=np.int)\n",
    "    words = sentence.split()\n",
    "    jdx = 0\n",
    "    for idx, word in enumerate(words):\n",
    "        if idx >= mLength:\n",
    "            break\n",
    "        try:\n",
    "            vec[jdx] = lut[word]\n",
    "            jdx += 1\n",
    "        except:\n",
    "            continue\n",
    "            # null word out of set - keep 0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader fir train and test\n",
    "class CommentData(Dataset):\n",
    "    \n",
    "    def __init__(self, comments, mapping, labels=labels):\n",
    "        self.frames = comments\n",
    "        self.labels = labels\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        element, label = self.frames[idx]\n",
    "        enc_element = sentenceToVec(element, lut=self.mapping)\n",
    "        enc_label = self.encode(label)\n",
    "        return (enc_element, enc_label)\n",
    "    \n",
    "    # one-hot encoding on element fetch\n",
    "    def encode(self, label):\n",
    "        location = self.labels.index(label)\n",
    "        return location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM/GRU - tanh+softmax mixed attention\n",
    "#https://arxiv.org/pdf/1703.03130.pdf\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_layers, output_size, hidden_dim, weights, freeze, cell_style='LSTM'):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.style = cell_style\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.penalty = 0\n",
    "        \n",
    "        vocab_size, embedding_dim = weights_matrix.shape\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.load_state_dict({'weight': torch.Tensor(weights)})\n",
    "        \n",
    "        if freeze:\n",
    "            self.word_embeddings.requires_grad = False\n",
    "        \n",
    "        if cell_style is 'LSTM':\n",
    "            self.memory = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=num_layers)\n",
    "        elif cell_style is 'GRU':\n",
    "            self.memory = nn.GRU(embedding_dim, hidden_dim,  bidirectional=True, num_layers=num_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.mixer1 = nn.Linear(2*hidden_dim, 124, bias=False)\n",
    "        self.mixer2 = nn.Linear(124, 12, bias=False)\n",
    "        self.linear = nn.Linear(12*2*hidden_dim, 1024)\n",
    "        self.output = nn.Linear(1024, output_size)\n",
    "\n",
    "    def selfAttention(self, mem_out):\n",
    "        attention = self.mixer2(torch.tanh(self.mixer1(mem_out)))\n",
    "        attention = attention.permute(0, 2, 1)\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        \n",
    "        return attention\n",
    "\n",
    "    def forward(self, sentences):\n",
    "\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        embeds = embeds.permute(1, 0, 2)\n",
    "        \n",
    "        if self.style is 'LSTM':\n",
    "            state = (Variable(torch.zeros(2*self.num_layers, sentences.size(0), self.hidden_dim).cuda()),\n",
    "                     Variable(torch.zeros(2*self.num_layers, sentences.size(0), self.hidden_dim).cuda()))\n",
    "            output, (hidden_state, cell_state) = self.memory(embeds, state)\n",
    "        \n",
    "        elif self.style is 'GRU':\n",
    "            state = Variable(torch.zeros(2*self.num_layers, sentences.size(0), self.hidden_dim).cuda())\n",
    "            output, hidden_state = self.memory(embeds, state)\n",
    "    \n",
    "        output = output.permute(1, 0, 2)\n",
    "        attention = self.selfAttention(output)\n",
    "        hidden_state = torch.bmm(attention, output)\n",
    "\n",
    "        scores = self.output(self.dropout(self.linear(hidden_state.view(-1, hidden_state.size()[1]*hidden_state.size()[2]))))\n",
    "        \n",
    "        # penalty factor\n",
    "        AAT = torch.bmm(attention, attention.transpose(1,2)).cuda()\n",
    "        I = (torch.eye(12).unsqueeze(0).repeat(sentences.size(0), 1, 1)).cuda()\n",
    "        self.penalty = torch.norm(AAT - I) / sentences.size(0)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (word_embeddings): Embedding(61248, 300)\n",
      "  (memory): LSTM(300, 128, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (mixer1): Linear(in_features=256, out_features=124, bias=False)\n",
      "  (mixer2): Linear(in_features=124, out_features=12, bias=False)\n",
      "  (linear): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "  (output): Linear(in_features=1024, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SelfAttention(NUM_LAYERS, 20, HIDDEN_SIZE, weights_matrix, freeze=WEIGHT_FREEZE, cell_style=CELL_TYPE).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 3.33828\n",
      "[1,   100] loss: 2.95235\n",
      "[1,   150] loss: 2.69866\n",
      "[1,   200] loss: 2.60977\n",
      "[1,   250] loss: 2.48757\n",
      "Epoch: 1 Training Acc: 0.3329384615384615\n",
      "Epoch: 1 Validation Acc: 0.4444\n",
      "[2,    50] loss: 2.39592\n",
      "[2,   100] loss: 2.33490\n",
      "[2,   150] loss: 2.28044\n",
      "[2,   200] loss: 2.27036\n",
      "[2,   250] loss: 2.23764\n",
      "Epoch: 2 Training Acc: 0.5012307692307693\n",
      "Epoch: 2 Validation Acc: 0.505\n",
      "[3,    50] loss: 2.13482\n",
      "[3,   100] loss: 2.12726\n",
      "[3,   150] loss: 2.12061\n",
      "[3,   200] loss: 2.10979\n",
      "[3,   250] loss: 2.08470\n",
      "Epoch: 3 Training Acc: 0.5570923076923077\n",
      "Epoch: 3 Validation Acc: 0.5294\n",
      "[4,    50] loss: 1.97546\n",
      "[4,   100] loss: 1.98080\n",
      "[4,   150] loss: 1.97545\n",
      "[4,   200] loss: 1.97389\n",
      "[4,   250] loss: 1.95740\n",
      "Epoch: 4 Training Acc: 0.5986923076923077\n",
      "Epoch: 4 Validation Acc: 0.5432\n",
      "[5,    50] loss: 1.84248\n",
      "[5,   100] loss: 1.85083\n",
      "[5,   150] loss: 1.83000\n",
      "[5,   200] loss: 1.85713\n",
      "[5,   250] loss: 1.86363\n",
      "Epoch: 5 Training Acc: 0.6326615384615385\n",
      "Epoch: 5 Validation Acc: 0.5506\n",
      "[6,    50] loss: 1.75035\n",
      "[6,   100] loss: 1.71614\n",
      "[6,   150] loss: 1.71313\n",
      "[6,   200] loss: 1.72959\n",
      "[6,   250] loss: 1.73975\n",
      "Epoch: 6 Training Acc: 0.6662769230769231\n",
      "Epoch: 6 Validation Acc: 0.5554\n",
      "[7,    50] loss: 1.60730\n",
      "[7,   100] loss: 1.59870\n",
      "[7,   150] loss: 1.62542\n",
      "[7,   200] loss: 1.61113\n",
      "[7,   250] loss: 1.61078\n",
      "Epoch: 7 Training Acc: 0.7017076923076923\n",
      "Epoch: 7 Validation Acc: 0.5538\n",
      "[8,    50] loss: 1.46245\n",
      "[8,   100] loss: 1.49883\n",
      "[8,   150] loss: 1.47888\n",
      "[8,   200] loss: 1.50529\n",
      "[8,   250] loss: 1.50150\n",
      "Epoch: 8 Training Acc: 0.7377076923076923\n",
      "Epoch: 8 Validation Acc: 0.5486\n",
      "[9,    50] loss: 1.36816\n",
      "[9,   100] loss: 1.36084\n",
      "[9,   150] loss: 1.36089\n",
      "[9,   200] loss: 1.36962\n",
      "[9,   250] loss: 1.37673\n",
      "Epoch: 9 Training Acc: 0.7740769230769231\n",
      "Epoch: 9 Validation Acc: 0.5454\n",
      "[10,    50] loss: 1.23071\n",
      "[10,   100] loss: 1.23585\n",
      "[10,   150] loss: 1.25106\n",
      "[10,   200] loss: 1.26973\n",
      "[10,   250] loss: 1.27465\n",
      "Epoch: 10 Training Acc: 0.8064769230769231\n",
      "Epoch: 10 Validation Acc: 0.5404\n",
      "[11,    50] loss: 1.12618\n",
      "[11,   100] loss: 1.13690\n",
      "[11,   150] loss: 1.14412\n",
      "[11,   200] loss: 1.16235\n",
      "[11,   250] loss: 1.15613\n",
      "Epoch: 11 Training Acc: 0.8403076923076923\n",
      "Epoch: 11 Validation Acc: 0.538\n",
      "[12,    50] loss: 1.02914\n",
      "[12,   100] loss: 1.03367\n",
      "[12,   150] loss: 1.05577\n",
      "[12,   200] loss: 1.06397\n",
      "[12,   250] loss: 1.06133\n",
      "Epoch: 12 Training Acc: 0.8683384615384615\n",
      "Epoch: 12 Validation Acc: 0.5354\n",
      "[13,    50] loss: 0.94328\n",
      "[13,   100] loss: 0.95861\n",
      "[13,   150] loss: 0.95975\n",
      "[13,   200] loss: 0.98020\n",
      "[13,   250] loss: 0.98882\n",
      "Epoch: 13 Training Acc: 0.8923076923076924\n",
      "Epoch: 13 Validation Acc: 0.5298\n",
      "[14,    50] loss: 0.88144\n",
      "[14,   100] loss: 0.88541\n",
      "[14,   150] loss: 0.89637\n",
      "[14,   200] loss: 0.90847\n",
      "[14,   250] loss: 0.91009\n",
      "Epoch: 14 Training Acc: 0.9113230769230769\n",
      "Epoch: 14 Validation Acc: 0.5224\n",
      "[15,    50] loss: 0.81827\n",
      "[15,   100] loss: 0.82267\n",
      "[15,   150] loss: 0.84300\n",
      "[15,   200] loss: 0.84987\n",
      "[15,   250] loss: 0.85699\n",
      "Epoch: 15 Training Acc: 0.9284461538461538\n",
      "Epoch: 15 Validation Acc: 0.5184\n",
      "[16,    50] loss: 0.76959\n",
      "[16,   100] loss: 0.78131\n",
      "[16,   150] loss: 0.78629\n",
      "[16,   200] loss: 0.79754\n",
      "[16,   250] loss: 0.80708\n",
      "Epoch: 16 Training Acc: 0.9415230769230769\n",
      "Epoch: 16 Validation Acc: 0.5186\n",
      "[17,    50] loss: 0.72510\n",
      "[17,   100] loss: 0.74544\n",
      "[17,   150] loss: 0.75540\n",
      "[17,   200] loss: 0.75451\n",
      "[17,   250] loss: 0.76156\n",
      "Epoch: 17 Training Acc: 0.9520615384615385\n",
      "Epoch: 17 Validation Acc: 0.5186\n",
      "[18,    50] loss: 0.69963\n",
      "[18,   100] loss: 0.70799\n",
      "[18,   150] loss: 0.71742\n",
      "[18,   200] loss: 0.72471\n",
      "[18,   250] loss: 0.72987\n",
      "Epoch: 18 Training Acc: 0.9611538461538461\n",
      "Epoch: 18 Validation Acc: 0.5056\n",
      "[19,    50] loss: 0.67322\n",
      "[19,   100] loss: 0.68647\n",
      "[19,   150] loss: 0.68029\n",
      "[19,   200] loss: 0.68832\n",
      "[19,   250] loss: 0.69324\n",
      "Epoch: 19 Training Acc: 0.9675230769230769\n",
      "Epoch: 19 Validation Acc: 0.5106\n",
      "[20,    50] loss: 0.64718\n",
      "[20,   100] loss: 0.65425\n",
      "[20,   150] loss: 0.65638\n",
      "[20,   200] loss: 0.66181\n",
      "[20,   250] loss: 0.65730\n",
      "Epoch: 20 Training Acc: 0.9733538461538461\n",
      "Epoch: 20 Validation Acc: 0.5034\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "train = CommentData(training_obj, word_to_ix)\n",
    "val = CommentData(testing_obj, word_to_ix)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=256, num_workers=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=256, num_workers=8)\n",
    "# train cycle here\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    for i, (comment, target) in enumerate(train_loader):\n",
    "        # tensor to device\n",
    "        comment = comment.to(device=device, dtype=torch.int64)\n",
    "        target = target.to(device=device, dtype=torch.int64)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = net(comment)\n",
    "        error = loss(output, target) + 3*net.penalty\n",
    "        error.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 0.3)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += error.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        # Get predictions\n",
    "        preds = F.softmax(output, dim=1)\n",
    "        preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct_preds = torch.eq(preds_cls, target)\n",
    "        correct += torch.sum(correct_preds).detach().cpu().item()\n",
    "        total += len(correct_preds)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print(\"Epoch:\", epoch+1,\"Training Acc:\",train_acc)\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    for i, (comment, target) in enumerate(val_loader):\n",
    "\n",
    "        comment = comment.to(device=device, dtype=torch.int64)\n",
    "        target = target.to(device=device, dtype=torch.int64)\n",
    "        output = net(comment)\n",
    "\n",
    "        # Get predictions\n",
    "        preds = F.softmax(output, dim=1)\n",
    "        preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "        # Count number of correct predictions\n",
    "        correct_preds = torch.eq(preds_cls, target)\n",
    "        correct += torch.sum(correct_preds).detach().cpu().item()\n",
    "        total += len(correct_preds)\n",
    "\n",
    "    valid_acc = correct / total\n",
    "    print(\"Epoch:\", epoch+1,\"Validation Acc:\",valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FOLD TESTING SCRIPT\n",
    "    - Adapted for flags best model - will not run if settings are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        #np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(subset):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for x,y in subset:\n",
    "        data.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(data)\n",
    "    \n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leverages pandas for fast csv load but operates in numpy\n",
    "class kFold():\n",
    "    def __init__(self, data, numFolds=5):\n",
    "        self.data = data\n",
    "        self.numFolds = numFolds\n",
    "        self.splits = []\n",
    "        \n",
    "    def generateSplits(self):\n",
    "        np.random.shuffle(self.data)\n",
    "        \n",
    "        folds = []\n",
    "        splitPoint = self.data.shape[0] // (self.numFolds)  #breakpoint index jump\n",
    "        \n",
    "        for i in range(self.numFolds - 1):\n",
    "            folds.append(self.data[i*splitPoint:(i+1)*splitPoint, :])\n",
    "            \n",
    "        folds.append(self.data[(i+1)*splitPoint:,:]) #get extra points in last batch\n",
    "        \n",
    "        # create split permutations 80/10/10\n",
    "        foldDivisor = len(folds[0]) // 2\n",
    "        for i in range(self.numFolds):\n",
    "            train = []\n",
    "            for k in range(self.numFolds):\n",
    "                if i == k:\n",
    "                    validation = folds[i][:foldDivisor] \n",
    "                    test = folds[i][foldDivisor:] \n",
    "                else:\n",
    "                    train.append(folds[k])\n",
    "            \n",
    "            train = np.vstack(train) # adapt dims\n",
    "            self.splits.append((train, validation, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen tuples for splitting\n",
    "kfold_data = []\n",
    "for idx in range(clean_data.shape[0]):\n",
    "    item = (clean_data[idx], clean_labels[idx])\n",
    "    kfold_data.append(item)\n",
    "kfold_data = np.asarray(kfold_data)\n",
    "\n",
    "commentFolds = kFold(kfold_data)\n",
    "commentFolds.generateSplits()\n",
    "splits = commentFolds.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Acc: 0.32353571428571426\n",
      "Epoch: 1 Validation Acc: 0.45071428571428573\n",
      "Epoch: 1 Testing Acc: 0.44985714285714284\n",
      "Epoch: 2 Training Acc: 0.4996607142857143\n",
      "Epoch: 2 Validation Acc: 0.5064285714285715\n",
      "Epoch: 2 Testing Acc: 0.4977142857142857\n",
      "Epoch: 3 Training Acc: 0.555875\n",
      "Epoch: 3 Validation Acc: 0.5331428571428571\n",
      "Epoch: 3 Testing Acc: 0.527\n",
      "Epoch: 4 Training Acc: 0.597625\n",
      "Epoch: 4 Validation Acc: 0.5441428571428572\n",
      "Epoch: 4 Testing Acc: 0.5358571428571428\n",
      "Epoch: 5 Training Acc: 0.6340178571428572\n",
      "Epoch: 5 Validation Acc: 0.5554285714285714\n",
      "Epoch: 5 Testing Acc: 0.5415714285714286\n",
      "Epoch: 6 Training Acc: 0.6719107142857143\n",
      "Epoch: 6 Validation Acc: 0.5481428571428572\n",
      "Epoch: 6 Testing Acc: 0.5417142857142857\n",
      "Epoch: 7 Training Acc: 0.7092142857142857\n",
      "Epoch: 7 Validation Acc: 0.5511428571428572\n",
      "Epoch: 7 Testing Acc: 0.545\n",
      "Epoch: 8 Training Acc: 0.7489107142857143\n",
      "Epoch: 8 Validation Acc: 0.5455714285714286\n",
      "Epoch: 8 Testing Acc: 0.5422857142857143\n",
      "Epoch: 9 Training Acc: 0.7873392857142857\n",
      "Epoch: 9 Validation Acc: 0.5424285714285715\n",
      "Epoch: 9 Testing Acc: 0.5352857142857143\n",
      "Epoch: 10 Training Acc: 0.8251785714285714\n",
      "Epoch: 10 Validation Acc: 0.5315714285714286\n",
      "Epoch: 10 Testing Acc: 0.5284285714285715\n",
      "Epoch: 1 Training Acc: 0.32442857142857146\n",
      "Epoch: 1 Validation Acc: 0.44014285714285717\n",
      "Epoch: 1 Testing Acc: 0.43785714285714283\n",
      "Epoch: 2 Training Acc: 0.4991607142857143\n",
      "Epoch: 2 Validation Acc: 0.49914285714285717\n",
      "Epoch: 2 Testing Acc: 0.5022857142857143\n",
      "Epoch: 3 Training Acc: 0.5547321428571429\n",
      "Epoch: 3 Validation Acc: 0.5264285714285715\n",
      "Epoch: 3 Testing Acc: 0.5311428571428571\n",
      "Epoch: 4 Training Acc: 0.5958392857142857\n",
      "Epoch: 4 Validation Acc: 0.5371428571428571\n",
      "Epoch: 4 Testing Acc: 0.5461428571428572\n",
      "Epoch: 5 Training Acc: 0.63125\n",
      "Epoch: 5 Validation Acc: 0.5394285714285715\n",
      "Epoch: 5 Testing Acc: 0.5428571428571428\n",
      "Epoch: 6 Training Acc: 0.667375\n",
      "Epoch: 6 Validation Acc: 0.5465714285714286\n",
      "Epoch: 6 Testing Acc: 0.5458571428571428\n",
      "Epoch: 7 Training Acc: 0.7037321428571428\n",
      "Epoch: 7 Validation Acc: 0.547\n",
      "Epoch: 7 Testing Acc: 0.5457142857142857\n",
      "Epoch: 8 Training Acc: 0.7450714285714286\n",
      "Epoch: 8 Validation Acc: 0.5435714285714286\n",
      "Epoch: 8 Testing Acc: 0.5404285714285715\n",
      "Epoch: 9 Training Acc: 0.7855892857142858\n",
      "Epoch: 9 Validation Acc: 0.5358571428571428\n",
      "Epoch: 9 Testing Acc: 0.5352857142857143\n",
      "Epoch: 10 Training Acc: 0.8224821428571428\n",
      "Epoch: 10 Validation Acc: 0.5288571428571428\n",
      "Epoch: 10 Testing Acc: 0.5291428571428571\n",
      "Epoch: 1 Training Acc: 0.33219642857142856\n",
      "Epoch: 1 Validation Acc: 0.451\n",
      "Epoch: 1 Testing Acc: 0.443\n",
      "Epoch: 2 Training Acc: 0.49989285714285714\n",
      "Epoch: 2 Validation Acc: 0.515\n",
      "Epoch: 2 Testing Acc: 0.5102857142857142\n",
      "Epoch: 3 Training Acc: 0.5554642857142857\n",
      "Epoch: 3 Validation Acc: 0.5435714285714286\n",
      "Epoch: 3 Testing Acc: 0.5321428571428571\n",
      "Epoch: 4 Training Acc: 0.5935178571428571\n",
      "Epoch: 4 Validation Acc: 0.5487142857142857\n",
      "Epoch: 4 Testing Acc: 0.5385714285714286\n",
      "Epoch: 5 Training Acc: 0.6315714285714286\n",
      "Epoch: 5 Validation Acc: 0.5531428571428572\n",
      "Epoch: 5 Testing Acc: 0.5418571428571428\n",
      "Epoch: 6 Training Acc: 0.6669464285714286\n",
      "Epoch: 6 Validation Acc: 0.5581428571428572\n",
      "Epoch: 6 Testing Acc: 0.5495714285714286\n",
      "Epoch: 7 Training Acc: 0.7041785714285714\n",
      "Epoch: 7 Validation Acc: 0.5598571428571428\n",
      "Epoch: 7 Testing Acc: 0.542\n",
      "Epoch: 8 Training Acc: 0.74675\n",
      "Epoch: 8 Validation Acc: 0.55\n",
      "Epoch: 8 Testing Acc: 0.5321428571428571\n",
      "Epoch: 9 Training Acc: 0.7847321428571429\n",
      "Epoch: 9 Validation Acc: 0.5464285714285714\n",
      "Epoch: 9 Testing Acc: 0.5262857142857142\n",
      "Epoch: 10 Training Acc: 0.8210892857142857\n",
      "Epoch: 10 Validation Acc: 0.5375714285714286\n",
      "Epoch: 10 Testing Acc: 0.5202857142857142\n",
      "Epoch: 1 Training Acc: 0.3219107142857143\n",
      "Epoch: 1 Validation Acc: 0.44014285714285717\n",
      "Epoch: 1 Testing Acc: 0.4552857142857143\n",
      "Epoch: 2 Training Acc: 0.5001785714285715\n",
      "Epoch: 2 Validation Acc: 0.49142857142857144\n",
      "Epoch: 2 Testing Acc: 0.5068571428571429\n",
      "Epoch: 3 Training Acc: 0.5569107142857143\n",
      "Epoch: 3 Validation Acc: 0.5182857142857142\n",
      "Epoch: 3 Testing Acc: 0.5301428571428571\n",
      "Epoch: 4 Training Acc: 0.5975178571428571\n",
      "Epoch: 4 Validation Acc: 0.5321428571428571\n",
      "Epoch: 4 Testing Acc: 0.5508571428571428\n",
      "Epoch: 5 Training Acc: 0.6324642857142857\n",
      "Epoch: 5 Validation Acc: 0.529\n",
      "Epoch: 5 Testing Acc: 0.5495714285714286\n",
      "Epoch: 6 Training Acc: 0.67\n",
      "Epoch: 6 Validation Acc: 0.5334285714285715\n",
      "Epoch: 6 Testing Acc: 0.5582857142857143\n",
      "Epoch: 7 Training Acc: 0.7075\n",
      "Epoch: 7 Validation Acc: 0.5308571428571428\n",
      "Epoch: 7 Testing Acc: 0.556\n",
      "Epoch: 8 Training Acc: 0.7482142857142857\n",
      "Epoch: 8 Validation Acc: 0.5252857142857142\n",
      "Epoch: 8 Testing Acc: 0.5498571428571428\n",
      "Epoch: 9 Training Acc: 0.7849821428571429\n",
      "Epoch: 9 Validation Acc: 0.5144285714285715\n",
      "Epoch: 9 Testing Acc: 0.5304285714285715\n",
      "Epoch: 10 Training Acc: 0.8210178571428571\n",
      "Epoch: 10 Validation Acc: 0.5157142857142857\n",
      "Epoch: 10 Testing Acc: 0.5307142857142857\n",
      "Epoch: 1 Training Acc: 0.3352857142857143\n",
      "Epoch: 1 Validation Acc: 0.4472857142857143\n",
      "Epoch: 1 Testing Acc: 0.43642857142857144\n",
      "Epoch: 2 Training Acc: 0.5054642857142857\n",
      "Epoch: 2 Validation Acc: 0.505\n",
      "Epoch: 2 Testing Acc: 0.51\n",
      "Epoch: 3 Training Acc: 0.5602142857142857\n",
      "Epoch: 3 Validation Acc: 0.5308571428571428\n",
      "Epoch: 3 Testing Acc: 0.5262857142857142\n",
      "Epoch: 4 Training Acc: 0.5978571428571429\n",
      "Epoch: 4 Validation Acc: 0.5421428571428571\n",
      "Epoch: 4 Testing Acc: 0.5407142857142857\n",
      "Epoch: 5 Training Acc: 0.6341607142857143\n",
      "Epoch: 5 Validation Acc: 0.5534285714285714\n",
      "Epoch: 5 Testing Acc: 0.5432857142857143\n",
      "Epoch: 6 Training Acc: 0.6709285714285714\n",
      "Epoch: 6 Validation Acc: 0.5557142857142857\n",
      "Epoch: 6 Testing Acc: 0.5408571428571428\n",
      "Epoch: 7 Training Acc: 0.7074821428571428\n",
      "Epoch: 7 Validation Acc: 0.5504285714285714\n",
      "Epoch: 7 Testing Acc: 0.5444285714285715\n",
      "Epoch: 8 Training Acc: 0.7479285714285714\n",
      "Epoch: 8 Validation Acc: 0.544\n",
      "Epoch: 8 Testing Acc: 0.54\n",
      "Epoch: 9 Training Acc: 0.7882678571428572\n",
      "Epoch: 9 Validation Acc: 0.5422857142857143\n",
      "Epoch: 9 Testing Acc: 0.5395714285714286\n",
      "Epoch: 10 Training Acc: 0.8239285714285715\n",
      "Epoch: 10 Validation Acc: 0.5292857142857142\n",
      "Epoch: 10 Testing Acc: 0.5305714285714286\n",
      "Finished Cross Val\n"
     ]
    }
   ],
   "source": [
    "best_vals = [0,0,0,0,0]\n",
    "best_test = [0,0,0,0,0]\n",
    "\n",
    "for s, split in enumerate(splits):\n",
    "    # unpacl split\n",
    "    train, val, test = split\n",
    "    \n",
    "    # unpack training data for space generation\n",
    "    training_data, _ = unpack(train)\n",
    "    \n",
    "    # tokenize and remove min words on \"training set\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=tt.tokenize, ngram_range=(1,1), min_df=WORD_CUTOFF)\n",
    "    training_vec = tfidf_vectorizer.fit_transform(training_data)\n",
    "    \n",
    "    # revert back to word space - unordered tokens\n",
    "    training_set = tfidf_vectorizer.inverse_transform(training_vec) \n",
    "\n",
    "    #generate vocab space\n",
    "    items = []\n",
    "    for comment in training_set:\n",
    "        items.extend(comment)\n",
    "\n",
    "    set_vocab = set(items)\n",
    "    word_to_ix = {word: i+1 for i, word in enumerate(set_vocab)} # leave zero index for null\n",
    "\n",
    "    # embed space\n",
    "    matrix_len = len(set_vocab)+1 # allow set buffer\n",
    "    glove = vocab.GloVe(name='6B', dim=GLOVE_SIZE)\n",
    "    weights_matrix = np.zeros((matrix_len, GLOVE_SIZE), dtype=np.float32)    \n",
    "    \n",
    "    for i, word in enumerate(set_vocab):\n",
    "        try: \n",
    "            weights_matrix[i+1] = glove[word]\n",
    "        except KeyError:\n",
    "            weights_matrix[i+1] = np.random.normal(scale=0.6, size=(GLOVE_SIZE, ))\n",
    "\n",
    "    # clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # make model and optims\n",
    "    net = SelfAttention(NUM_LAYERS, 20, HIDDEN_SIZE, weights_matrix, freeze=WEIGHT_FREEZE, cell_style=CELL_TYPE).to(device)\n",
    "    loss = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=3e-4)\n",
    "    \n",
    "    epochs = 10\n",
    "    \n",
    "    # genrate loading obj    \n",
    "    trainer = CommentData(train, word_to_ix)\n",
    "    validator = CommentData(val, word_to_ix)\n",
    "    tester = CommentData(test, word_to_ix)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainer, batch_size=256, num_workers=8, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(validator, batch_size=128, num_workers=8)\n",
    "    test_loader = torch.utils.data.DataLoader(tester, batch_size=128, num_workers=8)\n",
    "    \n",
    "    best_score = 0\n",
    "    \n",
    "    # train cycle\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "        flag = False\n",
    "\n",
    "        for i, (comment, target) in enumerate(train_loader):\n",
    "            # tensor to device\n",
    "            comment = comment.to(device=device, dtype=torch.int64)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = net(comment) + 3*net.penalty\n",
    "            error = loss(output, target)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += error.item()\n",
    "            if i % 5000 == 4999:    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 5000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).detach().cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Training Acc:\",train_acc)\n",
    "\n",
    "        net.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "\n",
    "        for i, (comment, target) in enumerate(val_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.int64)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).detach().cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        valid_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Validation Acc:\",valid_acc)\n",
    "        \n",
    "        #save best model\n",
    "        if(valid_acc > best_score):\n",
    "            flag = True\n",
    "            best_score = valid_acc\n",
    "            best_vals[s] = valid_acc\n",
    "            torch.save(net.state_dict(), './net'+str(s)+'.pth.tar')\n",
    "        \n",
    "        net.eval()\n",
    "        correct = 0.\n",
    "        total = 0.\n",
    "\n",
    "        for i, (comment, target) in enumerate(test_loader):\n",
    "\n",
    "            comment = comment.to(device=device, dtype=torch.int64)\n",
    "            target = target.to(device=device, dtype=torch.int64)\n",
    "            output = net(comment)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            preds_cls = preds.argmax(dim=1)\n",
    "\n",
    "            # Count number of correct predictions\n",
    "            correct_preds = torch.eq(preds_cls, target)\n",
    "            correct += torch.sum(correct_preds).detach().cpu().item()\n",
    "            total += len(correct_preds)\n",
    "\n",
    "        test_acc = correct / total\n",
    "        print(\"Epoch:\", epoch+1,\"Testing Acc:\",test_acc)\n",
    "        if flag:\n",
    "            best_test[s] = test_acc\n",
    "\n",
    "print(\"Finished Cross Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5598571428571428\n",
      "0.5502857142857143\n",
      "0.009407335392324063\n",
      "0.5582857142857143\n",
      "0.5456857142857142\n",
      "0.0065205483680751955\n"
     ]
    }
   ],
   "source": [
    "print(np.max(best_vals))\n",
    "print(np.mean(best_vals))\n",
    "print(np.std(best_vals))\n",
    "print(np.max(best_test))\n",
    "print(np.mean(best_test))\n",
    "print(np.std(best_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
